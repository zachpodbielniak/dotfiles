#!/usr/bin/python3

# Container check for distrobox - do this BEFORE any other imports
import os
import subprocess
import sys

ctr_id = os.environ.get("CONTAINER_ID", "")
no_dbox_check = os.environ.get("NO_DBOX_CHECK", "").lower() in ("1", "true")
if not no_dbox_check and ctr_id != "dev":
    cmd = ["distrobox", "enter", "dev", "--", *sys.argv]
    subprocess.run(cmd)
    sys.exit(0)

# Now import everything else inside the dev container
import argparse
import asyncio
import concurrent.futures
import json
import select
import time
import hashlib
from pathlib import Path
from datetime import datetime

# Try to import database dependencies
try:
    import psycopg2
    import psycopg2.extras
    from psycopg2.extras import RealDictCursor
    DB_AVAILABLE = True
except ImportError:
    DB_AVAILABLE = False

# Default decision model for --pick-best feature
DEFAULT_DECISION_MODEL = "claude-opus-4-20250514"

# Database configuration for AI chat storage
AI_CHATS_DB_CONFIG = {
    'host': os.environ.get('AI_CHATS_DB_HOST', '127.0.0.1'),
    'port': int(os.environ.get('AI_CHATS_DB_PORT', '5432')),
    'database': os.environ.get('AI_CHATS_DB_NAME', 'ai_chats'),
    'user': os.environ.get('AI_CHATS_DB_USER', 'postgres'),
    'password': os.environ.get('AI_CHATS_DB_PASSWORD', '')
}

# Model to provider mapping
MODEL_PROVIDERS = {
    # Claude models
    "claude-3-5-sonnet-20241022": "claudpy",
    "claude-3-5-sonnet-latest": "claudpy", 
    "claude-3-5-haiku-20241022": "claudpy",
    "claude-3-5-haiku-latest": "claudpy",
    "claude-3-opus-20240229": "claudpy",
    "claude-3-opus-latest": "claudpy",
    "claude-3-sonnet-20240229": "claudpy",
    "claude-3-haiku-20240307": "claudpy",
    "claude-sonnet-4-20250514": "claudpy",
    "claude-opus-4-20250514": "claudpy",
    "claude-3-7-sonnet-latest": "claudpy",
    
    # Grok models
    "grok-3": "grokpy",
    "grok-3-mini": "grokpy", 
    "grok-3-fast": "grokpy",
    "grok-3-mini-fast": "grokpy",
    "grok-2-1212": "grokpy",
    "grok-2-image-1212": "grokpy",
    "grok-2-vision-1212": "grokpy",
    
    # Gemini models
    "gemini-2.5-pro-preview-06-05": "geminpy",
    "gemini-2.5-pro-preview-05-06": "geminpy",
    "gemini-2.5-flash-preview-04-17": "geminpy",
    "gemini-2.0-flash": "geminpy",
    "gemini-2.0-flash-preview-image-generation": "geminpy",
    "gemini-2.0-flash-lite": "geminpy",
    "gemini-1.5-flash": "geminpy",
    "gemini-1.5-flash-8b": "geminpy",
    "gemini-1.5-pro": "geminpy",
    "gemini-embedding-exp": "geminpy",
    "imagen-3.0-generate-002": "geminpy",
    "veo-2.0-generate-001": "geminpy",
    "gemini-2.0-flash-live-001": "geminpy",
    "text-embedding-004": "geminpy",
    "embedding-001": "geminpy",
    "models/aqa": "geminpy",
    
    # OpenAI models
    "gpt-4o": "openpy",
    "gpt-4-turbo": "openpy",
    "gpt-4": "openpy",
    "gpt-4-32k": "openpy",
    "gpt-4o-mini": "openpy",
    "gpt-3.5-turbo": "openpy",
    "gpt-3.5-turbo-16k": "openpy",
    "dall-e-3": "openpy",
    "dall-e-2": "openpy",
    "whisper-1": "openpy",
    "text-embedding-3-small": "openpy",
    "text-embedding-3-large": "openpy",
    "text-embedding-ada-002": "openpy",
    
    # Perplexity models
    "sonar-pro": "perpy",
    "sonar": "perpy",
    "reasoning-pro": "perpy",
    "sonar-reasoning-pro": "perpy",
    "r1-1776": "perpy",
}

def get_provider_for_model(model):
    """Get the provider for a given model, or return 'ollampy' as default for unknown models."""
    return MODEL_PROVIDERS.get(model, "ollampy")

def check_db_available(debug=False):
    """Check if the AI chats database is available."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database dependencies not available (psycopg2)", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        conn.close()
        return True
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Could not connect to AI chats database: {e}", file=sys.stderr)
        return False

def extract_user_prompt(full_prompt):
    """Extract the actual user prompt from a prompt that may include context."""
    # Check if the prompt contains context
    if "=== CONTEXT FROM PREVIOUS CONVERSATIONS ===" in full_prompt and "=== END CONTEXT ===" in full_prompt:
        # Find the end of the context section
        end_context_marker = "=== END CONTEXT ==="
        context_end = full_prompt.find(end_context_marker)
        if context_end != -1:
            # Extract everything after the context section
            user_prompt_start = context_end + len(end_context_marker)
            user_prompt = full_prompt[user_prompt_start:].strip()
            return user_prompt
    
    # If no context found, return the original prompt
    return full_prompt

def store_chat_in_db(prompt, response, provider, model, metadata=None, tags=None, context_ids=None, context_id=None, debug=False, dry_run=False, show_id=False):
    """Store chat interaction in the database with context_uuid support."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for chat storage", file=sys.stderr)
        return None
    
    if dry_run:
        print("Would store chat in AI chats database", file=sys.stderr)
        return None
    
    if not check_db_available(debug):
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor()
        
        # Extract the actual user prompt (without context)
        clean_prompt = extract_user_prompt(prompt)
        
        # Generate hashes for the clean prompt and response
        prompt_hash = hashlib.sha256(clean_prompt.encode('utf-8')).hexdigest()
        response_hash = hashlib.sha256(response.encode('utf-8')).hexdigest()
        
        # Determine context_uuid
        context_uuid = None
        
        # Priority 1: Use directly specified context_id (--context-id)
        if context_id:
            # Close current cursor and use RealDictCursor for resolve_context_id
            cursor.close()
            resolved_context = resolve_context_id(context_id, debug)
            if resolved_context:
                context_uuid = resolved_context
                if debug:
                    print(f"Using specified context: {context_uuid}", file=sys.stderr)
            else:
                # Error already printed by resolve_context_id
                conn.close()
                return None
            
            # Reopen cursor for the rest of the function
            cursor = conn.cursor()
        
        # Priority 2: Lookup context_uuid from existing chat_ids (--id)
        elif context_ids:
            # Lookup context_uuid from the first chat_id provided
            chat_id = context_ids[0]
            # Handle partial UUID (8 characters)
            if len(chat_id) == 8:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s LIMIT 1
                """, [f"{chat_id}%"])
            else:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats WHERE id = %s::uuid
                """, [chat_id])
            
            result = cursor.fetchone()
            if result:
                context_uuid = result[0]
                if debug:
                    print(f"Using existing context from chat: {context_uuid}", file=sys.stderr)
            else:
                if debug:
                    print(f"Warning: Chat ID {chat_id} not found, creating new context", file=sys.stderr)
        
        # Priority 3: Create new context if none found
        if not context_uuid:
            cursor.execute("""
                INSERT INTO ai_chat_contexts (name) VALUES ('<untitled>')
                RETURNING context_uuid
            """)
            context_uuid = cursor.fetchone()[0]
            if debug:
                print(f"Created new context: {context_uuid}", file=sys.stderr)
        
        # Extract metadata
        request_timestamp = metadata.get('request_timestamp') if metadata else datetime.now()
        response_timestamp = metadata.get('response_timestamp') if metadata else datetime.now()
        duration_ms = metadata.get('duration_ms') if metadata else None
        tokens_input = metadata.get('tokens_input') if metadata else None
        tokens_output = metadata.get('tokens_output') if metadata else None
        cost_input_usd = metadata.get('cost_input_usd') if metadata else None
        cost_output_usd = metadata.get('cost_output_usd') if metadata else None
        cost_total_usd = metadata.get('cost_total_usd') if metadata else None
        extra_metadata = metadata.get('extra_metadata', {}) if metadata else {}
        
        # Process tags
        tags_array = tags if tags else []
        
        # Insert into database and return the UUID
        cursor.execute("""
            INSERT INTO ai_chats (
                context_uuid, prompt, response, provider, model,
                request_timestamp, response_timestamp, duration_ms,
                tokens_input, tokens_output,
                cost_input_usd, cost_output_usd, cost_total_usd,
                prompt_hash, response_hash, tags, metadata
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING id
        """, (
            context_uuid, clean_prompt, response, provider, model,
            request_timestamp, response_timestamp, duration_ms,
            tokens_input, tokens_output,
            cost_input_usd, cost_output_usd, cost_total_usd,
            prompt_hash, response_hash, tags_array, json.dumps(extra_metadata)
        ))
        
        new_id = cursor.fetchone()[0]
        
        # Update context tags with all tags from chats in this context
        if tags_array:  # Only update if this chat has tags
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET tags = ARRAY(
                    SELECT DISTINCT unnest(array_agg(tag))
                    FROM (
                        SELECT unnest(tags) as tag 
                        FROM ai_chats 
                        WHERE context_uuid = %s AND tags IS NOT NULL
                    ) all_tags
                    WHERE tag IS NOT NULL AND tag != ''
                )
                WHERE context_uuid = %s
            """, [context_uuid, context_uuid])
            
            if debug:
                print(f"Updated context tags for context: {context_uuid}", file=sys.stderr)
        
        conn.commit()
        cursor.close()
        conn.close()
        
        if debug:
            print("Chat stored in database successfully", file=sys.stderr)
        
        if show_id:
            print(f"New chat ID: {new_id}", file=sys.stderr)
            print(f"Context UUID: {context_uuid}", file=sys.stderr)
        
        return str(new_id)
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to store chat in database: {e}", file=sys.stderr)
        return None

def search_ai_chats(query, limit=10, provider_filter=None, model_filter=None, tags_filter=None, output_format='text', debug=False):
    """Search AI chats using full-text search."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Build WHERE clauses
        where_clauses = []
        params = []
        
        # Add text search if query is not a wildcard
        if query and query != "*":
            where_clauses.append(
                "(to_tsvector('english', a.prompt) @@ plainto_tsquery('english', %s) OR "
                "to_tsvector('english', a.response) @@ plainto_tsquery('english', %s))"
            )
            params.extend([query, query])
        
        if provider_filter:
            where_clauses.append("a.provider = %s")
            params.append(provider_filter)
        
        if model_filter:
            where_clauses.append("a.model = %s")
            params.append(model_filter)
        
        if tags_filter:
            # Support searching for any of the provided tags
            tag_conditions = []
            for tag in tags_filter:
                tag_conditions.append("a.tags @> %s")
                params.append([tag.strip()])
            if tag_conditions:
                where_clauses.append(f"({' OR '.join(tag_conditions)})")
        
        where_clause = " AND ".join(where_clauses) if where_clauses else "TRUE"
        
        # Search query with ranking
        if query and query != "*":
            sql = f"""
            SELECT 
                a.id,
                LEFT(a.prompt, 100) as prompt_snippet,
                LEFT(a.response, 100) as response_snippet,
                a.provider,
                a.model,
                a.request_timestamp,
                a.duration_ms,
                a.cost_total_usd,
                a.tags,
                a.context_uuid,
                c.name as context_name,
                ts_rank(
                    to_tsvector('english', a.prompt || ' ' || a.response), 
                    plainto_tsquery('english', %s)
                ) as rank
            FROM ai_chats a
            JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            WHERE {where_clause}
            ORDER BY rank DESC, a.request_timestamp DESC
            LIMIT %s
            """
            params.extend([query, limit])
        else:
            sql = f"""
            SELECT 
                a.id,
                LEFT(a.prompt, 100) as prompt_snippet,
                LEFT(a.response, 100) as response_snippet,
                a.provider,
                a.model,
                a.request_timestamp,
                a.duration_ms,
                a.cost_total_usd,
                a.tags,
                a.context_uuid,
                c.name as context_name,
                0 as rank
            FROM ai_chats a
            JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            WHERE {where_clause}
            ORDER BY a.request_timestamp DESC
            LIMIT %s
            """
            params.append(limit)
        
        cursor.execute(sql, params)
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        if output_format == 'json':
            print(json.dumps([dict(r) for r in results], default=str, indent=2))
        else:
            if not results:
                print("No results found.")
                return True
            
            print(f"\nSearch results for: \"{query}\"")
            print("=" * 50)
            
            for i, r in enumerate(results, 1):
                print(f"\n{i}. [{str(r['id'])}] {r['provider']}/{r['model']}")
                print(f"   Context: {r['context_name']} ({str(r['context_uuid'])[:8]}...)")
                print(f"   Time: {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                if r['duration_ms']:
                    print(f"   Duration: {r['duration_ms']}ms")
                if r['cost_total_usd']:
                    print(f"   Cost: ${r['cost_total_usd']:.4f}")
                if r['tags']:
                    print(f"   Tags: {', '.join(r['tags'])}")
                print(f"   Prompt: {r['prompt_snippet']}{'...' if len(r['prompt_snippet']) >= 100 else ''}")
                print(f"   Response: {r['response_snippet']}{'...' if len(r['response_snippet']) >= 100 else ''}")
            
            print(f"\nFound {len(results)} results.")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error searching AI chats: {e}", file=sys.stderr)
        return False

def search_contexts(query, limit=10, output_format='text', debug=False):
    """Search AI chat contexts by name, summary content, or tags."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Handle wildcard search for all contexts
        if query == '*':
            where_clause = "TRUE"  # Show all contexts
            params = []
            order_params = []
        else:
            # Build search query with multiple conditions
            search_conditions = []
            params = []
            
            # Search in name (case insensitive)
            search_conditions.append("LOWER(name) LIKE LOWER(%s)")
            params.append(f"%{query}%")
            
            # Search in summary (full-text search if available, otherwise LIKE)
            search_conditions.append("to_tsvector('english', COALESCE(summary, '')) @@ plainto_tsquery('english', %s)")
            params.append(query)
            
            # Search in tags (using GIN array search)
            search_conditions.append("tags @> ARRAY[%s] OR %s = ANY(tags)")
            params.extend([query.lower(), query.lower()])
            
            # Combine all search conditions with OR
            where_clause = " OR ".join([f"({condition})" for condition in search_conditions])
            
            # Parameters for ordering (only used when not wildcard)
            order_params = [f"%{query}%", query.lower(), query.lower()]
        
        # Build SQL query with appropriate ordering
        if query == '*':
            # For wildcard, simple ordering by activity
            sql = f"""
            SELECT 
                context_uuid,
                name,
                summary,
                tags,
                created_at,
                updated_at,
                summary_last_updated,
                (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count,
                (SELECT MAX(request_timestamp) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as last_chat_time
            FROM ai_chat_contexts ctx
            WHERE {where_clause}
            ORDER BY 
                last_chat_time DESC NULLS LAST,
                created_at DESC
            LIMIT %s
            """
            params.append(limit)
        else:
            # For regular search, relevance-based ordering
            sql = f"""
            SELECT 
                context_uuid,
                name,
                summary,
                tags,
                created_at,
                updated_at,
                summary_last_updated,
                (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count,
                (SELECT MAX(request_timestamp) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as last_chat_time
            FROM ai_chat_contexts ctx
            WHERE {where_clause}
            ORDER BY 
                CASE 
                    WHEN LOWER(name) LIKE LOWER(%s) THEN 1
                    WHEN tags @> ARRAY[%s] OR %s = ANY(tags) THEN 2
                    ELSE 3
                END,
                last_chat_time DESC NULLS LAST,
                created_at DESC
            LIMIT %s
            """
            # Add parameters for ordering and limit
            params.extend(order_params + [limit])
        
        if debug:
            print(f"Search SQL: {sql}", file=sys.stderr)
            print(f"Parameters: {params}", file=sys.stderr)
        
        cursor.execute(sql, params)
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        if output_format == 'json':
            print(json.dumps([dict(r) for r in results], default=str, indent=2))
        else:
            if not results:
                if query == '*':
                    print("No contexts found.")
                else:
                    print("No contexts found.")
                return True
            
            if query == '*':
                print(f"\nAll contexts ({len(results)} found):")
            else:
                print(f"\nContext search results for: \"{query}\"")
            print("=" * 60)
            
            for i, r in enumerate(results, 1):
                uuid_short = str(r['context_uuid'])[:8]
                print(f"\n{i}. [{uuid_short}...] {r['name']}")
                print(f"   UUID: {r['context_uuid']}")
                print(f"   Created: {r['created_at'].strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"   Chats: {r['chat_count']}")
                
                if r['last_chat_time']:
                    print(f"   Last Chat: {r['last_chat_time'].strftime('%Y-%m-%d %H:%M:%S')}")
                
                if r['summary']:
                    summary_preview = r['summary'][:150]
                    print(f"   Summary: {summary_preview}{'...' if len(r['summary']) > 150 else ''}")
                
                if r['tags']:
                    print(f"   Tags: {', '.join(r['tags'])}")
                
                if r['summary_last_updated']:
                    print(f"   Summary Updated: {r['summary_last_updated'].strftime('%Y-%m-%d %H:%M:%S')}")
            
            if query == '*':
                print(f"\nShowing {len(results)} contexts.")
            else:
                print(f"\nFound {len(results)} contexts.")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error searching contexts: {e}", file=sys.stderr)
        return False

def view_ai_chat(chat_id, output_format='text', debug=False, show_history=False, show_full_history=False, trace_mode=False, content_only=False):
    """View full AI chat by UUID with optional conversation history."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Check if chat_id is a partial UUID (8 characters)
        if len(chat_id) == 8:
            # Search for UUIDs starting with this prefix
            cursor.execute("""
                SELECT a.*, c.name as context_name 
                FROM ai_chats a 
                JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid 
                WHERE CAST(a.id AS TEXT) LIKE %s
            """, [f"{chat_id}%"])
            
            results = cursor.fetchall()
            
            if not results:
                cursor.close()
                conn.close()
                print(f"No chat found with ID starting with: {chat_id}")
                return False
            elif len(results) > 1:
                cursor.close()
                conn.close()
                print(f"Multiple chats found with ID starting with '{chat_id}':")
                for r in results:
                    print(f"  - {r['id']} ({r['provider']}/{r['model']}, {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')})")
                print("Please provide the full UUID to view a specific chat.")
                return False
            else:
                result = results[0]
                chat_id = str(result['id'])  # Use full UUID for history functionality
        else:
            # Try to use as full UUID
            cursor.execute("""
                SELECT a.*, c.name as context_name 
                FROM ai_chats a 
                JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid 
                WHERE a.id = %s::uuid
            """, [chat_id])
            
            result = cursor.fetchone()
            
            if not result:
                cursor.close()
                conn.close()
                print(f"No chat found with ID: {chat_id}")
                return False
        
        cursor.close()
        conn.close()
        
        # Handle history reconstruction if requested
        if show_history or show_full_history:
            history = build_conversation_history(chat_id, include_forward=show_full_history, debug=debug)
            if history:
                print(format_conversation_history(history, trace_mode=trace_mode, content_only=content_only))
                return True
            elif show_history or show_full_history:
                print("No conversation history found. Showing individual chat:")
        
        # Handle content-only format
        if content_only:
            print(format_chat_content_only(result))
        elif output_format == 'json':
            print(json.dumps(dict(result), default=str, indent=2))
        else:
            print("=" * 80)
            print(f"Chat ID: {result['id']}")
            print(f"Context: {result['context_name']} ({result['context_uuid']})")
            print(f"Provider: {result['provider']}")
            print(f"Model: {result['model']}")
            print(f"Request Time: {result['request_timestamp']}")
            if result['response_timestamp']:
                print(f"Response Time: {result['response_timestamp']}")
            if result['duration_ms']:
                print(f"Duration: {result['duration_ms']}ms")
            if result['tokens_input']:
                print(f"Input Tokens: {result['tokens_input']}")
            if result['tokens_output']:
                print(f"Output Tokens: {result['tokens_output']}")
            if result['cost_total_usd']:
                print(f"Total Cost: ${result['cost_total_usd']:.6f}")
            if result['tags']:
                print(f"Tags: {', '.join(result['tags'])}")
            print("=" * 80)
            print("\nPROMPT:")
            print("-" * 40)
            print(result['prompt'])
            print("\nRESPONSE:")
            print("-" * 40)
            print(result['response'])
            if result['metadata'] and result['metadata'] != '{}':
                print("\nMETADATA:")
                print("-" * 40)
                metadata = json.loads(result['metadata']) if isinstance(result['metadata'], str) else result['metadata']
                print(json.dumps(metadata, indent=2))
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error viewing AI chat: {e}", file=sys.stderr)
        return False

def delete_ai_chat(chat_id, confirm=False, debug=False):
    """Delete AI chat by UUID."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # First, get the chat details
        cursor.execute("""
            SELECT id, LEFT(prompt, 100) as prompt_snippet, provider, model, request_timestamp
            FROM ai_chats WHERE id = %s::uuid
        """, [chat_id])
        
        result = cursor.fetchone()
        
        if not result:
            print(f"No chat found with ID: {chat_id}")
            cursor.close()
            conn.close()
            return False
        
        # Confirm deletion
        if not confirm:
            print("About to delete chat:")
            print(f"  ID: {result['id']}")
            print(f"  Provider/Model: {result['provider']}/{result['model']}")
            print(f"  Time: {result['request_timestamp']}")
            print(f"  Prompt: {result['prompt_snippet']}{'...' if len(result['prompt_snippet']) >= 100 else ''}")
            response = input("Are you sure? (y/N): ")
            if response.lower() != 'y':
                print("Deletion cancelled.")
                cursor.close()
                conn.close()
                return False
        
        # Delete the chat
        cursor.execute("DELETE FROM ai_chats WHERE id = %s::uuid", [chat_id])
        conn.commit()
        
        cursor.close()
        conn.close()
        
        print("Chat deleted successfully.")
        return True
    
    except psycopg2.Error as e:
        print(f"Error deleting AI chat: {e}", file=sys.stderr)
        return False

def get_chat_context(chat_ids, debug=False):
    """Retrieve chat content by UUIDs (full or partial 8-char) for use as context."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for context retrieval", file=sys.stderr)
        return []
    
    if not check_db_available(debug):
        if debug:
            print("Warning: Could not connect to AI chats database for context", file=sys.stderr)
        return []
    
    if not chat_ids:
        return []
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Resolve partial UUIDs to full UUIDs first
        resolved_ids = []
        for chat_id in chat_ids:
            if len(chat_id) == 8:
                # Search for UUIDs starting with this prefix
                cursor.execute("""
                    SELECT id FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s
                """, [f"{chat_id}%"])
                
                matches = cursor.fetchall()
                
                if not matches:
                    print(f"Error: No chat found with ID starting with: {chat_id}", file=sys.stderr)
                    cursor.close()
                    conn.close()
                    return None  # Signal error to caller
                elif len(matches) > 1:
                    print(f"Error: Multiple chats found with ID starting with '{chat_id}':", file=sys.stderr)
                    for match in matches:
                        cursor.execute("""
                            SELECT provider, model, request_timestamp 
                            FROM ai_chats WHERE id = %s::uuid
                        """, [str(match['id'])])
                        details = cursor.fetchone()
                        print(f"  - {match['id']} ({details['provider']}/{details['model']}, {details['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')})", file=sys.stderr)
                    print("Please provide the full UUID to disambiguate.", file=sys.stderr)
                    cursor.close()
                    conn.close()
                    return None  # Signal error to caller
                else:
                    resolved_ids.append(str(matches[0]['id']))
            else:
                # Use as full UUID
                resolved_ids.append(chat_id)
        
        # Now query for all resolved UUIDs
        placeholders = ', '.join(['%s::uuid'] * len(resolved_ids))
        cursor.execute(f"""
            SELECT id, prompt, response, provider, model, request_timestamp, tags
            FROM ai_chats 
            WHERE id IN ({placeholders})
            ORDER BY request_timestamp ASC
        """, resolved_ids)
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        context_chats = []
        found_ids = set()
        
        for result in results:
            found_ids.add(str(result['id']))
            context_chats.append({
                'id': str(result['id']),
                'prompt': result['prompt'],
                'response': result['response'],
                'provider': result['provider'],
                'model': result['model'],
                'timestamp': result['request_timestamp'],
                'tags': result['tags'] or []
            })
        
        # Report missing chats
        missing_ids = set(resolved_ids) - found_ids
        if missing_ids and debug:
            print(f"Warning: Could not find chats: {', '.join(missing_ids)}", file=sys.stderr)
        
        return context_chats
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to retrieve chat context: {e}", file=sys.stderr)
        return []

def format_context_for_prompt(context_chats):
    """Format retrieved chats into context text for the prompt."""
    if not context_chats:
        return ""
    
    context_parts = ["=== CONTEXT FROM PREVIOUS CONVERSATIONS ===\n"]
    
    for i, chat in enumerate(context_chats, 1):
        context_parts.append(f"Context {i} (ID: {chat['id'][:8]}..., {chat['provider']}/{chat['model']}):")
        
        if chat['tags']:
            context_parts.append(f"Tags: {', '.join(chat['tags'])}")
        
        context_parts.append(f"User: {chat['prompt']}")
        context_parts.append(f"Assistant: {chat['response']}")
        context_parts.append("")  # Empty line between contexts
    
    context_parts.append("=== END CONTEXT ===\n")
    
    return "\n".join(context_parts)

def build_conversation_history(chat_id, include_forward=False, debug=False):
    """Build conversation history by getting all chats in the same context."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for history reconstruction", file=sys.stderr)
        return []
    
    if not check_db_available(debug):
        if debug:
            print("Warning: Could not connect to AI chats database for history", file=sys.stderr)
        return []
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # First, get the context_uuid for the given chat_id
        # Handle partial UUID (8 characters)
        if len(chat_id) == 8:
            cursor.execute("""
                SELECT context_uuid FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s LIMIT 1
            """, [f"{chat_id}%"])
        else:
            cursor.execute("""
                SELECT context_uuid FROM ai_chats WHERE id = %s::uuid
            """, [chat_id])
        
        result = cursor.fetchone()
        if not result:
            cursor.close()
            conn.close()
            if debug:
                print(f"Warning: Chat ID {chat_id} not found", file=sys.stderr)
            return []
        
        context_uuid = result['context_uuid']
        
        # Get all chats in the same context, ordered by timestamp
        cursor.execute("""
            SELECT a.id, a.prompt, a.response, a.provider, a.model, a.request_timestamp, a.tags, a.metadata, 
                   a.context_uuid, c.name as context_name
            FROM ai_chats a
            JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            WHERE a.context_uuid = %s
            ORDER BY a.request_timestamp ASC
        """, [context_uuid])
        
        all_chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        # Convert to the expected format
        conversation_history = []
        for chat in all_chats:
            conversation_history.append({
                'id': str(chat['id']),
                'prompt': chat['prompt'],
                'response': chat['response'],
                'provider': chat['provider'],
                'model': chat['model'],
                'timestamp': chat['request_timestamp'],
                'tags': chat['tags'] or [],
                'context_uuid': str(chat['context_uuid']),
                'context_name': chat['context_name']
            })
        
        if debug:
            print(f"Found {len(conversation_history)} chats in context {context_uuid}", file=sys.stderr)
        
        return conversation_history
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to build conversation history: {e}", file=sys.stderr)
        return []

def view_context(context_uuid, output_format='text', debug=False, content_only=False, by_chat_uuid=False):
    """View entire conversation context by context_uuid or by chat_uuid."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # If by_chat_uuid is True, lookup context_uuid from chat_uuid
        if by_chat_uuid:
            chat_uuid = context_uuid  # Rename for clarity
            
            # Handle partial chat UUID (8 characters)
            if len(chat_uuid) == 8:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s
                """, [f"{chat_uuid}%"])
                
                results = cursor.fetchall()
                
                if not results:
                    cursor.close()
                    conn.close()
                    print(f"No chat found with UUID starting with: {chat_uuid}")
                    return False
                elif len(results) > 1:
                    cursor.close()
                    conn.close()
                    print(f"Multiple chats found with UUID starting with '{chat_uuid}':")
                    # Get more details for disambiguation
                    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
                    cursor.execute("""
                        SELECT id, provider, model, request_timestamp 
                        FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s
                        ORDER BY request_timestamp DESC
                    """, [f"{chat_uuid}%"])
                    chat_results = cursor.fetchall()
                    for r in chat_results:
                        print(f"  - {r['id']} ({r['provider']}/{r['model']}, {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')})")
                    cursor.close()
                    conn.close()
                    print("Please provide the full chat UUID to view its context.")
                    return False
                else:
                    context_uuid = str(results[0]['context_uuid'])
            else:
                # Full chat UUID
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats WHERE id = %s::uuid
                """, [chat_uuid])
                
                result = cursor.fetchone()
                if not result:
                    cursor.close()
                    conn.close()
                    print(f"No chat found with UUID: {chat_uuid}")
                    return False
                
                context_uuid = str(result['context_uuid'])
            
            if debug:
                print(f"Found context UUID {context_uuid} for chat UUID {chat_uuid}", file=sys.stderr)
        
        # Use updated resolver for context UUID/name
        elif not by_chat_uuid:
            # Close current cursor and use resolve_context_id
            cursor.close()
            resolved_context = resolve_context_id(context_uuid, debug)
            if not resolved_context:
                conn.close()
                return False
            
            context_uuid = resolved_context
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get context info
        cursor.execute("""
            SELECT * FROM ai_chat_contexts WHERE context_uuid = %s::uuid
        """, [context_uuid])
        
        context_info = cursor.fetchone()
        if not context_info:
            cursor.close()
            conn.close()
            print(f"No context found with UUID: {context_uuid}")
            return False
        
        # Get all chats in this context
        cursor.execute("""
            SELECT * FROM ai_chats WHERE context_uuid = %s
            ORDER BY request_timestamp ASC
        """, [context_uuid])
        
        chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not chats:
            print(f"No chats found in context: {context_uuid}")
            return False
        
        if content_only:
            # Show content-only format
            output = []
            output.append(f"# {context_info['name']} ({context_info['context_uuid']})")
            output.append("")
            
            # Add summary as first subsection if available
            if context_info['summary']:
                output.append("## Summary")
                output.append("")
                output.append(context_info['summary'])
                output.append("")
                output.append("---")
                output.append("")
            
            for chat in chats:
                full_id = str(chat['id'])
                output.append(f"## Question ({full_id} - {chat['provider']}/{chat['model']})")
                output.append("")
                output.append(chat['prompt'])
                output.append("")
                output.append(f"## Response ({full_id} - {chat['provider']}/{chat['model']})")
                output.append("")
                output.append(chat['response'])
                output.append("")
                output.append("---")
                output.append("")
            
            # Remove last separator
            if output and output[-2] == "---":
                output = output[:-2]
            
            print("\n".join(output))
        elif output_format == 'json':
            context_data = {
                'context': dict(context_info),
                'chats': [dict(chat) for chat in chats]
            }
            print(json.dumps(context_data, default=str, indent=2))
        else:
            # Regular format
            print("=" * 80)
            print(f"Context: {context_info['name']}")
            print(f"Context UUID: {context_info['context_uuid']}")
            print(f"Created: {context_info['created_at']}")
            print(f"Updated: {context_info['updated_at']}")
            print(f"Total Chats: {len(chats)}")
            print("=" * 80)
            
            for i, chat in enumerate(chats, 1):
                print(f"\n[{i}] {chat['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')} | {chat['provider']}/{chat['model']} | {str(chat['id'])[:8]}...")
                if chat['tags']:
                    print(f"   Tags: {', '.join(chat['tags'])}")
                print("-" * 80)
                print(f"PROMPT:\n{chat['prompt']}")
                print(f"\nRESPONSE:\n{chat['response']}")
            
            # Display summary at the bottom if available
            if context_info['summary']:
                print("\n" + "=" * 80)
                print("CONTEXT SUMMARY:")
                print("=" * 80)
                print(context_info['summary'])
                if context_info['summary_last_updated']:
                    print(f"\nLast updated: {context_info['summary_last_updated'].strftime('%Y-%m-%d %H:%M:%S')}")
                if context_info['summary_provider'] and context_info['summary_model']:
                    print(f"Generated by: {context_info['summary_provider']}/{context_info['summary_model']}")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error viewing context: {e}", file=sys.stderr)
        return False

def edit_context(context_identifier, name=None, debug=False):
    """Edit context properties like name."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    if not name:
        print("Error: --name is required when using --edit-context", file=sys.stderr)
        return False
    
    try:
        # Resolve context identifier (UUID, partial UUID, or name)
        resolved_context = resolve_context_id(context_identifier, debug)
        if not resolved_context:
            return False
        
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Update the context name
        cursor.execute("""
            UPDATE ai_chat_contexts SET name = %s WHERE context_uuid = %s::uuid
            RETURNING *
        """, [name, resolved_context])
        
        result = cursor.fetchone()
        if not result:
            cursor.close()
            conn.close()
            print(f"No context found with UUID: {context_uuid}")
            return False
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"Context updated successfully:")
        print(f"  UUID: {result['context_uuid']}")
        print(f"  Name: {result['name']}")
        print(f"  Updated: {result['updated_at']}")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error editing context: {e}", file=sys.stderr)
        return False

def summarize_context(context_identifier, model=None, debug=False):
    """Generate a summary for a context using the specified model."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-3"  # Default model
    
    # Get provider for the model
    provider = get_provider_for_model(model)
    if not provider:
        print(f"Error: No provider found for model {model}", file=sys.stderr)
        return False
    
    try:
        # Resolve context identifier (UUID, partial UUID, or name)
        resolved_context = resolve_context_id(context_identifier, debug)
        if not resolved_context:
            return False
        
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get all chats in this context
        cursor.execute("""
            SELECT a.*, c.name as context_name
            FROM ai_chats a
            JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            WHERE a.context_uuid = %s
            ORDER BY a.request_timestamp ASC
        """, [resolved_context])
        
        chats = cursor.fetchall()
        
        if not chats:
            cursor.close()
            conn.close()
            print(f"No chats found in context: {resolved_context}")
            return False
        
        # Build conversation text for summarization
        conversation_text = []
        for chat in chats:
            conversation_text.append(f"User: {chat['prompt']}")
            conversation_text.append(f"Assistant: {chat['response']}")
        
        full_conversation = "\n\n".join(conversation_text)
        
        # Create summarization prompt
        summary_prompt = f"""Please provide a comprehensive summary of this conversation. Focus on the main topics discussed, key decisions made, and important information exchanged. Keep the summary informative but concise (2-3 paragraphs maximum).

Conversation:
{full_conversation}

Summary:"""
        
        print(f"Generating summary using {provider}/{model}...")
        
        # Run the summarization using the specified provider/model
        import subprocess
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(summary_prompt)
            temp_file = f.name
        
        try:
            cmd = [provider, "--model", model, "--no-preserve", "-f", temp_file]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode != 0:
                print(f"Error running {provider}: {result.stderr}", file=sys.stderr)
                return False
            
            summary = result.stdout.strip()
            if not summary:
                print("Error: Empty summary generated", file=sys.stderr)
                return False
            
            # Update context with summary
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET summary = %s, summary_provider = %s, summary_model = %s, summary_last_updated = CURRENT_TIMESTAMP
                WHERE context_uuid = %s
                RETURNING *
            """, [summary, provider, model, resolved_context])
            
            result_row = cursor.fetchone()
            conn.commit()
            cursor.close()
            conn.close()
            
            print(f"Summary generated successfully for context: {chats[0]['context_name']}")
            print(f"Context UUID: {resolved_context}")
            print(f"Summary Provider/Model: {provider}/{model}")
            print(f"\nSummary:\n{summary}")
            
            return True
            
        finally:
            import os
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    except psycopg2.Error as e:
        print(f"Error summarizing context: {e}", file=sys.stderr)
        return False
    except subprocess.TimeoutExpired:
        print("Error: Summary generation timed out", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during summarization: {e}", file=sys.stderr)
        return False

def generate_context_name(context_identifier, model=None, debug=False):
    """Generate a name for a context based on its summary."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-3"  # Default model
    
    # Get provider for the model
    provider = get_provider_for_model(model)
    if not provider:
        print(f"Error: No provider found for model {model}", file=sys.stderr)
        return False
    
    try:
        # Resolve context identifier (UUID, partial UUID, or name)
        resolved_context = resolve_context_id(context_identifier, debug)
        if not resolved_context:
            return False
        
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get context info
        cursor.execute("""
            SELECT * FROM ai_chat_contexts WHERE context_uuid = %s
        """, [resolved_context])
        
        context_info = cursor.fetchone()
        if not context_info:
            cursor.close()
            conn.close()
            print(f"No context found with UUID: {resolved_context}")
            return False
        
        # Check if we have a summary
        if not context_info['summary']:
            print(f"No summary found for context. Generating summary first...")
            cursor.close()
            conn.close()
            
            # Generate summary first
            if not summarize_context(resolved_context, model, debug):
                return False
            
            # Re-fetch context info
            conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            cursor.execute("""
                SELECT * FROM ai_chat_contexts WHERE context_uuid = %s
            """, [resolved_context])
            context_info = cursor.fetchone()
        
        summary = context_info['summary']
        
        # Create name generation prompt
        name_prompt = f"""Based on the following conversation summary, generate a concise, descriptive name for this conversation context. The name should be 8 words or less and capture the main topic or purpose of the conversation.

Summary:
{summary}

Generate a name that is:
- 8 words or less
- Descriptive and clear
- Suitable as a conversation title
- Focused on the main topic

Name:"""
        
        print(f"Generating context name using {provider}/{model}...")
        
        # Run the name generation using the specified provider/model
        import subprocess
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(name_prompt)
            temp_file = f.name
        
        try:
            cmd = [provider, "--model", model, "--no-preserve", "-f", temp_file]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            
            if result.returncode != 0:
                print(f"Error running {provider}: {result.stderr}", file=sys.stderr)
                return False
            
            generated_name = result.stdout.strip()
            if not generated_name:
                print("Error: Empty name generated", file=sys.stderr)
                return False
            
            # Clean up the generated name (remove quotes, limit length)
            generated_name = generated_name.strip('"\'').strip()
            words = generated_name.split()
            if len(words) > 8:
                generated_name = ' '.join(words[:8])
            
            # Update context with the new name
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET name = %s
                WHERE context_uuid = %s
                RETURNING *
            """, [generated_name, resolved_context])
            
            result_row = cursor.fetchone()
            conn.commit()
            cursor.close()
            conn.close()
            
            print(f"Name generated successfully for context:")
            print(f"Context UUID: {resolved_context}")
            print(f"New Name: {generated_name}")
            print(f"Generated by: {provider}/{model}")
            
            return True
            
        finally:
            import os
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    except psycopg2.Error as e:
        print(f"Error generating context name: {e}", file=sys.stderr)
        return False
    except subprocess.TimeoutExpired:
        print("Error: Name generation timed out", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during name generation: {e}", file=sys.stderr)
        return False

def summarize_contexts_missing(model=None, debug=False):
    """Summarize all contexts that don't have summaries."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-3"  # Default model
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Find contexts without summaries
        cursor.execute("""
            SELECT context_uuid, name FROM ai_chat_contexts 
            WHERE summary IS NULL OR summary = ''
            ORDER BY created_at DESC
        """)
        
        contexts_to_summarize = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not contexts_to_summarize:
            print("No contexts found that need summaries.")
            return True
        
        print(f"Found {len(contexts_to_summarize)} contexts without summaries.")
        print(f"Will summarize using model: {model}")
        
        success_count = 0
        failure_count = 0
        
        for context in contexts_to_summarize:
            context_uuid = str(context['context_uuid'])
            context_name = context['name'] or '<untitled>'
            
            print(f"\nSummarizing context: {context_name} ({context_uuid[:8]}...)")
            
            if summarize_context(context_uuid, model, debug):
                success_count += 1
                print(f" Successfully summarized context {context_uuid[:8]}...")
            else:
                failure_count += 1
                print(f" Failed to summarize context {context_uuid[:8]}...")
        
        print(f"\nSummary complete:")
        print(f"- Successfully summarized: {success_count}")
        print(f"- Failed to summarize: {failure_count}")
        
        return failure_count == 0
        
    except psycopg2.Error as e:
        print(f"Error finding contexts to summarize: {e}", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during batch summarization: {e}", file=sys.stderr)
        return False

def summarize_contexts_outdated(model=None, debug=False):
    """Update summaries for contexts with newer chats than summary_last_updated."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-3"  # Default model
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Find contexts that have chats newer than their summary_last_updated
        cursor.execute("""
            SELECT DISTINCT ctx.context_uuid, ctx.name, ctx.summary_last_updated,
                   MAX(chat.request_timestamp) as latest_chat
            FROM ai_chat_contexts ctx
            JOIN ai_chats chat ON ctx.context_uuid = chat.context_uuid
            WHERE ctx.summary IS NOT NULL 
              AND ctx.summary != ''
              AND (ctx.summary_last_updated IS NULL 
                   OR chat.request_timestamp > ctx.summary_last_updated)
            GROUP BY ctx.context_uuid, ctx.name, ctx.summary_last_updated
            ORDER BY latest_chat DESC
        """)
        
        contexts_to_update = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not contexts_to_update:
            print("No contexts found with outdated summaries.")
            return True
        
        print(f"Found {len(contexts_to_update)} contexts with outdated summaries.")
        print(f"Will update summaries using model: {model}")
        
        success_count = 0
        failure_count = 0
        
        for context in contexts_to_update:
            context_uuid = str(context['context_uuid'])
            context_name = context['name'] or '<untitled>'
            last_updated = context['summary_last_updated']
            latest_chat = context['latest_chat']
            
            print(f"\nUpdating summary for context: {context_name} ({context_uuid[:8]}...)")
            if last_updated:
                print(f"  Last summary update: {last_updated}")
            else:
                print(f"  Last summary update: Never")
            print(f"  Latest chat: {latest_chat}")
            
            if summarize_context(context_uuid, model, debug):
                success_count += 1
                print(f" Successfully updated summary for context {context_uuid[:8]}...")
            else:
                failure_count += 1
                print(f" Failed to update summary for context {context_uuid[:8]}...")
        
        print(f"\nSummary update complete:")
        print(f"- Successfully updated: {success_count}")
        print(f"- Failed to update: {failure_count}")
        
        return failure_count == 0
        
    except psycopg2.Error as e:
        print(f"Error finding contexts to update: {e}", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during batch summary update: {e}", file=sys.stderr)
        return False

def resolve_context_id(context_identifier, debug=False):
    """Resolve a context identifier (UUID, partial UUID, or name) to a full context UUID."""
    if not DB_AVAILABLE:
        if debug:
            print("Error: Database dependencies not available", file=sys.stderr)
        return None
    
    if not check_db_available(debug):
        if debug:
            print("Error: Could not connect to AI chats database", file=sys.stderr)
        return None
    
    def is_uuid_like(text):
        """Check if text looks like a UUID (full or partial)."""
        import re
        # Full UUID pattern
        if re.match(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', text, re.IGNORECASE):
            return True
        # Partial UUID pattern (8 hex characters)
        if len(text) == 8 and re.match(r'^[0-9a-f]{8}$', text, re.IGNORECASE):
            return True
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Determine if input is UUID-like or a name
        if is_uuid_like(context_identifier):
            # Handle UUID-based lookup
            if len(context_identifier) == 8:
                # Partial UUID
                cursor.execute("""
                    SELECT context_uuid, name FROM ai_chat_contexts 
                    WHERE CAST(context_uuid AS TEXT) LIKE %s
                """, [f"{context_identifier}%"])
                
                results = cursor.fetchall()
                
                if not results:
                    cursor.close()
                    conn.close()
                    print(f"No context found with UUID starting with: {context_identifier}", file=sys.stderr)
                    return None
                elif len(results) > 1:
                    cursor.close()
                    conn.close()
                    print(f"Multiple contexts found with UUID starting with '{context_identifier}':", file=sys.stderr)
                    for r in results:
                        context_name = r['name'] or '<untitled>'
                        print(f"  - {r['context_uuid']} ({context_name})", file=sys.stderr)
                    print("Please provide the full UUID to specify a unique context.", file=sys.stderr)
                    return None
                else:
                    resolved_uuid = str(results[0]['context_uuid'])
                    if debug:
                        context_name = results[0]['name'] or '<untitled>'
                        print(f"Resolved partial UUID {context_identifier} to {resolved_uuid} ({context_name})", file=sys.stderr)
                    cursor.close()
                    conn.close()
                    return resolved_uuid
            else:
                # Full UUID - validate it exists
                cursor.execute("""
                    SELECT context_uuid, name FROM ai_chat_contexts 
                    WHERE context_uuid = %s::uuid
                """, [context_identifier])
                
                result = cursor.fetchone()
                if not result:
                    cursor.close()
                    conn.close()
                    print(f"No context found with UUID: {context_identifier}", file=sys.stderr)
                    return None
                
                if debug:
                    context_name = result['name'] or '<untitled>'
                    print(f"Validated UUID {context_identifier} ({context_name})", file=sys.stderr)
                
                cursor.close()
                conn.close()
                return context_identifier
        else:
            # Handle name-based lookup
            cursor.execute("""
                SELECT context_uuid, name FROM ai_chat_contexts 
                WHERE name = %s
            """, [context_identifier])
            
            results = cursor.fetchall()
            
            if not results:
                cursor.close()
                conn.close()
                print(f"No context found with name: {context_identifier}", file=sys.stderr)
                return None
            elif len(results) > 1:
                cursor.close()
                conn.close()
                print(f"Multiple contexts found with name '{context_identifier}':", file=sys.stderr)
                for r in results:
                    print(f"  - {r['context_uuid']} ({r['name']})", file=sys.stderr)
                print("Please provide a unique context name or use the UUID.", file=sys.stderr)
                return None
            else:
                resolved_uuid = str(results[0]['context_uuid'])
                if debug:
                    print(f"Resolved name '{context_identifier}' to {resolved_uuid}", file=sys.stderr)
                cursor.close()
                conn.close()
                return resolved_uuid
    
    except psycopg2.Error as e:
        if debug:
            print(f"Error resolving context identifier: {e}", file=sys.stderr)
        return None
    except Exception as e:
        if debug:
            print(f"Error during context identifier resolution: {e}", file=sys.stderr)
        return None

def format_chat_content_only(chat_data):
    """Format a single chat in content-only markdown format."""
    # Use full UUID for chat ID
    full_id = str(chat_data['id'])
    context_uuid = str(chat_data['context_uuid'])
    context_name = chat_data.get('context_name', '<untitled>')
    
    output = []
    output.append(f"# {context_name} ({context_uuid})")
    output.append("")
    output.append(f"## Question ({full_id} - {chat_data['provider']}/{chat_data['model']})")
    output.append("")
    output.append(chat_data['prompt'])
    output.append("")
    output.append(f"## Response ({full_id} - {chat_data['provider']}/{chat_data['model']})")
    output.append("")
    output.append(chat_data['response'])
    
    return "\n".join(output)

def format_conversation_history(history, trace_mode=False, content_only=False):
    """Format conversation history for display."""
    if not history:
        return "No conversation history found."
    
    if content_only:
        # Show content-only format for the entire conversation
        output = []
        # Use the first chat's context info for the main header
        if history:
            first_chat = history[0]
            context_name = first_chat.get('context_name', '<untitled>')
            context_uuid = first_chat.get('context_uuid', 'unknown')
            output.append(f"# {context_name} ({context_uuid})")
            output.append("")
        
        for chat in history:
            full_id = chat['id']
            output.append(f"## Question ({full_id} - {chat['provider']}/{chat['model']})")
            output.append("")
            output.append(chat['prompt'])
            output.append("")
            output.append(f"## Response ({full_id} - {chat['provider']}/{chat['model']})")
            output.append("")
            output.append(chat['response'])
            output.append("")
            output.append("---")
            output.append("")
        
        # Remove the last separator
        if output and output[-2] == "---":
            output = output[:-2]
        
        return "\n".join(output)
    elif trace_mode:
        # Show trace format
        output = ["Conversation Flow Trace:"]
        output.append("=" * 40)
        
        for i, chat in enumerate(history):
            timestamp = chat['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            tags_str = f" [{', '.join(chat['tags'])}]" if chat['tags'] else ""
            output.append(f"{i+1:2d}. {chat['id'][:8]}... | {timestamp} | {chat['provider']}/{chat['model']}{tags_str}")
        
        return "\n".join(output)
    else:
        # Show full conversation format
        output = ["Conversation History:"]
        output.append("=" * 80)
        
        for i, chat in enumerate(history):
            timestamp = chat['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            tags_str = f" [{', '.join(chat['tags'])}]" if chat['tags'] else ""
            
            output.append(f"\n[{i+1}] {timestamp} | {chat['provider']}/{chat['model']} | {chat['id'][:8]}...{tags_str}")
            output.append("-" * 80)
            output.append(f"User: {chat['prompt']}")
            output.append(f"\nAssistant: {chat['response']}")
            output.append("")
        
        return "\n".join(output)

def pick_best_response(prompt, model_responses, decision_model):
    """Use a reasoning model to pick the best response from multiple models."""
    # Prepare the decision prompt
    decision_prompt = f"""Original User Query:
{prompt}

I received multiple AI responses to this query. Please analyze them and select the BEST response based on:
1. Accuracy and correctness
2. Completeness and thoroughness
3. Clarity and coherence
4. Relevance to the query
5. Practical usefulness

Here are the responses from different models:

"""
    
    for i, (model, response) in enumerate(model_responses):
        if response.get("success") and response.get("output"):
            decision_prompt += f"=== Response {i+1} from {model} ===\n{response['output']}\n\n"
    
    decision_prompt += """Please analyze all responses and tell me which response number is the BEST (1, 2, 3, etc).
Start your response with "BEST: X" where X is the response number.
Then provide a brief explanation of why that response is superior to the others.
"""
    
    # Get the provider for the decision model
    provider = get_provider_for_model(decision_model)
    
    # Build command for decision model
    cmd = [provider, "--model", decision_model, "--no-preserve", decision_prompt]
    
    try:
        # Run the decision model
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0 and result.stdout:
            # Parse the response to find the best model
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if line.strip().startswith("BEST:"):
                    try:
                        # Extract the number
                        best_num = int(line.split(":")[1].strip().split()[0]) - 1
                        if 0 <= best_num < len(model_responses):
                            best_model = model_responses[best_num][0]
                            reasoning = '\n'.join(lines[1:])  # Rest is explanation
                            return best_model, reasoning.strip()
                    except:
                        pass
            
        # If parsing fails, return the first successful response
        for model, response in model_responses:
            if response.get("success"):
                return model, "Could not determine best response, defaulting to first successful model"
                
    except Exception as e:
        print(f"Error running decision model: {e}", file=sys.stderr)
        # Return first successful response as fallback
        for model, response in model_responses:
            if response.get("success"):
                return model, "Decision model failed, defaulting to first successful model"
    
    return None, "No successful responses to choose from"

def save_multi_model_transaction(models, prompt, responses, metadata=None, tags=None, context_ids=None, context_id=None, store_db=True, debug=False, dry_run=False, show_id=False):
    """Save multi-model chat transaction to markdown file organized by date and optionally to database."""
    # Create directory structure if it doesn't exist
    base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
    base_path.mkdir(parents=True, exist_ok=True)
    
    # Generate filename based on today's date
    today = datetime.now()
    filename = today.strftime("%Y-%m-%d.md")
    file_path = base_path / filename
    
    # Generate timestamp for section header
    timestamp = today.strftime("%Y-%m-%d %H:%M:%S")
    
    # Prepare content
    content = f"\n# {timestamp} - Multi-Model Query (aipy)\n\n"
    content += f"## Prompt\n\n```\n{prompt}\n```\n\n"
    
    # Add model responses
    content += f"## Model Responses\n\n"
    
    for i, (model, response) in enumerate(zip(models, responses)):
        content += f"### {i+1}. {model}\n\n"
        if response.get("success"):
            content += f"{response.get('output', 'No output captured')}\n\n"
        else:
            content += f"**Error**: {response.get('error', 'Unknown error')}\n\n"
    
    # Add metadata if provided
    if metadata:
        content += f"## Metadata\n\n"
        content += f"- **Provider**: aipy (multi-model wrapper)\n"
        content += f"- **Models**: {', '.join(models)}\n"
        if 'total_time' in metadata:
            content += f"- **Total Time**: {metadata['total_time']:.2f}s\n"
        if 'parallel' in metadata:
            content += f"- **Execution**: {'Parallel' if metadata['parallel'] else 'Sequential'}\n"
        if 'iterations' in metadata:
            content += f"- **Iterations**: {metadata['iterations']}\n"
        if 'best_model' in metadata:
            content += f"- **Best Response**: {metadata['best_model']}\n"
        if 'decision_model' in metadata:
            content += f"- **Decision Model**: {metadata['decision_model']}\n"
        if 'decision_reasoning' in metadata:
            content += f"\n### Decision Reasoning\n\n{metadata['decision_reasoning']}\n"
    
    content += "\n---\n"
    
    # Write to file (append if exists)
    try:
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        print(f"Warning: Could not save chat transaction: {e}", file=sys.stderr)
    
    # Store in database if requested and available
    if store_db:
        # For multi-model responses, store each successful response separately
        for i, (model, response) in enumerate(zip(models, responses)):
            if response.get("success") and response.get('output'):
                provider = get_provider_for_model(model)
                
                # Prepare database metadata
                db_metadata = {
                    'request_timestamp': today,
                    'response_timestamp': today,
                    'extra_metadata': {
                        'source': 'aipy_multi_model',
                        'model_index': i,
                        'total_models': len(models),
                        'is_best_response': metadata.get('best_model') == model if metadata else False,
                        'execution_type': 'parallel' if metadata and metadata.get('parallel') else 'sequential',
                        'iterations': metadata.get('iterations', 1) if metadata else 1
                    }
                }
                
                # Add timing if available
                if metadata and 'total_time' in metadata:
                    db_metadata['duration_ms'] = int(metadata['total_time'] * 1000)
                
                # Add decision info if available
                if metadata and metadata.get('best_model') == model:
                    db_metadata['extra_metadata']['decision_model'] = metadata.get('decision_model')
                    db_metadata['extra_metadata']['decision_reasoning'] = metadata.get('decision_reasoning')
                
                new_id = store_chat_in_db(
                    prompt, 
                    response['output'], 
                    provider, 
                    model, 
                    db_metadata,
                    tags=tags,
                    context_ids=context_ids,
                    context_id=context_id,
                    debug=debug, 
                    dry_run=dry_run,
                    show_id=show_id
                )
                
                # Store the new ID for summary display
                if new_id and 'new_chat_ids' not in metadata:
                    if not hasattr(metadata, 'get'):
                        metadata = metadata or {}
                    metadata['new_chat_ids'] = []
                if new_id:
                    metadata.setdefault('new_chat_ids', []).append(new_id)

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        prog="aipy",
        description="""

                    Universal AI Provider Wrapper


Intelligently routes requests to appropriate AI providers while maintaining
conversation history, context threading, and advanced search capabilities.

Supported Providers: Claude, Grok, Gemini, OpenAI, Ollama, Perplexity
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""

                                EXAMPLES


 BASIC USAGE
  aipy "What is quantum computing?"
  aipy --model gpt-4o "Explain machine learning"
  aipy --model claude-3-5-sonnet-latest "Help with coding"

 MULTI-MODEL COMPARISON
  aipy --model "gpt-4o,claude-3-5-sonnet,grok-3" "Compare AI approaches"
  aipy --model "gpt-4o,claude-3-5-sonnet" --pick-best "Complex analysis"

 IMAGE GENERATION
  aipy --image-gen "A beautiful sunset over mountains"
  aipy --times 3 --image-gen "Abstract art in blue tones"
  aipy --image-gen --hd --output sunset.png "Photorealistic sunset"

 FILE AND MEDIA INPUT
  aipy -f config.yaml "Explain this configuration"
  aipy -f script.py -f README.md "Review this code project"
  aipy --image screenshot.png "What's shown in this image?"

 DATABASE OPERATIONS
  # Search chat history
  aipy --search "quantum computing"
  aipy --search "python" --provider-filter claudpy
  aipy --search "API" --model-filter gpt-4o
  aipy --search "*" --tags-filter "work,coding"

  # View conversations (supports partial UUIDs)
  aipy --view 3740590a
  aipy --view 3740590a-7356-4136-8247-2657318797b5
  
  # View with conversation history
  aipy --view 3740590a --history          # Show leading conversation
  aipy --view 3740590a --full-history     # Show complete thread
  aipy --view 3740590a --trace            # Show UUID/timestamp flow

  # Delete conversations
  aipy --delete 3740590a
  aipy --delete 3740590a -y               # Skip confirmation

  CONVERSATION TAGGING
  aipy --tags "work,python,debugging" "Help fix my code"
  aipy --tags "research,ai" "Explain transformers"

 CONTEXT THREADING (supports partial UUIDs)
  aipy --chat-id 3740590a "Continue our discussion"
  aipy --chat-id 3740590a --chat-id 339e0ffc "Build on both conversations"
  aipy --chat-id 3740590a --tags "synthesis" "Synthesize our discussion"

 TRACKING AND MONITORING
  aipy --show-id "New conversation"        # Shows UUID on stderr
  aipy --summary "Another conversation"    # Shows UUID in summary
  aipy --debug "Test with debug info"      # Detailed execution info
  aipy --dry-run "Preview request"         # Show what would be sent

  UTILITY COMMANDS
  aipy -L                                  # List all available models
  aipy --search "*" --limit 20            # Show recent 20 conversations
  aipy --view 3740590a --json             # Output in JSON format


                           WORKFLOW EXAMPLES


 Research Workflow:
  1. aipy --search "topic" | head -5           # Find relevant discussions
  2. aipy --chat-id <uuid> --tags "research" "Expand on this topic"
  3. aipy --view <new-uuid> --history          # Review conversation thread

 Code Review Workflow:
  1. aipy -f code.py --tags "review" "Review this code"
  2. aipy --chat-id <uuid> --tags "follow-up" "Implement the suggestions"
  3. aipy --view <uuid> --full-history         # See complete review process

 Decision Making:
  aipy --model "gpt-4o,claude-3-5-sonnet,grok-3" --pick-best "Complex decision"

For more information, see: ~/Documents/notes/02_areas/repos/dotfiles/aipy.md
        """
    )
    
    # Positional argument
    parser.add_argument("prompt", nargs="?", 
                        help="The prompt to send to the AI")
    
    #  CORE OPTIONS 
    core_group = parser.add_argument_group(" Core Options")
    core_group.add_argument("--model", default="grok-3", metavar="MODEL",
                           help="Model(s) to use. Comma-separated for parallel execution (default: grok-3)")
    core_group.add_argument("--provider", choices=["claudpy", "grokpy", "geminpy", "openpy", "ollampy", "perpy"],
                           help="Force specific provider (overrides model-based routing)")
    core_group.add_argument("--prompt", dest="prompt_flag", metavar="TEXT",
                           help="Prompt to prepend to the input")
    core_group.add_argument("--times", type=int, default=1, metavar="N",
                           help="Number of times to repeat request (useful for image generation)")
    core_group.add_argument("-L", "--list-models", action="store_true",
                           help="List all available models and exit")
    
    #  INPUT OPTIONS   
    input_group = parser.add_argument_group(" Input Options")
    input_group.add_argument("-f", "--file", action="append", dest="files", metavar="FILE",
                            help="Include file content (can be used multiple times)")
    input_group.add_argument("--image", action="append", dest="images", metavar="IMAGE",
                            help="Include image files for vision analysis")
    
    #  GENERATION MODES 
    gen_group = parser.add_argument_group(" Generation Modes")
    gen_group.add_argument("--image-gen", action="store_true",
                          help="Generate images instead of text")
    gen_group.add_argument("--embedding", action="store_true",
                          help="Generate text embeddings")
    gen_group.add_argument("--regen", action="store_true",
                          help="Regenerate/enhance existing images")
    gen_group.add_argument("--output", metavar="FILE",
                          help="Output filename for generated content")
    gen_group.add_argument("--size", metavar="SIZE",
                          help="Image size (format depends on provider)")
    gen_group.add_argument("--hd", action="store_true",
                          help="Use HD quality for image generation")
    
    #  MULTI-MODEL OPTIONS 
    multi_group = parser.add_argument_group(" Multi-Model Options")
    multi_group.add_argument("--pick-best", action="store_true",
                            help="Use reasoning model to pick best response from multiple models")
    multi_group.add_argument("--decision-model", default=DEFAULT_DECISION_MODEL, metavar="MODEL",
                            help=f"Model for picking best response (default: {DEFAULT_DECISION_MODEL})")
    
    #  DATABASE OPERATIONS 
    db_group = parser.add_argument_group(" Database Operations")
    db_group.add_argument("--search", metavar="QUERY",
                         help="Search AI chat history in database")
    db_group.add_argument("--view", metavar="UUID",
                         help="View AI chat by UUID (supports 8-char partial UUIDs)")
    db_group.add_argument("--delete", metavar="UUID",
                         help="Delete AI chat by UUID (supports 8-char partial UUIDs)")
    db_group.add_argument("--content-only", action="store_true",
                         help="When viewing, show only content in clean markdown format")
    db_group.add_argument("--limit", type=int, default=10, metavar="N",
                         help="Limit number of search results (default: 10)")
    
    #  HISTORY & CONTEXT 
    context_group = parser.add_argument_group(" History & Context")
    context_group.add_argument("--chat-id", action="append", dest="context_ids", metavar="UUID",
                              help="Include previous chat as context (supports 8-char partial UUIDs)")
    context_group.add_argument("--tags", metavar="TAGS",
                              help="Tag conversation with comma-separated tags")
    context_group.add_argument("--history", action="store_true",
                              help="Show conversation history leading to this chat")
    context_group.add_argument("--full-history", action="store_true",
                              help="Show complete conversation thread including forward refs")
    context_group.add_argument("--trace", action="store_true",
                              help="Show conversation flow as UUID/timestamp trace")
    context_group.add_argument("--use-context", action="store_true",
                              help="Include today's chat history")
    context_group.add_argument("--use-context-from", metavar="DATE",
                              help="Include chat history from date (YYYY-MM-DD)")
    
    #  CONTEXT MANAGEMENT   
    context_mgmt_group = parser.add_argument_group("  Context Management")
    context_mgmt_group.add_argument("--view-context", metavar="CONTEXT",
                                   help="View entire conversation context (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--by-chat-uuid", action="store_true",
                                   help="View context by chat UUID (use with --view-context)")
    context_mgmt_group.add_argument("--edit-context", metavar="CONTEXT", 
                                   help="Edit context properties (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--name", metavar="NAME",
                                   help="Set context name (use with --edit-context)")
    context_mgmt_group.add_argument("--summarize-context", metavar="CONTEXT",
                                   help="Generate summary for context (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--generate-context-name", metavar="CONTEXT",
                                   help="Generate name for context based on summary (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--summarize-contexts-missing", action="store_true",
                                   help="Summarize all contexts that don't have summaries")
    context_mgmt_group.add_argument("--summarize-contexts-outdated", action="store_true",
                                   help="Update summaries for contexts with newer chats")
    context_mgmt_group.add_argument("--search-contexts", metavar="QUERY",
                                   help="Search contexts by name, summary content, or tags")
    context_mgmt_group.add_argument("--context-id", metavar="CONTEXT",
                                   help="Add new chat to specific context (UUID, 8-char partial UUID, or name)")
    
    #  SEARCH FILTERS 
    filter_group = parser.add_argument_group(" Search Filters")
    filter_group.add_argument("--provider-filter", metavar="PROVIDER",
                             help="Filter search results by provider")
    filter_group.add_argument("--model-filter", metavar="MODEL",
                             help="Filter search results by model")
    filter_group.add_argument("--tags-filter", metavar="TAGS",
                             help="Filter search results by tags (comma-separated)")
    
    #  OUTPUT & DEBUG 
    output_group = parser.add_argument_group(" Output & Debug")
    output_group.add_argument("--json", action="store_true",
                             help="Return response in JSON format")
    output_group.add_argument("--summary", action="store_true",
                             help="Show usage summary after execution")
    output_group.add_argument("--show-id", action="store_true",
                             help="Display UUID of newly created chat")
    output_group.add_argument("--debug", action="store_true",
                             help="Enable detailed debug output")
    output_group.add_argument("--dry-run", action="store_true",
                             help="Show what would be sent without executing")
    output_group.add_argument("-S", "--no-streaming", action="store_true",
                             help="Disable streaming output")
    output_group.add_argument("--no-color", action="store_true",
                             help="Disable colored output")
    
    #  ADVANCED OPTIONS 
    advanced_group = parser.add_argument_group("  Advanced Options")
    advanced_group.add_argument("--personality", metavar="ROLE",
                               help="Set AI personality/role")
    advanced_group.add_argument("--no-preserve", action="store_true",
                               help="Don't save chat transaction to database or files")
    advanced_group.add_argument("-y", "--yes", action="store_true",
                               help="Skip confirmation prompts (for delete operations)")
    advanced_group.add_argument("--serve", action="store_true",
                               help="Start web interface for browsing contexts and chats")
    advanced_group.add_argument("--endpoint", metavar="URL",
                               help="Ollama API endpoint")
    advanced_group.add_argument("--profile-performance", action="store_true",
                               help="Profile Ollama model performance")
    advanced_group.add_argument("--profile-prompt", metavar="TEXT",
                               help="Custom prompt for performance profiling")
    advanced_group.add_argument("--profile-models", metavar="MODELS",
                               help="Specific models to profile")
    advanced_group.add_argument("--profile-show-output", action="store_true",
                               help="Show model outputs in performance table")
    
    return parser.parse_args()

def run_provider_command(provider, args, model=None, iteration=None, stdin_input=None):
    """Run a command with the specified provider."""
    cmd = [provider]
    
    # Add model if specified and different from default
    if model and model != "grok-3":
        cmd.extend(["--model", model])
    
    # Add all the flags and options
    if args.debug:
        cmd.append("--debug")
    if args.json:
        cmd.append("--json")
    if args.no_streaming:
        cmd.append("--no-streaming")
    if args.no_color:
        cmd.append("--no-color")
    if args.summary:
        cmd.append("--summary")
    if args.dry_run:
        cmd.append("--dry-run")
    if args.embedding:
        cmd.append("--embedding")
    if args.image_gen:
        cmd.append("--image-gen")
    if args.regen:
        cmd.append("--regen")
    if args.hd:
        cmd.append("--hd")
    if args.use_context:
        cmd.append("--use-context")
    # Always pass --no-preserve to providers since aipy handles preservation
    cmd.append("--no-preserve")
    if args.list_models:
        cmd.append("--list-models")
    if args.profile_performance:
        cmd.append("--profile-performance")
    if args.profile_show_output:
        cmd.append("--profile-show-output")
    
    # Add options with values
    if args.files:
        for file in args.files:
            cmd.extend(["-f", file])
    if args.images:
        for image in args.images:
            cmd.extend(["--image", image])
    if args.output:
        # For multiple iterations, modify output filename
        if iteration is not None and args.times > 1:
            output_path = Path(args.output)
            stem = output_path.stem
            suffix = output_path.suffix
            new_output = f"{stem}-{iteration:03d}{suffix}"
            cmd.extend(["--output", new_output])
        else:
            cmd.extend(["--output", args.output])
    if args.size:
        cmd.extend(["--size", args.size])
    if args.use_context_from:
        cmd.extend(["--use-context-from", args.use_context_from])
    if args.personality:
        cmd.extend(["--personality", args.personality])
    if args.endpoint:
        cmd.extend(["--endpoint", args.endpoint])
    if args.profile_prompt:
        cmd.extend(["--profile-prompt", args.profile_prompt])
    if args.profile_models:
        cmd.extend(["--profile-models", args.profile_models])
    
    # Add --prompt if provided via flag (not positional) and not being passed via stdin
    if args.prompt_flag and not stdin_input and not args.list_models:
        cmd.extend(["--prompt", args.prompt_flag])
    
    return cmd, stdin_input

def execute_command(cmd_and_stdin, model=None, iteration=None, capture_output=False):
    """Execute a command and return the result."""
    try:
        if isinstance(cmd_and_stdin, tuple):
            cmd, stdin_input = cmd_and_stdin
        else:
            cmd, stdin_input = cmd_and_stdin, None
            
        if iteration is not None:
            print(f"[{model or 'default'} - Iteration {iteration + 1}]", file=sys.stderr)
        elif model:
            print(f"[{model}]", file=sys.stderr)
        
        # If we have stdin input, pass it to the command
        if stdin_input:
            result = subprocess.run(cmd, input=stdin_input, text=True, capture_output=capture_output)
        else:
            result = subprocess.run(cmd, capture_output=capture_output, text=True)
            
        response = {
            "success": result.returncode == 0,
            "returncode": result.returncode,
            "model": model,
            "iteration": iteration,
            "cmd": " ".join(cmd) if isinstance(cmd, list) else str(cmd)
        }
        
        if capture_output:
            response["output"] = result.stdout
            if result.stderr:
                response["stderr"] = result.stderr
                
        return response
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "model": model,
            "iteration": iteration,
            "cmd": " ".join(cmd) if isinstance(cmd, (list, tuple)) else str(cmd)
        }

def list_all_models():
    """List models from all providers."""
    providers = ["claudpy", "grokpy", "geminpy", "openpy", "ollampy", "perpy"]
    
    print(" All Available AI Models\n")
    print("=" * 80)
    
    for provider in providers:
        print(f"\n {provider.upper()}")
        print("-" * 40)
        
        try:
            # Run the list-models command for each provider
            result = subprocess.run([provider, "--list-models"], 
                                    capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                # Clean up the output to remove extra formatting
                output = result.stdout.strip()
                if output:
                    print(output)
                else:
                    print("  No models returned")
            else:
                print(f"  Error: {result.stderr.strip()}")
        except subprocess.TimeoutExpired:
            print("  Timeout - provider not responding")
        except FileNotFoundError:
            print(f"  Provider '{provider}' not found in PATH")
        except Exception as e:
            print(f"  Error: {e}")
    
    print("\n" + "=" * 80)
    print(" Usage: aipy --model <model-name> \"your prompt\"")
    print(" Multiple models: aipy --model model1,model2 \"your prompt\"")
    print(" Model routing is automatic based on the model name")

def start_web_server(debug=False, host='127.0.0.1', port=5000):
    """Start the web interface for browsing contexts and chats."""
    try:
        from flask import Flask, render_template_string, request, jsonify, redirect, url_for
        import markdown
        import html
    except ImportError:
        print("Error: Flask and markdown are required for web server mode", file=sys.stderr)
        print("Install with: pip install flask markdown", file=sys.stderr)
        return False
    
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available for web server", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    app = Flask(__name__)
    app.config['SECRET_KEY'] = 'aipy-web-interface-secret'
    
    # HTML Template with HTMX and Dark Mode
    HTML_TEMPLATE = '''
<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIPy - Context Browser</title>
    <script src="https://unpkg.com/htmx.org@1.9.10"></script>
    <style>
        :root[data-theme="light"] {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --bg-tertiary: #e9ecef;
            --text-primary: #212529;
            --text-secondary: #6c757d;
            --border: #dee2e6;
            --accent: #0d6efd;
            --accent-hover: #0b5ed7;
            --success: #198754;
            --warning: #fd7e14;
            --danger: #dc3545;
        }
        
        :root[data-theme="dark"] {
            --bg-primary: #0d1117;
            --bg-secondary: #161b22;
            --bg-tertiary: #21262d;
            --text-primary: #f0f6fc;
            --text-secondary: #8b949e;
            --border: #30363d;
            --accent: #58a6ff;
            --accent-hover: #388bfd;
            --success: #238636;
            --warning: #f85149;
            --danger: #f85149;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Noto Sans', Helvetica, Arial, sans-serif;
            background-color: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px 0;
            border-bottom: 1px solid var(--border);
            margin-bottom: 30px;
        }
        
        .logo {
            font-size: 24px;
            font-weight: bold;
            color: var(--accent);
        }
        
        .theme-toggle {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            color: var(--text-primary);
            padding: 8px 12px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
        }
        
        .theme-toggle:hover {
            background: var(--bg-tertiary);
        }
        
        .search-bar {
            width: 100%;
            padding: 12px 16px;
            border: 1px solid var(--border);
            border-radius: 6px;
            background: var(--bg-secondary);
            color: var(--text-primary);
            font-size: 16px;
            margin-bottom: 20px;
        }
        
        .search-bar:focus {
            outline: none;
            border-color: var(--accent);
            box-shadow: 0 0 0 3px rgba(88, 166, 255, 0.1);
        }
        
        .main-content {
            display: grid;
            grid-template-columns: 350px 1fr;
            gap: 30px;
        }
        
        .sidebar {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 20px;
            height: fit-content;
        }
        
        .content-area {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 20px;
            min-height: 600px;
        }
        
        .context-item {
            padding: 12px;
            margin-bottom: 8px;
            border: 1px solid var(--border);
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
        }
        
        .context-item:hover {
            background: var(--bg-tertiary);
            border-color: var(--accent);
        }
        
        .context-item.active {
            background: var(--accent);
            color: white;
            border-color: var(--accent);
        }
        
        .context-name {
            font-weight: 600;
            margin-bottom: 4px;
        }
        
        .context-meta {
            font-size: 12px;
            color: var(--text-secondary);
        }
        
        .context-item.active .context-meta {
            color: rgba(255, 255, 255, 0.8);
        }
        
        .chat-thread {
            border: 1px solid var(--border);
            border-radius: 6px;
            margin-bottom: 20px;
            overflow: hidden;
        }
        
        .chat-header {
            background: var(--bg-tertiary);
            padding: 12px 16px;
            border-bottom: 1px solid var(--border);
            font-weight: 600;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .chat-content {
            padding: 16px;
        }
        
        .chat-prompt, .chat-response {
            margin-bottom: 16px;
        }
        
        .chat-prompt-label, .chat-response-label {
            font-weight: 600;
            margin-bottom: 8px;
            color: var(--accent);
        }
        
        .chat-text {
            background: var(--bg-primary);
            padding: 12px;
            border-radius: 4px;
            border: 1px solid var(--border);
            white-space: pre-wrap;
            font-family: inherit;
        }
        
        .context-summary {
            background: var(--bg-tertiary);
            padding: 16px;
            border-radius: 6px;
            margin-bottom: 20px;
            border: 1px solid var(--border);
        }
        
        .summary-header {
            font-weight: 600;
            margin-bottom: 8px;
            color: var(--accent);
        }
        
        .action-buttons {
            display: flex;
            gap: 8px;
            margin-top: 12px;
        }
        
        .btn {
            padding: 6px 12px;
            border: 1px solid var(--border);
            border-radius: 4px;
            background: var(--bg-secondary);
            color: var(--text-primary);
            cursor: pointer;
            font-size: 12px;
            text-decoration: none;
            display: inline-block;
        }
        
        .btn:hover {
            background: var(--bg-tertiary);
        }
        
        .btn.primary {
            background: var(--accent);
            color: white;
            border-color: var(--accent);
        }
        
        .btn.primary:hover {
            background: var(--accent-hover);
        }
        
        .loading {
            opacity: 0.6;
            pointer-events: none;
        }
        
        .empty-state {
            text-align: center;
            padding: 60px 20px;
            color: var(--text-secondary);
        }
        
        .empty-state h3 {
            margin-bottom: 8px;
        }
        
        .markdown-content {
            line-height: 1.6;
        }
        
        .markdown-content h1, .markdown-content h2, .markdown-content h3 {
            margin-top: 24px;
            margin-bottom: 12px;
            color: var(--text-primary);
        }
        
        .markdown-content h1 {
            border-bottom: 1px solid var(--border);
            padding-bottom: 8px;
        }
        
        .markdown-content h2 {
            border-bottom: 1px solid var(--border);
            padding-bottom: 4px;
        }
        
        .markdown-content p {
            margin-bottom: 12px;
        }
        
        .markdown-content pre {
            background: var(--bg-primary);
            padding: 12px;
            border-radius: 6px;
            border: 1px solid var(--border);
            overflow-x: auto;
            margin: 12px 0;
        }
        
        .markdown-content code {
            background: var(--bg-primary);
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'SF Mono', Monaco, Inconsolata, 'Roboto Mono', 'Fira Code', monospace;
        }
        
        .markdown-content blockquote {
            border-left: 4px solid var(--accent);
            padding-left: 16px;
            margin: 12px 0;
            color: var(--text-secondary);
        }
        
        @media (max-width: 768px) {
            .main-content {
                grid-template-columns: 1fr;
            }
            
            .sidebar {
                order: 2;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <div class="logo"> AIPy Context Browser</div>
            <button class="theme-toggle" onclick="toggleTheme()">
                 Dark Mode
            </button>
        </div>
        
        <input type="text" 
               class="search-bar" 
               placeholder="Search contexts by name, summary, or tags..."
               hx-get="/search"
               hx-trigger="input changed delay:300ms"
               hx-target="#context-list"
               name="q">
        
        <div class="main-content">
            <div class="sidebar">
                <div id="context-list" hx-get="/contexts" hx-trigger="load">
                    <div class="loading">Loading contexts...</div>
                </div>
            </div>
            
            <div class="content-area">
                <div id="content-detail">
                    <div class="empty-state">
                        <h3>Welcome to AIPy Context Browser</h3>
                        <p>Select a context from the sidebar to view its details</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <script>
        function toggleTheme() {
            const root = document.documentElement;
            const current = root.getAttribute('data-theme');
            const next = current === 'dark' ? 'light' : 'dark';
            root.setAttribute('data-theme', next);
            
            const button = document.querySelector('.theme-toggle');
            button.textContent = next === 'dark' ? ' Dark Mode' : ' Light Mode';
            
            localStorage.setItem('theme', next);
        }
        
        // Load saved theme
        const savedTheme = localStorage.getItem('theme') || 'dark';
        document.documentElement.setAttribute('data-theme', savedTheme);
        document.querySelector('.theme-toggle').textContent = 
            savedTheme === 'dark' ? ' Dark Mode' : ' Light Mode';
    </script>
</body>
</html>
    '''
    
    @app.route('/')
    def index():
        return render_template_string(HTML_TEMPLATE)
    
    @app.route('/contexts')
    def get_contexts():
        try:
            conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            cursor.execute("""
                SELECT 
                    context_uuid,
                    name,
                    summary,
                    tags,
                    created_at,
                    (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count,
                    (SELECT MAX(request_timestamp) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as last_chat_time
                FROM ai_chat_contexts ctx
                ORDER BY last_chat_time DESC NULLS LAST, created_at DESC
                LIMIT 50
            """)
            
            contexts = cursor.fetchall()
            cursor.close()
            conn.close()
            
            html_content = ""
            for ctx in contexts:
                uuid_short = str(ctx['context_uuid'])[:8]
                name = html.escape(ctx['name'] or '<untitled>')
                chat_count = ctx['chat_count']
                last_chat = ctx['last_chat_time'].strftime('%Y-%m-%d %H:%M') if ctx['last_chat_time'] else 'Never'
                
                html_content += f'''
                <div class="context-item" 
                     hx-get="/context/{ctx['context_uuid']}" 
                     hx-target="#content-detail"
                     onclick="document.querySelectorAll('.context-item').forEach(el => el.classList.remove('active')); this.classList.add('active');">
                    <div class="context-name">{name}</div>
                    <div class="context-meta">{uuid_short}...  {chat_count} chats  {last_chat}</div>
                </div>
                '''
            
            return html_content
            
        except Exception as e:
            return f'<div class="error">Error loading contexts: {e}</div>'
    
    @app.route('/search')
    def search_contexts_web():
        query = request.args.get('q', '').strip()
        if not query:
            return get_contexts()
        
        try:
            conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            if query == '*':
                where_clause = "TRUE"
                params = []
            else:
                search_conditions = [
                    "LOWER(name) LIKE LOWER(%s)",
                    "to_tsvector('english', COALESCE(summary, '')) @@ plainto_tsquery('english', %s)",
                    "tags @> ARRAY[%s] OR %s = ANY(tags)"
                ]
                where_clause = " OR ".join([f"({condition})" for condition in search_conditions])
                params = [f"%{query}%", query, query.lower(), query.lower()]
            
            sql = f"""
            SELECT 
                context_uuid,
                name,
                summary,
                tags,
                created_at,
                (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count,
                (SELECT MAX(request_timestamp) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as last_chat_time
            FROM ai_chat_contexts ctx
            WHERE {where_clause}
            ORDER BY last_chat_time DESC NULLS LAST, created_at DESC
            LIMIT 50
            """
            
            cursor.execute(sql, params)
            contexts = cursor.fetchall()
            cursor.close()
            conn.close()
            
            html_content = ""
            for ctx in contexts:
                uuid_short = str(ctx['context_uuid'])[:8]
                name = html.escape(ctx['name'] or '<untitled>')
                chat_count = ctx['chat_count']
                last_chat = ctx['last_chat_time'].strftime('%Y-%m-%d %H:%M') if ctx['last_chat_time'] else 'Never'
                
                html_content += f'''
                <div class="context-item" 
                     hx-get="/context/{ctx['context_uuid']}" 
                     hx-target="#content-detail"
                     onclick="document.querySelectorAll('.context-item').forEach(el => el.classList.remove('active')); this.classList.add('active');">
                    <div class="context-name">{name}</div>
                    <div class="context-meta">{uuid_short}...  {chat_count} chats  {last_chat}</div>
                </div>
                '''
            
            if not contexts:
                html_content = '<div class="empty-state"><h3>No contexts found</h3><p>Try a different search term</p></div>'
            
            return html_content
            
        except Exception as e:
            return f'<div class="error">Error searching contexts: {e}</div>'
    
    @app.route('/context/<context_uuid>')
    def get_context_detail(context_uuid):
        try:
            conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            # Get context info
            cursor.execute("SELECT * FROM ai_chat_contexts WHERE context_uuid = %s", [context_uuid])
            context = cursor.fetchone()
            
            if not context:
                cursor.close()
                conn.close()
                return '<div class="error">Context not found</div>'
            
            # Get chats
            cursor.execute("""
                SELECT * FROM ai_chats 
                WHERE context_uuid = %s 
                ORDER BY request_timestamp ASC
            """, [context_uuid])
            
            chats = cursor.fetchall()
            cursor.close()
            conn.close()
            
            name = html.escape(context['name'] or '<untitled>')
            uuid_short = str(context['context_uuid'])[:8]
            
            html_content = f'''
            <div class="context-detail">
                <div class="context-header">
                    <h2>{name}</h2>
                    <div class="action-buttons">
                        <button class="btn" 
                                hx-post="/context/{context_uuid}/rename" 
                                hx-prompt="Enter new name:"
                                hx-target="closest .context-detail"
                                hx-swap="outerHTML">
                             Rename
                        </button>
                        <button class="btn" 
                                hx-post="/context/{context_uuid}/generate-name" 
                                hx-target="closest .context-detail"
                                hx-swap="outerHTML">
                             Generate Name
                        </button>
                        <button class="btn" 
                                hx-post="/context/{context_uuid}/summarize" 
                                hx-target="closest .context-detail"
                                hx-swap="outerHTML">
                             Generate Summary
                        </button>
                    </div>
                </div>
                <div class="context-meta">
                    <strong>UUID:</strong> {context['context_uuid']}<br>
                    <strong>Created:</strong> {context['created_at'].strftime('%Y-%m-%d %H:%M:%S')}<br>
                    <strong>Chats:</strong> {len(chats)}
                </div>
            '''
            
            # Add summary if available
            if context['summary']:
                summary_html = markdown.markdown(context['summary'])
                provider_model = ""
                if context['summary_provider'] and context['summary_model']:
                    provider_model = f" (Generated by {context['summary_provider']}/{context['summary_model']})"
                
                html_content += f'''
                <div class="context-summary">
                    <div class="summary-header"> Summary{provider_model}</div>
                    <div class="markdown-content">{summary_html}</div>
                </div>
                '''
            
            # Add chats
            for i, chat in enumerate(chats, 1):
                chat_id_short = str(chat['id'])[:8]
                timestamp = chat['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')
                
                html_content += f'''
                <div class="chat-thread">
                    <div class="chat-header">
                        <span>Chat {i} - {chat['provider']}/{chat['model']}</span>
                        <span>{timestamp} ({chat_id_short}...)</span>
                    </div>
                    <div class="chat-content">
                        <div class="chat-prompt">
                            <div class="chat-prompt-label"> User:</div>
                            <div class="chat-text">{html.escape(chat['prompt'])}</div>
                        </div>
                        <div class="chat-response">
                            <div class="chat-response-label"> Assistant:</div>
                            <div class="chat-text markdown-content">{markdown.markdown(chat['response'])}</div>
                        </div>
                    </div>
                </div>
                '''
            
            html_content += '</div>'
            return html_content
            
        except Exception as e:
            return f'<div class="error">Error loading context: {e}</div>'
    
    @app.route('/context/<context_uuid>/rename', methods=['POST'])
    def rename_context(context_uuid):
        new_name = request.headers.get('HX-Prompt')
        if not new_name:
            return get_context_detail(context_uuid)
        
        try:
            conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET name = %s 
                WHERE context_uuid = %s
            """, [new_name, context_uuid])
            
            conn.commit()
            cursor.close()
            conn.close()
            
            # Return context detail with out-of-band sidebar update
            main_content = get_context_detail(context_uuid)
            sidebar_content = get_contexts()
            
            return main_content + f'<div id="context-list" hx-swap-oob="true">{sidebar_content}</div>'
            
        except Exception as e:
            return f'<div class="error">Error renaming context: {e}</div>'
    
    @app.route('/context/<context_uuid>/generate-name', methods=['POST'])
    def generate_name_web(context_uuid):
        success = generate_context_name(context_uuid, debug=debug)
        # Return context detail with out-of-band sidebar update
        main_content = get_context_detail(context_uuid)
        sidebar_content = get_contexts()
        
        return main_content + f'<div id="context-list" hx-swap-oob="true">{sidebar_content}</div>'
    
    @app.route('/context/<context_uuid>/summarize', methods=['POST'])
    def summarize_web(context_uuid):
        success = summarize_context(context_uuid, debug=debug)
        # Return context detail with out-of-band sidebar update
        main_content = get_context_detail(context_uuid)
        sidebar_content = get_contexts()
        
        return main_content + f'<div id="context-list" hx-swap-oob="true">{sidebar_content}</div>'
    
    print(f" Starting AIPy web server on http://{host}:{port}")
    print(" Use Ctrl+C to stop the server")
    
    try:
        app.run(host=host, port=port, debug=debug)
    except KeyboardInterrupt:
        print("\n Shutting down web server...")
    except Exception as e:
        print(f"Error starting web server: {e}", file=sys.stderr)
        return False
    
    return True

def main():
    args = parse_arguments()
    
    # Handle web server mode
    if args.serve:
        start_web_server(debug=args.debug)
        return
    
    # Handle database operations first
    if args.search:
        tags_filter = None
        if args.tags_filter:
            tags_filter = [tag.strip() for tag in args.tags_filter.split(',') if tag.strip()]
        
        search_ai_chats(
            args.search, 
            limit=args.limit,
            provider_filter=args.provider_filter,
            model_filter=args.model_filter,
            tags_filter=tags_filter,
            output_format='json' if args.json else 'text',
            debug=args.debug
        )
        return
    
    if args.search_contexts:
        search_contexts(
            args.search_contexts,
            limit=args.limit,
            output_format='json' if args.json else 'text',
            debug=args.debug
        )
        return
    
    if args.view:
        view_ai_chat(
            args.view,
            output_format='json' if args.json else 'text',
            debug=args.debug,
            show_history=args.history,
            show_full_history=args.full_history,
            trace_mode=args.trace,
            content_only=args.content_only
        )
        return
    
    if args.delete:
        delete_ai_chat(
            args.delete,
            confirm=args.yes,
            debug=args.debug
        )
        return
    
    if args.view_context:
        view_context(
            args.view_context,
            output_format='json' if args.json else 'text',
            debug=args.debug,
            content_only=args.content_only,
            by_chat_uuid=args.by_chat_uuid
        )
        return
    
    if args.edit_context:
        edit_context(
            args.edit_context,
            name=args.name,
            debug=args.debug
        )
        return
    
    if args.summarize_context:
        summarize_context(
            args.summarize_context,
            model=args.model,
            debug=args.debug
        )
        return
    
    if args.generate_context_name:
        generate_context_name(
            args.generate_context_name,
            model=args.model,
            debug=args.debug
        )
        return
    
    if args.summarize_contexts_missing:
        summarize_contexts_missing(
            model=args.model,
            debug=args.debug
        )
        return
    
    if args.summarize_contexts_outdated:
        summarize_contexts_outdated(
            model=args.model,
            debug=args.debug
        )
        return
    
    # Handle list-models specially
    if args.list_models:
        list_all_models()
        return
    
    # Check for stdin data
    stdin_data = ""
    if not sys.stdin.isatty() or sys.stdin in select.select([sys.stdin], [], [], 0)[0]:
        try:
            stdin_data = sys.stdin.read()
        except:
            pass  # If reading fails, continue without stdin data
    
    # Validate that we have some form of input for non-listing operations
    if not args.prompt and not args.prompt_flag and not stdin_data and not args.list_models:
        print("Error: No prompt provided. Use positional argument, --prompt flag, or stdin.", file=sys.stderr)
        sys.exit(1)
    
    # Load context from previous chats if requested
    context_chats = []
    if args.context_ids:
        context_chats = get_chat_context(args.context_ids, debug=args.debug)
        if context_chats is None:
            # Error occurred (likely duplicate partial UUIDs)
            sys.exit(1)
    
    # Combine all input sources like grokpy does
    combined_parts = []
    
    # Add context from previous chats first
    if context_chats:
        context_text = format_context_for_prompt(context_chats)
        combined_parts.append(context_text)
    
    # Add file contents if any
    if args.files:
        for file_path in args.files:
            try:
                with open(file_path, 'r') as f:
                    file_content = f.read()
                    combined_parts.append(f"=== File: {file_path} ===\n{file_content}\n")
            except IOError as e:
                print(f"Warning: Could not read file {file_path}: {e}", file=sys.stderr)
    
    # Add --prompt flag content if provided
    if args.prompt_flag:
        combined_parts.append(args.prompt_flag)
    
    # Add positional prompt if provided
    if args.prompt:
        combined_parts.append(args.prompt)
    
    # Add stdin content if any
    if stdin_data:
        combined_parts.append(stdin_data)
    
    # Combine all parts
    full_prompt = "\n".join(combined_parts) if combined_parts else ""
    
    # Store original prompt for --prompt flag
    args.original_prompt = args.prompt
    
    # Parse tags if provided
    tags = []
    if args.tags:
        tags = [tag.strip() for tag in args.tags.split(',') if tag.strip()]
    
    # Automatically add 'follow-up' tag when using context
    if args.context_ids:
        if 'follow-up' not in tags:
            tags.append('follow-up')
    
    # Convert back to None if empty for consistency
    tags = tags if tags else None
    
    # Parse models (can be comma-separated)
    models = [m.strip() for m in args.model.split(",") if m.strip()]
    
    # Prepare all commands to execute
    commands = []
    
    for model in models:
        # Determine provider
        if args.provider:
            provider = args.provider
        else:
            provider = get_provider_for_model(model)
        
        # Generate commands for multiple iterations
        for iteration in range(args.times):
            cmd_and_stdin = run_provider_command(provider, args, model, 
                                                 iteration if args.times > 1 else None, 
                                                 full_prompt if full_prompt else None)
            commands.append((cmd_and_stdin, model, iteration if args.times > 1 else None))
    
    # Track execution time
    start_time = time.time()
    
    # Execute commands
    if len(commands) == 1 and args.times == 1:
        # Single command - run directly for better interactive experience
        cmd_and_stdin, model, iteration = commands[0]
        cmd, stdin_input = cmd_and_stdin
        
        if args.debug:
            print(f"Debug: Executing: {' '.join(cmd)}", file=sys.stderr)
            if stdin_input:
                print(f"Debug: With stdin input: {len(stdin_input)} characters", file=sys.stderr)
        
        try:
            if stdin_input:
                result = subprocess.run(cmd, input=stdin_input, text=True, capture_output=True)
            else:
                result = subprocess.run(cmd, capture_output=True, text=True)
            
            # Print output to user
            if result.stdout:
                print(result.stdout, end='')
            if result.stderr:
                print(result.stderr, file=sys.stderr, end='')
            
            # Save transaction if not disabled
            if not args.no_preserve and result.returncode == 0:
                end_time = time.time()
                provider = get_provider_for_model(model)
                
                # For single model, also store directly in DB
                if not args.dry_run:
                    db_metadata = {
                        'request_timestamp': datetime.fromtimestamp(start_time),
                        'response_timestamp': datetime.fromtimestamp(end_time),
                        'duration_ms': int((end_time - start_time) * 1000),
                        'extra_metadata': {
                            'source': 'aipy_single_model',
                            'execution_type': 'single',
                            'iterations': 1
                        }
                    }
                    
                    new_id = store_chat_in_db(
                        full_prompt, 
                        result.stdout, 
                        provider, 
                        model, 
                        db_metadata,
                        tags=tags,
                        context_ids=args.context_ids,
                        context_id=args.context_id,
                        debug=args.debug, 
                        dry_run=args.dry_run,
                        show_id=args.show_id
                    )
                
                save_multi_model_transaction(
                    [model], 
                    full_prompt, 
                    [{"success": True, "output": result.stdout}],
                    {"total_time": end_time - start_time, "parallel": False, "iterations": 1, "new_chat_ids": [new_id] if new_id else []},
                    tags=tags,
                    context_ids=args.context_ids,
                    context_id=args.context_id,
                    store_db=False,  # Already stored above
                    debug=args.debug,
                    dry_run=args.dry_run,
                    show_id=args.show_id
                )
                
                # Show summary for single model if requested
                if args.summary and new_id:
                    print(f"\nNew chat ID: {new_id}", file=sys.stderr)
                
            if result.returncode != 0:
                sys.exit(result.returncode)
                
        except KeyboardInterrupt:
            print("\nInterrupted by user", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"Error executing command: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        # Multiple commands - need to capture output for preservation or pick-best
        capture_output = not args.no_preserve or args.pick_best
        
        if len(commands) > 1:
            print(f"Executing {len(commands)} requests in parallel...\n", file=sys.stderr)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(commands), 8)) as executor:
            futures = []
            for cmd_and_stdin, model, iteration in commands:
                cmd, stdin_input = cmd_and_stdin
                if args.debug:
                    print(f"Debug: Queuing: {' '.join(cmd)}", file=sys.stderr)
                    if stdin_input:
                        print(f"Debug: With stdin input: {len(stdin_input)} characters", file=sys.stderr)
                future = executor.submit(execute_command, cmd_and_stdin, model, iteration, capture_output)
                futures.append(future)
            
            # Wait for all to complete
            results = []
            model_order = []
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    model_order.append(result.get("model", "unknown"))
                    
                    # Print output as it arrives (if captured and not using pick-best)
                    if not args.pick_best and result.get("output"):
                        print(f"\n[{result.get('model', 'unknown')}]", file=sys.stderr)
                        print(result["output"], end='')
                    
                    if not result["success"]:
                        error_msg = result.get("error", f"Command failed with return code {result.get('returncode', 'unknown')}")
                        print(f"Error in {result.get('model', 'unknown')}: {error_msg}", file=sys.stderr)
                except Exception as e:
                    print(f"Error executing command: {e}", file=sys.stderr)
        
        # Handle pick-best logic
        best_model = None
        reasoning = None
        if args.pick_best and len(models) > 1:
            # Prepare model responses for pick_best_response
            model_responses = []
            for model in models:
                for result in results:
                    if result.get("model") == model and result.get("success"):
                        model_responses.append((model, result))
                        break
            
            if len(model_responses) > 1:
                print(f"\nAnalyzing {len(model_responses)} responses to pick the best one...", file=sys.stderr)
                best_model, reasoning = pick_best_response(full_prompt, model_responses, args.decision_model)
                
                if best_model:
                    # Find and display the best response
                    for model, result in model_responses:
                        if model == best_model:
                            print(f"\n[Best Response: {best_model}]", file=sys.stderr)
                            print(result["output"], end='')
                            break
                    
                    if args.debug:
                        print(f"\nDecision reasoning: {reasoning}", file=sys.stderr)
                else:
                    print("\nCould not determine best response", file=sys.stderr)
            elif len(model_responses) == 1:
                # Only one successful response, use it
                model, result = model_responses[0]
                print(f"\n[{model}]", file=sys.stderr)
                print(result["output"], end='')
        
        # Summary
        successful = sum(1 for r in results if r["success"])
        print(f"\nCompleted: {successful}/{len(commands)} requests successful", file=sys.stderr)
        
        # Show summary with new chat IDs if requested
        if args.summary and metadata and metadata.get('new_chat_ids'):
            print(f"\nNew chat IDs created:", file=sys.stderr)
            for chat_id in metadata.get('new_chat_ids', []):
                print(f"  {chat_id}", file=sys.stderr)
        
        # Save transaction if not disabled and we have successful results
        if not args.no_preserve and successful > 0:
            end_time = time.time()
            
            # Sort results by model order for consistent output
            sorted_results = []
            sorted_models = []
            for model in models:
                for result in results:
                    if result.get("model") == model and result not in sorted_results:
                        sorted_results.append(result)
                        sorted_models.append(model)
                        break
            
            # Prepare metadata
            metadata = {
                "total_time": end_time - start_time, 
                "parallel": len(commands) > 1, 
                "iterations": args.times
            }
            
            # Add pick-best metadata if applicable
            if args.pick_best and len(models) > 1 and best_model:
                metadata["best_model"] = best_model
                metadata["decision_model"] = args.decision_model
                metadata["decision_reasoning"] = reasoning
            
            save_multi_model_transaction(
                sorted_models, 
                full_prompt, 
                sorted_results,
                metadata,
                tags=tags,
                context_ids=args.context_ids,
                context_id=args.context_id,
                store_db=True,
                debug=args.debug,
                dry_run=args.dry_run,
                show_id=args.show_id
            )

if __name__ == "__main__":
    main()