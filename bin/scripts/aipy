#!/usr/bin/python3

# Container check for distrobox - do this BEFORE any other imports
import os
import subprocess
import sys

ctr_id = os.environ.get("CONTAINER_ID", "")
if ctr_id != "dev":
    cmd = ["distrobox", "enter", "dev", "--", *sys.argv]
    subprocess.run(cmd)
    sys.exit(0)

# Now import everything else inside the dev container
import argparse
import asyncio
import concurrent.futures
import json
import select
import time
import hashlib
from pathlib import Path
from datetime import datetime

# Try to import database dependencies
try:
    import psycopg2
    import psycopg2.extras
    from psycopg2.extras import RealDictCursor
    DB_AVAILABLE = True
except ImportError:
    DB_AVAILABLE = False

# Default decision model for --pick-best feature
DEFAULT_DECISION_MODEL = "claude-opus-4-20250514"

# Database configuration for AI chat storage
AI_CHATS_DB_CONFIG = {
    'host': os.environ.get('AI_CHATS_DB_HOST', '127.0.0.1'),
    'port': int(os.environ.get('AI_CHATS_DB_PORT', '5432')),
    'database': os.environ.get('AI_CHATS_DB_NAME', 'ai_chats'),
    'user': os.environ.get('AI_CHATS_DB_USER', 'postgres'),
    'password': os.environ.get('AI_CHATS_DB_PASSWORD', '')
}

# Model to provider mapping
MODEL_PROVIDERS = {
    # Claude models
    "claude-3-5-sonnet-20241022": "claudpy",
    "claude-3-5-sonnet-latest": "claudpy", 
    "claude-3-5-haiku-20241022": "claudpy",
    "claude-3-5-haiku-latest": "claudpy",
    "claude-3-opus-20240229": "claudpy",
    "claude-3-opus-latest": "claudpy",
    "claude-3-sonnet-20240229": "claudpy",
    "claude-3-haiku-20240307": "claudpy",
    "claude-sonnet-4-20250514": "claudpy",
    "claude-opus-4-20250514": "claudpy",
    "claude-3-7-sonnet-latest": "claudpy",
    
    # Grok models
    "grok-3": "grokpy",
    "grok-3-mini": "grokpy", 
    "grok-3-fast": "grokpy",
    "grok-3-mini-fast": "grokpy",
    "grok-2-1212": "grokpy",
    "grok-2-image-1212": "grokpy",
    "grok-2-vision-1212": "grokpy",
    
    # Gemini models
    "gemini-2.5-pro-preview-06-05": "geminpy",
    "gemini-2.5-pro-preview-05-06": "geminpy",
    "gemini-2.5-flash-preview-04-17": "geminpy",
    "gemini-2.0-flash": "geminpy",
    "gemini-2.0-flash-preview-image-generation": "geminpy",
    "gemini-2.0-flash-lite": "geminpy",
    "gemini-1.5-flash": "geminpy",
    "gemini-1.5-flash-8b": "geminpy",
    "gemini-1.5-pro": "geminpy",
    "gemini-embedding-exp": "geminpy",
    "imagen-3.0-generate-002": "geminpy",
    "veo-2.0-generate-001": "geminpy",
    "gemini-2.0-flash-live-001": "geminpy",
    "text-embedding-004": "geminpy",
    "embedding-001": "geminpy",
    "models/aqa": "geminpy",
    
    # OpenAI models
    "gpt-4o": "openpy",
    "gpt-4-turbo": "openpy",
    "gpt-4": "openpy",
    "gpt-4-32k": "openpy",
    "gpt-4o-mini": "openpy",
    "gpt-3.5-turbo": "openpy",
    "gpt-3.5-turbo-16k": "openpy",
    "dall-e-3": "openpy",
    "dall-e-2": "openpy",
    "whisper-1": "openpy",
    "text-embedding-3-small": "openpy",
    "text-embedding-3-large": "openpy",
    "text-embedding-ada-002": "openpy",
    
    # Perplexity models
    "sonar-pro": "perpy",
    "sonar": "perpy",
    "reasoning-pro": "perpy",
    "sonar-reasoning-pro": "perpy",
    "r1-1776": "perpy",
}

def get_provider_for_model(model):
    """Get the provider for a given model, or return 'ollampy' as default for unknown models."""
    return MODEL_PROVIDERS.get(model, "ollampy")

def check_db_available(debug=False):
    """Check if the AI chats database is available."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database dependencies not available (psycopg2)", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        conn.close()
        return True
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Could not connect to AI chats database: {e}", file=sys.stderr)
        return False

def store_chat_in_db(prompt, response, provider, model, metadata=None, tags=None, context_ids=None, debug=False, dry_run=False, show_id=False):
    """Store chat interaction in the database."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for chat storage", file=sys.stderr)
        return None
    
    if dry_run:
        print("Would store chat in AI chats database", file=sys.stderr)
        return None
    
    if not check_db_available(debug):
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor()
        
        # Generate hashes for prompt and response
        prompt_hash = hashlib.sha256(prompt.encode('utf-8')).hexdigest()
        response_hash = hashlib.sha256(response.encode('utf-8')).hexdigest()
        
        # Extract metadata
        request_timestamp = metadata.get('request_timestamp') if metadata else datetime.now()
        response_timestamp = metadata.get('response_timestamp') if metadata else datetime.now()
        duration_ms = metadata.get('duration_ms') if metadata else None
        tokens_input = metadata.get('tokens_input') if metadata else None
        tokens_output = metadata.get('tokens_output') if metadata else None
        cost_input_usd = metadata.get('cost_input_usd') if metadata else None
        cost_output_usd = metadata.get('cost_output_usd') if metadata else None
        cost_total_usd = metadata.get('cost_total_usd') if metadata else None
        extra_metadata = metadata.get('extra_metadata', {}) if metadata else {}
        
        # Add context references to metadata for conversation tracking
        if context_ids:
            extra_metadata['context_references'] = context_ids
        
        # Process tags
        tags_array = tags if tags else []
        
        # Insert into database and return the UUID
        cursor.execute("""
            INSERT INTO ai_chats (
                prompt, response, provider, model,
                request_timestamp, response_timestamp, duration_ms,
                tokens_input, tokens_output,
                cost_input_usd, cost_output_usd, cost_total_usd,
                prompt_hash, response_hash, tags, metadata
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING id
        """, (
            prompt, response, provider, model,
            request_timestamp, response_timestamp, duration_ms,
            tokens_input, tokens_output,
            cost_input_usd, cost_output_usd, cost_total_usd,
            prompt_hash, response_hash, tags_array, json.dumps(extra_metadata)
        ))
        
        new_id = cursor.fetchone()[0]
        conn.commit()
        cursor.close()
        conn.close()
        
        if debug:
            print("Chat stored in database successfully", file=sys.stderr)
        
        if show_id:
            print(f"New chat ID: {new_id}", file=sys.stderr)
        
        return str(new_id)
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to store chat in database: {e}", file=sys.stderr)
        return None

def search_ai_chats(query, limit=10, provider_filter=None, model_filter=None, tags_filter=None, output_format='text', debug=False):
    """Search AI chats using full-text search."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Build WHERE clauses
        where_clauses = []
        params = []
        
        # Add text search if query is not a wildcard
        if query and query != "*":
            where_clauses.append(
                "(to_tsvector('english', prompt) @@ plainto_tsquery('english', %s) OR "
                "to_tsvector('english', response) @@ plainto_tsquery('english', %s))"
            )
            params.extend([query, query])
        
        if provider_filter:
            where_clauses.append("provider = %s")
            params.append(provider_filter)
        
        if model_filter:
            where_clauses.append("model = %s")
            params.append(model_filter)
        
        if tags_filter:
            # Support searching for any of the provided tags
            tag_conditions = []
            for tag in tags_filter:
                tag_conditions.append("tags @> %s")
                params.append([tag.strip()])
            if tag_conditions:
                where_clauses.append(f"({' OR '.join(tag_conditions)})")
        
        where_clause = " AND ".join(where_clauses) if where_clauses else "TRUE"
        
        # Search query with ranking
        if query and query != "*":
            sql = f"""
            SELECT 
                id,
                LEFT(prompt, 100) as prompt_snippet,
                LEFT(response, 100) as response_snippet,
                provider,
                model,
                request_timestamp,
                duration_ms,
                cost_total_usd,
                tags,
                ts_rank(
                    to_tsvector('english', prompt || ' ' || response), 
                    plainto_tsquery('english', %s)
                ) as rank
            FROM ai_chats
            WHERE {where_clause}
            ORDER BY rank DESC, request_timestamp DESC
            LIMIT %s
            """
            params.extend([query, limit])
        else:
            sql = f"""
            SELECT 
                id,
                LEFT(prompt, 100) as prompt_snippet,
                LEFT(response, 100) as response_snippet,
                provider,
                model,
                request_timestamp,
                duration_ms,
                cost_total_usd,
                tags,
                0 as rank
            FROM ai_chats
            WHERE {where_clause}
            ORDER BY request_timestamp DESC
            LIMIT %s
            """
            params.append(limit)
        
        cursor.execute(sql, params)
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        if output_format == 'json':
            print(json.dumps([dict(r) for r in results], default=str, indent=2))
        else:
            if not results:
                print("No results found.")
                return True
            
            print(f"\nSearch results for: \"{query}\"")
            print("=" * 50)
            
            for i, r in enumerate(results, 1):
                print(f"\n{i}. [{str(r['id'])}] {r['provider']}/{r['model']}")
                print(f"   Time: {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                if r['duration_ms']:
                    print(f"   Duration: {r['duration_ms']}ms")
                if r['cost_total_usd']:
                    print(f"   Cost: ${r['cost_total_usd']:.4f}")
                if r['tags']:
                    print(f"   Tags: {', '.join(r['tags'])}")
                print(f"   Prompt: {r['prompt_snippet']}{'...' if len(r['prompt_snippet']) >= 100 else ''}")
                print(f"   Response: {r['response_snippet']}{'...' if len(r['response_snippet']) >= 100 else ''}")
            
            print(f"\nFound {len(results)} results.")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error searching AI chats: {e}", file=sys.stderr)
        return False

def view_ai_chat(chat_id, output_format='text', debug=False, show_history=False, show_full_history=False, trace_mode=False):
    """View full AI chat by UUID with optional conversation history."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Check if chat_id is a partial UUID (8 characters)
        if len(chat_id) == 8:
            # Search for UUIDs starting with this prefix
            cursor.execute("""
                SELECT * FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s
            """, [f"{chat_id}%"])
            
            results = cursor.fetchall()
            
            if not results:
                cursor.close()
                conn.close()
                print(f"No chat found with ID starting with: {chat_id}")
                return False
            elif len(results) > 1:
                cursor.close()
                conn.close()
                print(f"Multiple chats found with ID starting with '{chat_id}':")
                for r in results:
                    print(f"  - {r['id']} ({r['provider']}/{r['model']}, {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')})")
                print("Please provide the full UUID to view a specific chat.")
                return False
            else:
                result = results[0]
                chat_id = str(result['id'])  # Use full UUID for history functionality
        else:
            # Try to use as full UUID
            cursor.execute("""
                SELECT * FROM ai_chats WHERE id = %s::uuid
            """, [chat_id])
            
            result = cursor.fetchone()
            
            if not result:
                cursor.close()
                conn.close()
                print(f"No chat found with ID: {chat_id}")
                return False
        
        cursor.close()
        conn.close()
        
        # Handle history reconstruction if requested
        if show_history or show_full_history:
            history = build_conversation_history(chat_id, include_forward=show_full_history, debug=debug)
            if history:
                print(format_conversation_history(history, trace_mode=trace_mode))
                return True
            elif show_history or show_full_history:
                print("No conversation history found. Showing individual chat:")
        
        if output_format == 'json':
            print(json.dumps(dict(result), default=str, indent=2))
        else:
            print("=" * 80)
            print(f"Chat ID: {result['id']}")
            print(f"Provider: {result['provider']}")
            print(f"Model: {result['model']}")
            print(f"Request Time: {result['request_timestamp']}")
            if result['response_timestamp']:
                print(f"Response Time: {result['response_timestamp']}")
            if result['duration_ms']:
                print(f"Duration: {result['duration_ms']}ms")
            if result['tokens_input']:
                print(f"Input Tokens: {result['tokens_input']}")
            if result['tokens_output']:
                print(f"Output Tokens: {result['tokens_output']}")
            if result['cost_total_usd']:
                print(f"Total Cost: ${result['cost_total_usd']:.6f}")
            if result['tags']:
                print(f"Tags: {', '.join(result['tags'])}")
            print("=" * 80)
            print("\nPROMPT:")
            print("-" * 40)
            print(result['prompt'])
            print("\nRESPONSE:")
            print("-" * 40)
            print(result['response'])
            if result['metadata'] and result['metadata'] != '{}':
                print("\nMETADATA:")
                print("-" * 40)
                metadata = json.loads(result['metadata']) if isinstance(result['metadata'], str) else result['metadata']
                print(json.dumps(metadata, indent=2))
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error viewing AI chat: {e}", file=sys.stderr)
        return False

def delete_ai_chat(chat_id, confirm=False, debug=False):
    """Delete AI chat by UUID."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # First, get the chat details
        cursor.execute("""
            SELECT id, LEFT(prompt, 100) as prompt_snippet, provider, model, request_timestamp
            FROM ai_chats WHERE id = %s::uuid
        """, [chat_id])
        
        result = cursor.fetchone()
        
        if not result:
            print(f"No chat found with ID: {chat_id}")
            cursor.close()
            conn.close()
            return False
        
        # Confirm deletion
        if not confirm:
            print("About to delete chat:")
            print(f"  ID: {result['id']}")
            print(f"  Provider/Model: {result['provider']}/{result['model']}")
            print(f"  Time: {result['request_timestamp']}")
            print(f"  Prompt: {result['prompt_snippet']}{'...' if len(result['prompt_snippet']) >= 100 else ''}")
            response = input("Are you sure? (y/N): ")
            if response.lower() != 'y':
                print("Deletion cancelled.")
                cursor.close()
                conn.close()
                return False
        
        # Delete the chat
        cursor.execute("DELETE FROM ai_chats WHERE id = %s::uuid", [chat_id])
        conn.commit()
        
        cursor.close()
        conn.close()
        
        print("Chat deleted successfully.")
        return True
    
    except psycopg2.Error as e:
        print(f"Error deleting AI chat: {e}", file=sys.stderr)
        return False

def get_chat_context(chat_ids, debug=False):
    """Retrieve chat content by UUIDs (full or partial 8-char) for use as context."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for context retrieval", file=sys.stderr)
        return []
    
    if not check_db_available(debug):
        if debug:
            print("Warning: Could not connect to AI chats database for context", file=sys.stderr)
        return []
    
    if not chat_ids:
        return []
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Resolve partial UUIDs to full UUIDs first
        resolved_ids = []
        for chat_id in chat_ids:
            if len(chat_id) == 8:
                # Search for UUIDs starting with this prefix
                cursor.execute("""
                    SELECT id FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s
                """, [f"{chat_id}%"])
                
                matches = cursor.fetchall()
                
                if not matches:
                    print(f"Error: No chat found with ID starting with: {chat_id}", file=sys.stderr)
                    cursor.close()
                    conn.close()
                    return None  # Signal error to caller
                elif len(matches) > 1:
                    print(f"Error: Multiple chats found with ID starting with '{chat_id}':", file=sys.stderr)
                    for match in matches:
                        cursor.execute("""
                            SELECT provider, model, request_timestamp 
                            FROM ai_chats WHERE id = %s::uuid
                        """, [str(match['id'])])
                        details = cursor.fetchone()
                        print(f"  - {match['id']} ({details['provider']}/{details['model']}, {details['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')})", file=sys.stderr)
                    print("Please provide the full UUID to disambiguate.", file=sys.stderr)
                    cursor.close()
                    conn.close()
                    return None  # Signal error to caller
                else:
                    resolved_ids.append(str(matches[0]['id']))
            else:
                # Use as full UUID
                resolved_ids.append(chat_id)
        
        # Now query for all resolved UUIDs
        placeholders = ', '.join(['%s::uuid'] * len(resolved_ids))
        cursor.execute(f"""
            SELECT id, prompt, response, provider, model, request_timestamp, tags
            FROM ai_chats 
            WHERE id IN ({placeholders})
            ORDER BY request_timestamp ASC
        """, resolved_ids)
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        context_chats = []
        found_ids = set()
        
        for result in results:
            found_ids.add(str(result['id']))
            context_chats.append({
                'id': str(result['id']),
                'prompt': result['prompt'],
                'response': result['response'],
                'provider': result['provider'],
                'model': result['model'],
                'timestamp': result['request_timestamp'],
                'tags': result['tags'] or []
            })
        
        # Report missing chats
        missing_ids = set(resolved_ids) - found_ids
        if missing_ids and debug:
            print(f"Warning: Could not find chats: {', '.join(missing_ids)}", file=sys.stderr)
        
        return context_chats
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to retrieve chat context: {e}", file=sys.stderr)
        return []

def format_context_for_prompt(context_chats):
    """Format retrieved chats into context text for the prompt."""
    if not context_chats:
        return ""
    
    context_parts = ["=== CONTEXT FROM PREVIOUS CONVERSATIONS ===\n"]
    
    for i, chat in enumerate(context_chats, 1):
        context_parts.append(f"Context {i} (ID: {chat['id'][:8]}..., {chat['provider']}/{chat['model']}):")
        
        if chat['tags']:
            context_parts.append(f"Tags: {', '.join(chat['tags'])}")
        
        context_parts.append(f"User: {chat['prompt']}")
        context_parts.append(f"Assistant: {chat['response']}")
        context_parts.append("")  # Empty line between contexts
    
    context_parts.append("=== END CONTEXT ===\n")
    
    return "\n".join(context_parts)

def build_conversation_history(chat_id, include_forward=False, debug=False):
    """Build conversation history by following context references."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for history reconstruction", file=sys.stderr)
        return []
    
    if not check_db_available(debug):
        if debug:
            print("Warning: Could not connect to AI chats database for history", file=sys.stderr)
        return []
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get all chats to build the graph
        cursor.execute("""
            SELECT id, prompt, response, provider, model, request_timestamp, tags, metadata
            FROM ai_chats
            ORDER BY request_timestamp ASC
        """)
        
        all_chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        # Build chat lookup and reference graph
        chat_lookup = {}
        references = {}  # chat_id -> list of referenced chat_ids
        referenced_by = {}  # chat_id -> list of chats that reference it
        
        for chat in all_chats:
            chat_id_str = str(chat['id'])
            chat_lookup[chat_id_str] = {
                'id': chat_id_str,
                'prompt': chat['prompt'],
                'response': chat['response'],
                'provider': chat['provider'],
                'model': chat['model'],
                'timestamp': chat['request_timestamp'],
                'tags': chat['tags'] or []
            }
            
            # Extract context references from metadata
            metadata = chat['metadata']
            if isinstance(metadata, str):
                metadata = json.loads(metadata)
            
            context_refs = metadata.get('context_references', []) if metadata else []
            references[chat_id_str] = context_refs
            
            # Build reverse lookup
            for ref_id in context_refs:
                if ref_id not in referenced_by:
                    referenced_by[ref_id] = []
                referenced_by[ref_id].append(chat_id_str)
        
        # Build history chain
        history = []
        visited = set()
        
        def collect_backward_history(current_id):
            """Recursively collect chat history going backwards."""
            if current_id in visited or current_id not in chat_lookup:
                return
            
            visited.add(current_id)
            
            # First, collect all referenced chats (going backward)
            for ref_id in references.get(current_id, []):
                collect_backward_history(ref_id)
            
            # Then add this chat
            history.append(chat_lookup[current_id])
        
        def collect_forward_history(current_id):
            """Recursively collect chats that reference this one (going forward)."""
            if current_id in visited or current_id not in chat_lookup:
                return
            
            visited.add(current_id)
            
            # Add this chat first
            history.append(chat_lookup[current_id])
            
            # Then collect all chats that reference this one (going forward)
            for forward_id in referenced_by.get(current_id, []):
                collect_forward_history(forward_id)
        
        # Start collection
        if include_forward:
            # For full history, we need to find the root of the conversation tree
            # Start by finding all chats that are referenced by our target chat
            target_refs = references.get(chat_id, [])
            
            # Collect backward history for each reference point
            for ref_id in target_refs:
                collect_backward_history(ref_id)
            
            # Add the target chat itself
            if chat_id in chat_lookup and chat_id not in visited:
                history.append(chat_lookup[chat_id])
                visited.add(chat_id)
            
            # Then collect forward references
            for forward_id in referenced_by.get(chat_id, []):
                collect_forward_history(forward_id)
        else:
            # Just backward history to this chat
            collect_backward_history(chat_id)
        
        # Sort by timestamp to maintain chronological order
        history.sort(key=lambda x: x['timestamp'])
        
        return history
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to build conversation history: {e}", file=sys.stderr)
        return []

def format_conversation_history(history, trace_mode=False):
    """Format conversation history for display."""
    if not history:
        return "No conversation history found."
    
    if trace_mode:
        # Show trace format
        output = ["Conversation Flow Trace:"]
        output.append("=" * 40)
        
        for i, chat in enumerate(history):
            timestamp = chat['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            tags_str = f" [{', '.join(chat['tags'])}]" if chat['tags'] else ""
            output.append(f"{i+1:2d}. {chat['id'][:8]}... | {timestamp} | {chat['provider']}/{chat['model']}{tags_str}")
        
        return "\n".join(output)
    else:
        # Show full conversation format
        output = ["Conversation History:"]
        output.append("=" * 80)
        
        for i, chat in enumerate(history):
            timestamp = chat['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            tags_str = f" [{', '.join(chat['tags'])}]" if chat['tags'] else ""
            
            output.append(f"\n[{i+1}] {timestamp} | {chat['provider']}/{chat['model']} | {chat['id'][:8]}...{tags_str}")
            output.append("-" * 80)
            output.append(f"User: {chat['prompt']}")
            output.append(f"\nAssistant: {chat['response']}")
            output.append("")
        
        return "\n".join(output)

def pick_best_response(prompt, model_responses, decision_model):
    """Use a reasoning model to pick the best response from multiple models."""
    # Prepare the decision prompt
    decision_prompt = f"""Original User Query:
{prompt}

I received multiple AI responses to this query. Please analyze them and select the BEST response based on:
1. Accuracy and correctness
2. Completeness and thoroughness
3. Clarity and coherence
4. Relevance to the query
5. Practical usefulness

Here are the responses from different models:

"""
    
    for i, (model, response) in enumerate(model_responses):
        if response.get("success") and response.get("output"):
            decision_prompt += f"=== Response {i+1} from {model} ===\n{response['output']}\n\n"
    
    decision_prompt += """Please analyze all responses and tell me which response number is the BEST (1, 2, 3, etc).
Start your response with "BEST: X" where X is the response number.
Then provide a brief explanation of why that response is superior to the others.
"""
    
    # Get the provider for the decision model
    provider = get_provider_for_model(decision_model)
    
    # Build command for decision model
    cmd = [provider, "--model", decision_model, "--no-preserve", decision_prompt]
    
    try:
        # Run the decision model
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0 and result.stdout:
            # Parse the response to find the best model
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if line.strip().startswith("BEST:"):
                    try:
                        # Extract the number
                        best_num = int(line.split(":")[1].strip().split()[0]) - 1
                        if 0 <= best_num < len(model_responses):
                            best_model = model_responses[best_num][0]
                            reasoning = '\n'.join(lines[1:])  # Rest is explanation
                            return best_model, reasoning.strip()
                    except:
                        pass
            
        # If parsing fails, return the first successful response
        for model, response in model_responses:
            if response.get("success"):
                return model, "Could not determine best response, defaulting to first successful model"
                
    except Exception as e:
        print(f"Error running decision model: {e}", file=sys.stderr)
        # Return first successful response as fallback
        for model, response in model_responses:
            if response.get("success"):
                return model, "Decision model failed, defaulting to first successful model"
    
    return None, "No successful responses to choose from"

def save_multi_model_transaction(models, prompt, responses, metadata=None, tags=None, context_ids=None, store_db=True, debug=False, dry_run=False, show_id=False):
    """Save multi-model chat transaction to markdown file organized by date and optionally to database."""
    # Create directory structure if it doesn't exist
    base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
    base_path.mkdir(parents=True, exist_ok=True)
    
    # Generate filename based on today's date
    today = datetime.now()
    filename = today.strftime("%Y-%m-%d.md")
    file_path = base_path / filename
    
    # Generate timestamp for section header
    timestamp = today.strftime("%Y-%m-%d %H:%M:%S")
    
    # Prepare content
    content = f"\n# {timestamp} - Multi-Model Query (aipy)\n\n"
    content += f"## Prompt\n\n```\n{prompt}\n```\n\n"
    
    # Add model responses
    content += f"## Model Responses\n\n"
    
    for i, (model, response) in enumerate(zip(models, responses)):
        content += f"### {i+1}. {model}\n\n"
        if response.get("success"):
            content += f"{response.get('output', 'No output captured')}\n\n"
        else:
            content += f"**Error**: {response.get('error', 'Unknown error')}\n\n"
    
    # Add metadata if provided
    if metadata:
        content += f"## Metadata\n\n"
        content += f"- **Provider**: aipy (multi-model wrapper)\n"
        content += f"- **Models**: {', '.join(models)}\n"
        if 'total_time' in metadata:
            content += f"- **Total Time**: {metadata['total_time']:.2f}s\n"
        if 'parallel' in metadata:
            content += f"- **Execution**: {'Parallel' if metadata['parallel'] else 'Sequential'}\n"
        if 'iterations' in metadata:
            content += f"- **Iterations**: {metadata['iterations']}\n"
        if 'best_model' in metadata:
            content += f"- **Best Response**: {metadata['best_model']}\n"
        if 'decision_model' in metadata:
            content += f"- **Decision Model**: {metadata['decision_model']}\n"
        if 'decision_reasoning' in metadata:
            content += f"\n### Decision Reasoning\n\n{metadata['decision_reasoning']}\n"
    
    content += "\n---\n"
    
    # Write to file (append if exists)
    try:
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        print(f"Warning: Could not save chat transaction: {e}", file=sys.stderr)
    
    # Store in database if requested and available
    if store_db:
        # For multi-model responses, store each successful response separately
        for i, (model, response) in enumerate(zip(models, responses)):
            if response.get("success") and response.get('output'):
                provider = get_provider_for_model(model)
                
                # Prepare database metadata
                db_metadata = {
                    'request_timestamp': today,
                    'response_timestamp': today,
                    'extra_metadata': {
                        'source': 'aipy_multi_model',
                        'model_index': i,
                        'total_models': len(models),
                        'is_best_response': metadata.get('best_model') == model if metadata else False,
                        'execution_type': 'parallel' if metadata and metadata.get('parallel') else 'sequential',
                        'iterations': metadata.get('iterations', 1) if metadata else 1
                    }
                }
                
                # Add timing if available
                if metadata and 'total_time' in metadata:
                    db_metadata['duration_ms'] = int(metadata['total_time'] * 1000)
                
                # Add decision info if available
                if metadata and metadata.get('best_model') == model:
                    db_metadata['extra_metadata']['decision_model'] = metadata.get('decision_model')
                    db_metadata['extra_metadata']['decision_reasoning'] = metadata.get('decision_reasoning')
                
                new_id = store_chat_in_db(
                    prompt, 
                    response['output'], 
                    provider, 
                    model, 
                    db_metadata,
                    tags=tags,
                    context_ids=context_ids, 
                    debug=debug, 
                    dry_run=dry_run,
                    show_id=show_id
                )
                
                # Store the new ID for summary display
                if new_id and 'new_chat_ids' not in metadata:
                    if not hasattr(metadata, 'get'):
                        metadata = metadata or {}
                    metadata['new_chat_ids'] = []
                if new_id:
                    metadata.setdefault('new_chat_ids', []).append(new_id)

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Universal AI provider wrapper - intelligently routes to the appropriate AI service",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  aipy "What is quantum computing?"                    # Uses default model
  aipy --model gpt-4o "Explain AI"                   # Routes to OpenAI
  aipy --model sonar-pro "Latest news"               # Routes to Perplexity
  aipy --model claude-3-5-sonnet-latest "Help me"    # Routes to Claude
  aipy --model gpt-4o,claude-3-5-sonnet "Compare"    # Multiple models in parallel
  aipy --times 3 --image-gen "A sunset"              # Generate 3 images
  aipy -L                                             # List all available models
  
  # Database operations
  aipy --search "quantum computing"                   # Search chat history
  aipy --search "python" --provider-filter claudpy   # Search by provider
  aipy --search "API" --model-filter gpt-4o          # Search by model
  aipy --search "*" --tags-filter "work,coding"      # Search by tags
  aipy --view 12345678-1234-5678-9abc-123456789012   # View chat by UUID
  aipy --delete 12345678-1234-5678-9abc-123456789012 # Delete chat
  aipy --delete 12345678-1234-5678-9abc-123456789012 -y  # Delete without confirmation
  
  # Tagging conversations
  aipy --tags "work,python,debugging" "Help fix my code"  # Tag conversation
  aipy --tags "research,ai" "Explain transformers"        # Multiple tags
  
  # Using previous chats as context (auto-adds 'follow-up' tag)
  aipy --id 12345678-1234-5678-9abc-123456789012 "Continue from where we left off"
  aipy --id uuid1 --id uuid2 "Build on both previous conversations"
  aipy --id uuid1 --tags "synthesis" "Next steps based on previous discussion"
  
  # Tracking new conversations
  aipy --show-id "New conversation"                    # Shows UUID on stderr
  aipy --summary "Another conversation"                # Shows UUID in summary
  aipy --id uuid1 --show-id "Follow-up with tracking" # Shows both context and new ID
        """
    )
    
    # Core options
    parser.add_argument("prompt", nargs="?", help="The prompt to send to the AI")
    parser.add_argument("--prompt", dest="prompt_flag", help="Prompt to prepend to the input")
    parser.add_argument("--model", default="grok-3",
                        help="Model(s) to use. Can specify multiple models comma-separated for parallel execution")
    parser.add_argument("--times", type=int, default=1, 
                        help="Number of times to repeat the request (useful for image generation)")
    
    # Provider selection
    parser.add_argument("--provider", choices=["claudpy", "grokpy", "geminpy", "openpy", "ollampy", "perpy"],
                        help="Force a specific provider (overrides model-based routing)")
    
    # Output options
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")
    parser.add_argument("--json", action="store_true", help="Return JSON response")
    parser.add_argument("-S", "--no-streaming", action="store_true", help="Disable streaming")
    parser.add_argument("--no-color", action="store_true", help="Disable colored output")
    parser.add_argument("--summary", action="store_true", help="Show usage summary")
    parser.add_argument("--dry-run", action="store_true", help="Show what would be sent without executing")
    
    # Content input
    parser.add_argument("-f", "--file", action="append", dest="files",
                        help="Include file content (can be used multiple times)")
    parser.add_argument("--image", action="append", dest="images", 
                        help="Include image files for vision analysis")
    
    # Generation modes
    parser.add_argument("--image-gen", action="store_true", help="Generate images")
    parser.add_argument("--embedding", action="store_true", help="Generate embeddings")
    parser.add_argument("--regen", action="store_true", help="Regenerate/enhance images")
    
    # Image generation options
    parser.add_argument("--output", help="Output filename for generated content")
    parser.add_argument("--size", help="Image size (format depends on provider)")
    parser.add_argument("--hd", action="store_true", help="Use HD quality for image generation")
    
    # Model/provider info
    parser.add_argument("-L", "--list-models", action="store_true", help="List all available models")
    
    # Context and personality
    parser.add_argument("--use-context", action="store_true", help="Include today's chat history")
    parser.add_argument("--use-context-from", help="Include chat history from specific date (YYYY-MM-DD)")
    parser.add_argument("--personality", help="Set AI personality/role")
    parser.add_argument("--no-preserve", action="store_true", help="Don't save chat transaction")
    
    # Pick best feature
    parser.add_argument("--pick-best", action="store_true", 
                        help="When using multiple models, use a reasoning model to pick the best response")
    parser.add_argument("--decision-model", default=DEFAULT_DECISION_MODEL,
                        help=f"Model to use for picking the best response (default: {DEFAULT_DECISION_MODEL})")
    
    # AI Chat database operations
    parser.add_argument("--search", metavar="QUERY", 
                        help="Search AI chat history in database")
    parser.add_argument("--view", metavar="CHAT_ID", 
                        help="View full AI chat by UUID")
    parser.add_argument("--delete", metavar="CHAT_ID", 
                        help="Delete AI chat by UUID")
    
    # View history options
    parser.add_argument("--history", action="store_true",
                        help="When viewing, show conversation history leading to this chat")
    parser.add_argument("--full-history", action="store_true", 
                        help="When viewing, show complete conversation thread including forward references")
    parser.add_argument("--trace", action="store_true",
                        help="Show conversation flow as UUID/timestamp trace instead of full content")
    parser.add_argument("--limit", type=int, default=10, 
                        help="Limit number of search results (default: 10)")
    parser.add_argument("--provider-filter", 
                        help="Filter search results by provider")
    parser.add_argument("--model-filter", 
                        help="Filter search results by model")
    parser.add_argument("--tags-filter", 
                        help="Filter search results by tags (comma-separated)")
    parser.add_argument("-y", "--yes", action="store_true", 
                        help="Skip confirmation prompts (for delete)")
    
    # Content tagging
    parser.add_argument("--tags", 
                        help="Tag the conversation with comma-separated tags")
    
    # Context from previous chats
    parser.add_argument("--id", action="append", dest="context_ids",
                        help="Include previous chat(s) as context by UUID (can be used multiple times)")
    
    # Output options for new chats
    parser.add_argument("--show-id", action="store_true",
                        help="Display the UUID of the newly created chat")
    
    # Ollama-specific options
    parser.add_argument("--endpoint", help="Ollama API endpoint")
    parser.add_argument("--profile-performance", action="store_true", help="Profile Ollama model performance")
    parser.add_argument("--profile-prompt", help="Custom prompt for performance profiling")
    parser.add_argument("--profile-models", help="Specific models to profile")
    parser.add_argument("--profile-show-output", action="store_true", help="Show model outputs in performance table")
    
    return parser.parse_args()

def run_provider_command(provider, args, model=None, iteration=None, stdin_input=None):
    """Run a command with the specified provider."""
    cmd = [provider]
    
    # Add model if specified and different from default
    if model and model != "grok-3":
        cmd.extend(["--model", model])
    
    # Add all the flags and options
    if args.debug:
        cmd.append("--debug")
    if args.json:
        cmd.append("--json")
    if args.no_streaming:
        cmd.append("--no-streaming")
    if args.no_color:
        cmd.append("--no-color")
    if args.summary:
        cmd.append("--summary")
    if args.dry_run:
        cmd.append("--dry-run")
    if args.embedding:
        cmd.append("--embedding")
    if args.image_gen:
        cmd.append("--image-gen")
    if args.regen:
        cmd.append("--regen")
    if args.hd:
        cmd.append("--hd")
    if args.use_context:
        cmd.append("--use-context")
    # Always pass --no-preserve to providers since aipy handles preservation
    cmd.append("--no-preserve")
    if args.list_models:
        cmd.append("--list-models")
    if args.profile_performance:
        cmd.append("--profile-performance")
    if args.profile_show_output:
        cmd.append("--profile-show-output")
    
    # Add options with values
    if args.files:
        for file in args.files:
            cmd.extend(["-f", file])
    if args.images:
        for image in args.images:
            cmd.extend(["--image", image])
    if args.output:
        # For multiple iterations, modify output filename
        if iteration is not None and args.times > 1:
            output_path = Path(args.output)
            stem = output_path.stem
            suffix = output_path.suffix
            new_output = f"{stem}-{iteration:03d}{suffix}"
            cmd.extend(["--output", new_output])
        else:
            cmd.extend(["--output", args.output])
    if args.size:
        cmd.extend(["--size", args.size])
    if args.use_context_from:
        cmd.extend(["--use-context-from", args.use_context_from])
    if args.personality:
        cmd.extend(["--personality", args.personality])
    if args.endpoint:
        cmd.extend(["--endpoint", args.endpoint])
    if args.profile_prompt:
        cmd.extend(["--profile-prompt", args.profile_prompt])
    if args.profile_models:
        cmd.extend(["--profile-models", args.profile_models])
    
    # Add --prompt if provided via flag (not positional) and not being passed via stdin
    if args.prompt_flag and not stdin_input and not args.list_models:
        cmd.extend(["--prompt", args.prompt_flag])
    
    return cmd, stdin_input

def execute_command(cmd_and_stdin, model=None, iteration=None, capture_output=False):
    """Execute a command and return the result."""
    try:
        if isinstance(cmd_and_stdin, tuple):
            cmd, stdin_input = cmd_and_stdin
        else:
            cmd, stdin_input = cmd_and_stdin, None
            
        if iteration is not None:
            print(f"[{model or 'default'} - Iteration {iteration + 1}]", file=sys.stderr)
        elif model:
            print(f"[{model}]", file=sys.stderr)
        
        # If we have stdin input, pass it to the command
        if stdin_input:
            result = subprocess.run(cmd, input=stdin_input, text=True, capture_output=capture_output)
        else:
            result = subprocess.run(cmd, capture_output=capture_output, text=True)
            
        response = {
            "success": result.returncode == 0,
            "returncode": result.returncode,
            "model": model,
            "iteration": iteration,
            "cmd": " ".join(cmd) if isinstance(cmd, list) else str(cmd)
        }
        
        if capture_output:
            response["output"] = result.stdout
            if result.stderr:
                response["stderr"] = result.stderr
                
        return response
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "model": model,
            "iteration": iteration,
            "cmd": " ".join(cmd) if isinstance(cmd, (list, tuple)) else str(cmd)
        }

def list_all_models():
    """List models from all providers."""
    providers = ["claudpy", "grokpy", "geminpy", "openpy", "ollampy", "perpy"]
    
    print("ðŸ¤– All Available AI Models\n")
    print("=" * 80)
    
    for provider in providers:
        print(f"\nðŸ“¡ {provider.upper()}")
        print("-" * 40)
        
        try:
            # Run the list-models command for each provider
            result = subprocess.run([provider, "--list-models"], 
                                    capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                # Clean up the output to remove extra formatting
                output = result.stdout.strip()
                if output:
                    print(output)
                else:
                    print("  No models returned")
            else:
                print(f"  Error: {result.stderr.strip()}")
        except subprocess.TimeoutExpired:
            print("  Timeout - provider not responding")
        except FileNotFoundError:
            print(f"  Provider '{provider}' not found in PATH")
        except Exception as e:
            print(f"  Error: {e}")
    
    print("\n" + "=" * 80)
    print("ðŸ’¡ Usage: aipy --model <model-name> \"your prompt\"")
    print("ðŸ’¡ Multiple models: aipy --model model1,model2 \"your prompt\"")
    print("ðŸ’¡ Model routing is automatic based on the model name")

def main():
    args = parse_arguments()
    
    # Handle database operations first
    if args.search:
        tags_filter = None
        if args.tags_filter:
            tags_filter = [tag.strip() for tag in args.tags_filter.split(',') if tag.strip()]
        
        search_ai_chats(
            args.search, 
            limit=args.limit,
            provider_filter=args.provider_filter,
            model_filter=args.model_filter,
            tags_filter=tags_filter,
            output_format='json' if args.json else 'text',
            debug=args.debug
        )
        return
    
    if args.view:
        view_ai_chat(
            args.view,
            output_format='json' if args.json else 'text',
            debug=args.debug,
            show_history=args.history,
            show_full_history=args.full_history,
            trace_mode=args.trace
        )
        return
    
    if args.delete:
        delete_ai_chat(
            args.delete,
            confirm=args.yes,
            debug=args.debug
        )
        return
    
    # Handle list-models specially
    if args.list_models:
        list_all_models()
        return
    
    # Check for stdin data
    stdin_data = ""
    if not sys.stdin.isatty() or sys.stdin in select.select([sys.stdin], [], [], 0)[0]:
        try:
            stdin_data = sys.stdin.read()
        except:
            pass  # If reading fails, continue without stdin data
    
    # Validate that we have some form of input for non-listing operations
    if not args.prompt and not args.prompt_flag and not stdin_data and not args.list_models:
        print("Error: No prompt provided. Use positional argument, --prompt flag, or stdin.", file=sys.stderr)
        sys.exit(1)
    
    # Load context from previous chats if requested
    context_chats = []
    if args.context_ids:
        context_chats = get_chat_context(args.context_ids, debug=args.debug)
        if context_chats is None:
            # Error occurred (likely duplicate partial UUIDs)
            sys.exit(1)
    
    # Combine all input sources like grokpy does
    combined_parts = []
    
    # Add context from previous chats first
    if context_chats:
        context_text = format_context_for_prompt(context_chats)
        combined_parts.append(context_text)
    
    # Add file contents if any
    if args.files:
        for file_path in args.files:
            try:
                with open(file_path, 'r') as f:
                    file_content = f.read()
                    combined_parts.append(f"=== File: {file_path} ===\n{file_content}\n")
            except IOError as e:
                print(f"Warning: Could not read file {file_path}: {e}", file=sys.stderr)
    
    # Add --prompt flag content if provided
    if args.prompt_flag:
        combined_parts.append(args.prompt_flag)
    
    # Add positional prompt if provided
    if args.prompt:
        combined_parts.append(args.prompt)
    
    # Add stdin content if any
    if stdin_data:
        combined_parts.append(stdin_data)
    
    # Combine all parts
    full_prompt = "\n".join(combined_parts) if combined_parts else ""
    
    # Store original prompt for --prompt flag
    args.original_prompt = args.prompt
    
    # Parse tags if provided
    tags = []
    if args.tags:
        tags = [tag.strip() for tag in args.tags.split(',') if tag.strip()]
    
    # Automatically add 'follow-up' tag when using context
    if args.context_ids:
        if 'follow-up' not in tags:
            tags.append('follow-up')
    
    # Convert back to None if empty for consistency
    tags = tags if tags else None
    
    # Parse models (can be comma-separated)
    models = [m.strip() for m in args.model.split(",") if m.strip()]
    
    # Prepare all commands to execute
    commands = []
    
    for model in models:
        # Determine provider
        if args.provider:
            provider = args.provider
        else:
            provider = get_provider_for_model(model)
        
        # Generate commands for multiple iterations
        for iteration in range(args.times):
            cmd_and_stdin = run_provider_command(provider, args, model, 
                                                 iteration if args.times > 1 else None, 
                                                 full_prompt if full_prompt else None)
            commands.append((cmd_and_stdin, model, iteration if args.times > 1 else None))
    
    # Track execution time
    start_time = time.time()
    
    # Execute commands
    if len(commands) == 1 and args.times == 1:
        # Single command - run directly for better interactive experience
        cmd_and_stdin, model, iteration = commands[0]
        cmd, stdin_input = cmd_and_stdin
        
        if args.debug:
            print(f"Debug: Executing: {' '.join(cmd)}", file=sys.stderr)
            if stdin_input:
                print(f"Debug: With stdin input: {len(stdin_input)} characters", file=sys.stderr)
        
        try:
            if stdin_input:
                result = subprocess.run(cmd, input=stdin_input, text=True, capture_output=True)
            else:
                result = subprocess.run(cmd, capture_output=True, text=True)
            
            # Print output to user
            if result.stdout:
                print(result.stdout, end='')
            if result.stderr:
                print(result.stderr, file=sys.stderr, end='')
            
            # Save transaction if not disabled
            if not args.no_preserve and result.returncode == 0:
                end_time = time.time()
                provider = get_provider_for_model(model)
                
                # For single model, also store directly in DB
                if not args.dry_run:
                    db_metadata = {
                        'request_timestamp': datetime.fromtimestamp(start_time),
                        'response_timestamp': datetime.fromtimestamp(end_time),
                        'duration_ms': int((end_time - start_time) * 1000),
                        'extra_metadata': {
                            'source': 'aipy_single_model',
                            'execution_type': 'single',
                            'iterations': 1
                        }
                    }
                    
                    new_id = store_chat_in_db(
                        full_prompt, 
                        result.stdout, 
                        provider, 
                        model, 
                        db_metadata,
                        tags=tags,
                        context_ids=args.context_ids, 
                        debug=args.debug, 
                        dry_run=args.dry_run,
                        show_id=args.show_id
                    )
                
                save_multi_model_transaction(
                    [model], 
                    full_prompt, 
                    [{"success": True, "output": result.stdout}],
                    {"total_time": end_time - start_time, "parallel": False, "iterations": 1, "new_chat_ids": [new_id] if new_id else []},
                    tags=tags,
                    context_ids=args.context_ids,
                    store_db=False,  # Already stored above
                    debug=args.debug,
                    dry_run=args.dry_run,
                    show_id=args.show_id
                )
                
                # Show summary for single model if requested
                if args.summary and new_id:
                    print(f"\nNew chat ID: {new_id}", file=sys.stderr)
                
            if result.returncode != 0:
                sys.exit(result.returncode)
                
        except KeyboardInterrupt:
            print("\nInterrupted by user", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"Error executing command: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        # Multiple commands - need to capture output for preservation or pick-best
        capture_output = not args.no_preserve or args.pick_best
        
        if len(commands) > 1:
            print(f"Executing {len(commands)} requests in parallel...\n", file=sys.stderr)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(commands), 8)) as executor:
            futures = []
            for cmd_and_stdin, model, iteration in commands:
                cmd, stdin_input = cmd_and_stdin
                if args.debug:
                    print(f"Debug: Queuing: {' '.join(cmd)}", file=sys.stderr)
                    if stdin_input:
                        print(f"Debug: With stdin input: {len(stdin_input)} characters", file=sys.stderr)
                future = executor.submit(execute_command, cmd_and_stdin, model, iteration, capture_output)
                futures.append(future)
            
            # Wait for all to complete
            results = []
            model_order = []
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    model_order.append(result.get("model", "unknown"))
                    
                    # Print output as it arrives (if captured and not using pick-best)
                    if not args.pick_best and result.get("output"):
                        print(f"\n[{result.get('model', 'unknown')}]", file=sys.stderr)
                        print(result["output"], end='')
                    
                    if not result["success"]:
                        error_msg = result.get("error", f"Command failed with return code {result.get('returncode', 'unknown')}")
                        print(f"Error in {result.get('model', 'unknown')}: {error_msg}", file=sys.stderr)
                except Exception as e:
                    print(f"Error executing command: {e}", file=sys.stderr)
        
        # Handle pick-best logic
        best_model = None
        reasoning = None
        if args.pick_best and len(models) > 1:
            # Prepare model responses for pick_best_response
            model_responses = []
            for model in models:
                for result in results:
                    if result.get("model") == model and result.get("success"):
                        model_responses.append((model, result))
                        break
            
            if len(model_responses) > 1:
                print(f"\nAnalyzing {len(model_responses)} responses to pick the best one...", file=sys.stderr)
                best_model, reasoning = pick_best_response(full_prompt, model_responses, args.decision_model)
                
                if best_model:
                    # Find and display the best response
                    for model, result in model_responses:
                        if model == best_model:
                            print(f"\n[Best Response: {best_model}]", file=sys.stderr)
                            print(result["output"], end='')
                            break
                    
                    if args.debug:
                        print(f"\nDecision reasoning: {reasoning}", file=sys.stderr)
                else:
                    print("\nCould not determine best response", file=sys.stderr)
            elif len(model_responses) == 1:
                # Only one successful response, use it
                model, result = model_responses[0]
                print(f"\n[{model}]", file=sys.stderr)
                print(result["output"], end='')
        
        # Summary
        successful = sum(1 for r in results if r["success"])
        print(f"\nCompleted: {successful}/{len(commands)} requests successful", file=sys.stderr)
        
        # Show summary with new chat IDs if requested
        if args.summary and metadata and metadata.get('new_chat_ids'):
            print(f"\nNew chat IDs created:", file=sys.stderr)
            for chat_id in metadata.get('new_chat_ids', []):
                print(f"  {chat_id}", file=sys.stderr)
        
        # Save transaction if not disabled and we have successful results
        if not args.no_preserve and successful > 0:
            end_time = time.time()
            
            # Sort results by model order for consistent output
            sorted_results = []
            sorted_models = []
            for model in models:
                for result in results:
                    if result.get("model") == model and result not in sorted_results:
                        sorted_results.append(result)
                        sorted_models.append(model)
                        break
            
            # Prepare metadata
            metadata = {
                "total_time": end_time - start_time, 
                "parallel": len(commands) > 1, 
                "iterations": args.times
            }
            
            # Add pick-best metadata if applicable
            if args.pick_best and len(models) > 1 and best_model:
                metadata["best_model"] = best_model
                metadata["decision_model"] = args.decision_model
                metadata["decision_reasoning"] = reasoning
            
            save_multi_model_transaction(
                sorted_models, 
                full_prompt, 
                sorted_results,
                metadata,
                tags=tags,
                context_ids=args.context_ids,
                store_db=True,
                debug=args.debug,
                dry_run=args.dry_run,
                show_id=args.show_id
            )

if __name__ == "__main__":
    main()