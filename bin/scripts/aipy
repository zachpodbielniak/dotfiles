#!/usr/bin/python3

# dotfiles - Personal configuration files and scripts
# Copyright (C) 2025  Zach Podbielniak
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.


# Container check for distrobox - do this BEFORE any other imports
import os
import subprocess
import sys

ctr_id = os.environ.get("CONTAINER_ID", "")
no_dbox_check = os.environ.get("NO_DBOX_CHECK", "").lower() in ("1", "true")
if not no_dbox_check and ctr_id != "dev":
    cmd = ["distrobox", "enter", "dev", "--", *sys.argv]
    subprocess.run(cmd)
    sys.exit(0)

# Now import everything else inside the dev container
import argparse
import asyncio
import concurrent.futures
import json
import os
import re
import select
import subprocess
import time
import hashlib
from pathlib import Path
from datetime import datetime, timedelta

# Try to import database dependencies
try:
    import psycopg2
    import psycopg2.extras
    from psycopg2.extras import RealDictCursor
    DB_AVAILABLE = True
except ImportError:
    DB_AVAILABLE = False

# Try to import rich for markdown rendering
try:
    from rich.console import Console
    from rich.markdown import Markdown
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

# Default decision model for --pick-best feature
DEFAULT_DECISION_MODEL = "claude-opus-4-20250514"

# Database configuration for AI chat storage
AI_CHATS_DB_CONFIG = {
    'host': os.environ.get('AI_CHATS_DB_HOST', '127.0.0.1'),
    'port': int(os.environ.get('AI_CHATS_DB_PORT', '5432')),
    'database': os.environ.get('AI_CHATS_DB_NAME', 'ai_chats'),
    'user': os.environ.get('AI_CHATS_DB_USER', 'postgres'),
    'password': os.environ.get('AI_CHATS_DB_PASSWORD', '')
}

# Model to provider mapping
MODEL_PROVIDERS = {
    # Claude models (December 2025 update)
    "claude-sonnet-4-5": "claudpy",
    "claude-opus-4-5": "claudpy",
    "claude-haiku-4-5": "claudpy",
    "claude-opus-4-1": "claudpy",
    "claude-sonnet-4": "claudpy",
    "claude-opus-4": "claudpy",
    "claude-sonnet-4-20250514": "claudpy",
    "claude-opus-4-20250514": "claudpy",
    "claude-3-7-sonnet-latest": "claudpy",
    "claude-3-5-sonnet-20241022": "claudpy",
    "claude-3-5-sonnet-latest": "claudpy",
    "claude-3-5-haiku-20241022": "claudpy",
    "claude-3-5-haiku-latest": "claudpy",
    "claude-3-opus-20240229": "claudpy",
    "claude-3-opus-latest": "claudpy",
    "claude-3-sonnet-20240229": "claudpy",
    "claude-3-haiku-20240307": "claudpy",

    # Grok models (December 2025 update)
    "grok-4-1-fast-reasoning": "grokpy",
    "grok-4-1-fast-non-reasoning": "grokpy",
    "grok-4": "grokpy",
    "grok-4-heavy": "grokpy",
    "grok-3": "grokpy",
    "grok-3-mini": "grokpy",
    "grok-3-fast": "grokpy",
    "grok-3-mini-fast": "grokpy",
    "grok-2-1212": "grokpy",
    "grok-2-image-1212": "grokpy",
    "grok-2-vision-1212": "grokpy",

    # Gemini models (December 2025 update)
    "gemini-3-pro": "geminpy",
    "gemini-3-pro-image-preview": "geminpy",
    "gemini-2.5-pro": "geminpy",
    "gemini-2.5-flash": "geminpy",
    "gemini-2.5-flash-lite": "geminpy",
    "gemini-2.5-flash-live": "geminpy",
    "gemini-2.5-computer-use": "geminpy",
    "gemini-2.5-pro-preview-06-05": "geminpy",
    "gemini-2.5-pro-preview-05-06": "geminpy",
    "gemini-2.5-flash-preview": "geminpy",
    "gemini-2.0-flash": "geminpy",
    "gemini-2.0-flash-preview-image-generation": "geminpy",
    "gemini-2.0-flash-lite": "geminpy",
    "gemini-1.5-flash": "geminpy",
    "gemini-1.5-flash-8b": "geminpy",
    "gemini-1.5-pro": "geminpy",
    "gemini-embedding-exp": "geminpy",
    "imagen-3.0-generate-002": "geminpy",
    "veo-2.0-generate-001": "geminpy",
    "gemini-2.0-flash-live-001": "geminpy",
    "text-embedding-004": "geminpy",
    "embedding-001": "geminpy",
    "models/aqa": "geminpy",

    # OpenAI models (December 2025 update)
    "gpt-5": "openpy",
    "gpt-5-thinking": "openpy",
    "gpt-5-thinking-pro": "openpy",
    "gpt-5-codex": "openpy",
    "o4-mini": "openpy",
    "o3-mini": "openpy",
    "gpt-oss-120b": "openpy",
    "gpt-oss-20b": "openpy",
    "gpt-4.1": "openpy",
    "gpt-4.1-nano": "openpy",
    "gpt-4o": "openpy",
    "gpt-4-turbo": "openpy",
    "gpt-4": "openpy",
    "gpt-4-32k": "openpy",
    "gpt-4o-mini": "openpy",
    "gpt-3.5-turbo": "openpy",
    "gpt-3.5-turbo-16k": "openpy",
    "dall-e-3": "openpy",
    "dall-e-2": "openpy",
    "whisper-1": "openpy",
    "text-embedding-3-small": "openpy",
    "text-embedding-3-large": "openpy",
    "text-embedding-ada-002": "openpy",

    # Perplexity models (December 2025 update)
    "sonar-pro": "perpy",
    "sonar": "perpy",
    "sonar-deep-research": "perpy",
    "sonar-reasoning-pro": "perpy",
    "reasoning-pro": "perpy",
    "r1-1776": "perpy",
}

# Centralized model limits (context_window, max_output_tokens)
CENTRALIZED_MODEL_LIMITS = {
    # Claude models
    "claude-sonnet-4-5": (1_000_000, 16_000),
    "claude-opus-4-5": (200_000, 64_000),
    "claude-haiku-4-5": (200_000, 8_192),
    "claude-opus-4-1": (200_000, 32_000),
    "claude-sonnet-4": (200_000, 64_000),
    "claude-opus-4": (200_000, 32_000),
    "claude-sonnet-4-20250514": (200_000, 64_000),
    "claude-opus-4-20250514": (200_000, 32_000),

    # OpenAI models
    "gpt-5": (128_000, 16_384),
    "gpt-5-thinking": (128_000, 16_384),
    "gpt-5-thinking-pro": (128_000, 16_384),
    "gpt-5-codex": (128_000, 16_384),
    "gpt-4.1": (1_000_000, 32_768),
    "gpt-4.1-nano": (1_000_000, 32_768),
    "o4-mini": (128_000, 16_384),
    "o3-mini": (128_000, 16_384),
    "gpt-oss-120b": (4_096, 2_048),
    "gpt-oss-20b": (4_096, 2_048),
    "gpt-4o": (128_000, 16_384),
    "gpt-4-turbo": (128_000, 4_096),
    "gpt-4": (8_192, 4_096),
    "gpt-4-32k": (32_768, 4_096),
    "gpt-4o-mini": (128_000, 16_384),
    "gpt-3.5-turbo": (16_385, 4_096),

    # Gemini models
    "gemini-3-pro": (1_048_576, 65_536),
    "gemini-3-pro-image-preview": (1_048_576, 65_536),
    "gemini-2.5-pro": (1_048_576, 65_536),
    "gemini-2.5-flash": (1_048_576, 65_536),
    "gemini-2.5-flash-lite": (1_048_576, 65_536),
    "gemini-2.5-flash-live": (128_000, 8_000),
    "gemini-2.5-computer-use": (1_048_576, 65_536),
    "gemini-2.0-flash": (1_048_576, 8_192),
    "gemini-1.5-flash": (1_048_576, 8_192),
    "gemini-1.5-pro": (1_048_576, 65_536),

    # Grok models
    "grok-4-1-fast-reasoning": (256_000, 4_096),
    "grok-4-1-fast-non-reasoning": (256_000, 4_096),
    "grok-4": (256_000, 4_096),
    "grok-4-heavy": (256_000, 4_096),
    "grok-3": (1_000_000, 4_096),
    "grok-3-mini": (1_000_000, 4_096),

    # Perplexity models
    "sonar-pro": (200_000, 8_000),
    "sonar": (128_000, 4_000),
    "sonar-deep-research": (200_000, 16_000),
    "sonar-reasoning-pro": (200_000, 16_000),
    "reasoning-pro": (128_000, 8_000),
    "r1-1776": (256_000, 32_000),

    # Default fallback
    "default": (128_000, 4_096),
}

def get_model_limits(model_name):
    """Get context window and max output for a model."""
    return CENTRALIZED_MODEL_LIMITS.get(model_name, CENTRALIZED_MODEL_LIMITS["default"])

def get_provider_for_model(model):
    """Get the provider for a given model, or return 'ollampy' as default for unknown models."""
    return MODEL_PROVIDERS.get(model, "ollampy")

def check_db_available(debug=False):
    """Check if the AI chats database is available."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database dependencies not available (psycopg2)", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        conn.close()
        return True
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Could not connect to AI chats database: {e}", file=sys.stderr)
        return False

def extract_user_prompt(full_prompt):
    """Extract the actual user prompt from a prompt that may include context."""
    # Check if the prompt contains context
    if "=== CONTEXT FROM PREVIOUS CONVERSATIONS ===" in full_prompt and "=== END CONTEXT ===" in full_prompt:
        # Find the end of the context section
        end_context_marker = "=== END CONTEXT ==="
        context_end = full_prompt.find(end_context_marker)
        if context_end != -1:
            # Extract everything after the context section
            user_prompt_start = context_end + len(end_context_marker)
            user_prompt = full_prompt[user_prompt_start:].strip()
            return user_prompt

    # If no context found, return the original prompt
    return full_prompt

def load_preferences(debug=False):
    """Load personal preferences from the standard location.

    Returns the content of the preferences file, or None if not found/readable.
    Uses ${HOME} for path expansion.
    """
    prefs_path = os.path.expandvars("${HOME}/Documents/notes/02_areas/personal/ai/ai_personal_preferences.md")

    if debug:
        print(f"Debug: Attempting to load preferences from: {prefs_path}", file=sys.stderr)

    try:
        if not os.path.exists(prefs_path):
            if debug:
                print(f"Debug: Preferences file not found at {prefs_path}", file=sys.stderr)
            return None

        with open(prefs_path, 'r') as f:
            content = f.read()

        if debug:
            print(f"Debug: Successfully loaded preferences ({len(content)} bytes)", file=sys.stderr)

        return content
    except IOError as e:
        if debug:
            print(f"Debug: Error reading preferences file: {e}", file=sys.stderr)
        return None

def store_chat_in_db(prompt, response, provider, model, metadata=None, tags=None, context_ids=None, context_id=None, debug=False, dry_run=False, show_id=False, auto_name=False, context_name=None):
    """Store chat interaction in the database with context_uuid support."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for chat storage", file=sys.stderr)
        return None
    
    if dry_run:
        print("Would store chat in AI chats database", file=sys.stderr)
        return None
    
    if not check_db_available(debug):
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor()
        
        # Extract the actual user prompt (without context)
        clean_prompt = extract_user_prompt(prompt)
        
        # Generate hashes for the clean prompt and response
        prompt_hash = hashlib.sha256(clean_prompt.encode('utf-8')).hexdigest()
        response_hash = hashlib.sha256(response.encode('utf-8')).hexdigest()
        
        # Determine context_uuid
        context_uuid = None
        
        # Priority 1: Use directly specified context_id (--context-id)
        if context_id:
            # Close current cursor and use RealDictCursor for resolve_context_id
            cursor.close()
            resolved_context = resolve_context_id(context_id, debug)
            if resolved_context:
                context_uuid = resolved_context
                if debug:
                    print(f"Using specified context: {context_uuid}", file=sys.stderr)
            else:
                # Error already printed by resolve_context_id
                conn.close()
                return None
            
            # Reopen cursor for the rest of the function
            cursor = conn.cursor()
        
        # Priority 2: Lookup context_uuid from existing chat_ids (--id)
        elif context_ids:
            # Lookup context_uuid from the first chat_id provided
            chat_id = context_ids[0]
            # Handle partial UUID (8 characters)
            if len(chat_id) == 8:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s LIMIT 1
                """, [f"{chat_id}%"])
            else:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats WHERE id = %s::uuid
                """, [chat_id])
            
            result = cursor.fetchone()
            if result:
                context_uuid = result[0]
                if debug:
                    print(f"Using existing context from chat: {context_uuid}", file=sys.stderr)
            else:
                if debug:
                    print(f"Warning: Chat ID {chat_id} not found, creating new context", file=sys.stderr)
        
        # Priority 3: Create new context if none found
        new_context_created = False
        if not context_uuid:
            # Use provided context name or default to '<untitled>'
            context_name_to_use = context_name if context_name else '<untitled>'
            cursor.execute("""
                INSERT INTO ai_chat_contexts (name) VALUES (%s)
                RETURNING context_uuid
            """, [context_name_to_use])
            context_uuid = cursor.fetchone()[0]
            new_context_created = True
            if debug:
                print(f"Created new context '{context_name_to_use}': {context_uuid}", file=sys.stderr)
        
        # Extract metadata
        request_timestamp = metadata.get('request_timestamp') if metadata else datetime.now()
        response_timestamp = metadata.get('response_timestamp') if metadata else datetime.now()
        duration_ms = metadata.get('duration_ms') if metadata else None
        tokens_input = metadata.get('tokens_input') if metadata else None
        tokens_output = metadata.get('tokens_output') if metadata else None
        cost_input_usd = metadata.get('cost_input_usd') if metadata else None
        cost_output_usd = metadata.get('cost_output_usd') if metadata else None
        cost_total_usd = metadata.get('cost_total_usd') if metadata else None
        extra_metadata = metadata.get('extra_metadata', {}) if metadata else {}
        
        # Process tags
        tags_array = tags if tags else []
        
        # Insert into database and return the UUID
        cursor.execute("""
            INSERT INTO ai_chats (
                context_uuid, prompt, response, provider, model,
                request_timestamp, response_timestamp, duration_ms,
                tokens_input, tokens_output,
                cost_input_usd, cost_output_usd, cost_total_usd,
                prompt_hash, response_hash, tags, metadata
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            RETURNING id
        """, (
            context_uuid, clean_prompt, response, provider, model,
            request_timestamp, response_timestamp, duration_ms,
            tokens_input, tokens_output,
            cost_input_usd, cost_output_usd, cost_total_usd,
            prompt_hash, response_hash, tags_array, json.dumps(extra_metadata)
        ))
        
        new_id = cursor.fetchone()[0]
        
        # Update context tags with all tags from chats in this context
        if tags_array:  # Only update if this chat has tags
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET tags = ARRAY(
                    SELECT DISTINCT unnest(array_agg(tag))
                    FROM (
                        SELECT unnest(tags) as tag 
                        FROM ai_chats 
                        WHERE context_uuid = %s AND tags IS NOT NULL
                    ) all_tags
                    WHERE tag IS NOT NULL AND tag != ''
                )
                WHERE context_uuid = %s
            """, [context_uuid, context_uuid])
            
            if debug:
                print(f"Updated context tags for context: {context_uuid}", file=sys.stderr)
        
        conn.commit()
        cursor.close()
        conn.close()
        
        if debug:
            print("Chat stored in database successfully", file=sys.stderr)
        
        if show_id:
            print(f"New chat ID: {new_id}", file=sys.stderr)
            print(f"Context UUID: {context_uuid}", file=sys.stderr)
        
        # Auto-name new context if requested
        if auto_name and new_context_created:
            if debug:
                print(f"Auto-naming new context {context_uuid}...", file=sys.stderr)
            auto_generate_context_name(context_uuid, model, clean_prompt, response, debug)
        
        return str(new_id)
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to store chat in database: {e}", file=sys.stderr)
        return None

def search_ai_chats(query, limit=10, provider_filter=None, model_filter=None, tags_filter=None, output_format='text', debug=False):
    """Search AI chats using full-text search."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Build WHERE clauses
        where_clauses = []
        params = []
        
        # Add text search if query is not a wildcard
        if query and query != "*":
            where_clauses.append(
                "(to_tsvector('english', a.prompt) @@ plainto_tsquery('english', %s) OR "
                "to_tsvector('english', a.response) @@ plainto_tsquery('english', %s))"
            )
            params.extend([query, query])
        
        if provider_filter:
            where_clauses.append("a.provider = %s")
            params.append(provider_filter)
        
        if model_filter:
            where_clauses.append("a.model = %s")
            params.append(model_filter)
        
        if tags_filter:
            # Support searching for any of the provided tags
            tag_conditions = []
            for tag in tags_filter:
                tag_conditions.append("a.tags @> %s")
                params.append([tag.strip()])
            if tag_conditions:
                where_clauses.append(f"({' OR '.join(tag_conditions)})")
        
        where_clause = " AND ".join(where_clauses) if where_clauses else "TRUE"
        
        # Search query with ranking
        if query and query != "*":
            sql = f"""
            SELECT 
                a.id,
                LEFT(a.prompt, 100) as prompt_snippet,
                LEFT(a.response, 100) as response_snippet,
                a.provider,
                a.model,
                a.request_timestamp,
                a.duration_ms,
                a.cost_total_usd,
                a.tags,
                a.context_uuid,
                c.name as context_name,
                ts_rank(
                    to_tsvector('english', a.prompt || ' ' || a.response), 
                    plainto_tsquery('english', %s)
                ) as rank
            FROM ai_chats a
            JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            WHERE {where_clause}
            ORDER BY rank DESC, a.request_timestamp DESC
            LIMIT %s
            """
            params.extend([query, limit])
        else:
            sql = f"""
            SELECT 
                a.id,
                LEFT(a.prompt, 100) as prompt_snippet,
                LEFT(a.response, 100) as response_snippet,
                a.provider,
                a.model,
                a.request_timestamp,
                a.duration_ms,
                a.cost_total_usd,
                a.tags,
                a.context_uuid,
                c.name as context_name,
                0 as rank
            FROM ai_chats a
            JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            WHERE {where_clause}
            ORDER BY a.request_timestamp DESC
            LIMIT %s
            """
            params.append(limit)
        
        cursor.execute(sql, params)
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        if output_format == 'json':
            print(json.dumps([dict(r) for r in results], default=str, indent=2))
        else:
            if not results:
                print("No results found.")
                return True
            
            print(f"\nSearch results for: \"{query}\"")
            print("=" * 50)
            
            for i, r in enumerate(results, 1):
                print(f"\n{i}. [{str(r['id'])}] {r['provider']}/{r['model']}")
                print(f"   Context: {r['context_name']} ({str(r['context_uuid'])[:8]}...)")
                print(f"   Time: {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                if r['duration_ms']:
                    print(f"   Duration: {r['duration_ms']}ms")
                if r['cost_total_usd']:
                    print(f"   Cost: ${r['cost_total_usd']:.4f}")
                if r['tags']:
                    print(f"   Tags: {', '.join(r['tags'])}")
                print(f"   Prompt: {r['prompt_snippet']}{'...' if len(r['prompt_snippet']) >= 100 else ''}")
                print(f"   Response: {r['response_snippet']}{'...' if len(r['response_snippet']) >= 100 else ''}")
            
            print(f"\nFound {len(results)} results.")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error searching AI chats: {e}", file=sys.stderr)
        return False

def search_contexts(query, limit=10, output_format='text', debug=False):
    """Search AI chat contexts by name, summary content, or tags."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Handle wildcard search for all contexts
        if query == '*':
            where_clause = "TRUE"  # Show all contexts
            params = []
            order_params = []
        else:
            # Build search query with multiple conditions
            search_conditions = []
            params = []
            
            # Search in name (case insensitive)
            search_conditions.append("LOWER(name) LIKE LOWER(%s)")
            params.append(f"%{query}%")
            
            # Search in summary (full-text search if available, otherwise LIKE)
            search_conditions.append("to_tsvector('english', COALESCE(summary, '')) @@ plainto_tsquery('english', %s)")
            params.append(query)
            
            # Search in tags (using GIN array search)
            search_conditions.append("tags @> ARRAY[%s] OR %s = ANY(tags)")
            params.extend([query.lower(), query.lower()])
            
            # Combine all search conditions with OR
            where_clause = " OR ".join([f"({condition})" for condition in search_conditions])
            
            # Parameters for ordering (only used when not wildcard)
            order_params = [f"%{query}%", query.lower(), query.lower()]
        
        # Build SQL query with appropriate ordering
        if query == '*':
            # For wildcard, simple ordering by activity
            sql = f"""
            SELECT 
                context_uuid,
                name,
                summary,
                tags,
                created_at,
                updated_at,
                summary_last_updated,
                (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count,
                (SELECT MAX(request_timestamp) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as last_chat_time
            FROM ai_chat_contexts ctx
            WHERE {where_clause}
            ORDER BY 
                last_chat_time DESC NULLS LAST,
                created_at DESC
            LIMIT %s
            """
            params.append(limit)
        else:
            # For regular search, relevance-based ordering
            sql = f"""
            SELECT 
                context_uuid,
                name,
                summary,
                tags,
                created_at,
                updated_at,
                summary_last_updated,
                (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count,
                (SELECT MAX(request_timestamp) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as last_chat_time
            FROM ai_chat_contexts ctx
            WHERE {where_clause}
            ORDER BY 
                CASE 
                    WHEN LOWER(name) LIKE LOWER(%s) THEN 1
                    WHEN tags @> ARRAY[%s] OR %s = ANY(tags) THEN 2
                    ELSE 3
                END,
                last_chat_time DESC NULLS LAST,
                created_at DESC
            LIMIT %s
            """
            # Add parameters for ordering and limit
            params.extend(order_params + [limit])
        
        if debug:
            print(f"Search SQL: {sql}", file=sys.stderr)
            print(f"Parameters: {params}", file=sys.stderr)
        
        cursor.execute(sql, params)
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        if output_format == 'json':
            print(json.dumps([dict(r) for r in results], default=str, indent=2))
        else:
            if not results:
                if query == '*':
                    print("No contexts found.")
                else:
                    print("No contexts found.")
                return True
            
            if query == '*':
                print(f"\nAll contexts ({len(results)} found):")
            else:
                print(f"\nContext search results for: \"{query}\"")
            print("=" * 60)
            
            for i, r in enumerate(results, 1):
                uuid_short = str(r['context_uuid'])[:8]
                print(f"\n{i}. [{uuid_short}...] {r['name']}")
                print(f"   UUID: {r['context_uuid']}")
                print(f"   Created: {r['created_at'].strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"   Chats: {r['chat_count']}")
                
                if r['last_chat_time']:
                    print(f"   Last Chat: {r['last_chat_time'].strftime('%Y-%m-%d %H:%M:%S')}")
                
                if r['summary']:
                    summary_preview = r['summary'][:150]
                    print(f"   Summary: {summary_preview}{'...' if len(r['summary']) > 150 else ''}")
                
                if r['tags']:
                    print(f"   Tags: {', '.join(r['tags'])}")
                
                if r['summary_last_updated']:
                    print(f"   Summary Updated: {r['summary_last_updated'].strftime('%Y-%m-%d %H:%M:%S')}")
            
            if query == '*':
                print(f"\nShowing {len(results)} contexts.")
            else:
                print(f"\nFound {len(results)} contexts.")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error searching contexts: {e}", file=sys.stderr)
        return False

def fzf_search_chats(args):
    """Search chats using fzf with preview."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(args.debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get all chats
        cursor.execute("""
            SELECT 
                id,
                prompt,
                response,
                provider,
                model,
                request_timestamp,
                tags,
                context_uuid
            FROM ai_chats
            ORDER BY request_timestamp DESC
        """)
        
        chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not chats:
            print("No chats found.")
            return False
        
        # Create temporary file with chat entries for fzf
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            for chat in chats:
                # Format: UUID | timestamp | provider/model | first line of prompt
                timestamp = chat['request_timestamp'].strftime('%Y-%m-%d %H:%M')
                prompt_preview = chat['prompt'].split('\n')[0][:80]
                tags_str = f" [{', '.join(chat['tags'])}]" if chat['tags'] else ""
                line = f"{str(chat['id'])[:8]} | {timestamp} | {chat['provider']}/{chat['model']}{tags_str} | {prompt_preview}"
                f.write(line + '\n')
            temp_file = f.name
        
        # Set PGPASSWORD if available
        if AI_CHATS_DB_CONFIG.get('password'):
            os.environ['PGPASSWORD'] = AI_CHATS_DB_CONFIG['password']
        
        # Create preview script
        preview_script = """#!/bin/bash
export PGPASSWORD='{}'
uuid="${{1%% *}}"
# Find full UUID from partial
full_uuid=$(psql -h {} -p {} -d {} -U {} -t -c "SELECT id FROM ai_chats WHERE id::text LIKE '$uuid%' LIMIT 1" 2>/dev/null | tr -d ' ')
if [ -z "$full_uuid" ]; then
    echo "Chat not found"
    exit 1
fi
# Get chat details
psql -h {} -p {} -d {} -U {} -t -c "
SELECT 
    '━━━ PROMPT ━━━' || E'\\n' || prompt || E'\\n\\n' ||
    '━━━ RESPONSE ━━━' || E'\\n' || response
FROM ai_chats 
WHERE id = '$full_uuid'::uuid
" 2>/dev/null
""".format(
            AI_CHATS_DB_CONFIG.get('password', ''),
            AI_CHATS_DB_CONFIG['host'],
            AI_CHATS_DB_CONFIG['port'],
            AI_CHATS_DB_CONFIG['database'],
            AI_CHATS_DB_CONFIG['user'],
            AI_CHATS_DB_CONFIG['host'],
            AI_CHATS_DB_CONFIG['port'],
            AI_CHATS_DB_CONFIG['database'],
            AI_CHATS_DB_CONFIG['user']
        )
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as f:
            f.write(preview_script)
            preview_file = f.name
        
        os.chmod(preview_file, 0o755)
        
        try:
            # Find fzf command
            import shutil
            fzf_path = shutil.which('fzf')
            if not fzf_path:
                print("Error: fzf command not found. Please install fzf.", file=sys.stderr)
                return False
            
            # Run fzf
            fzf_cmd = [
                fzf_path,
                '--preview', f'{preview_file} {{}}',
                '--preview-window', 'down:70%:wrap',
                '--header', 'Select a chat (Enter to view/continue, Ctrl-C to cancel)',
                '--bind', 'ctrl-/:toggle-preview'
            ]
            
            result = subprocess.run(fzf_cmd, stdin=open(temp_file), capture_output=True, text=True)
            
            if result.returncode == 0 and result.stdout.strip():
                selected = result.stdout.strip()
                chat_uuid = selected.split(' | ')[0]
                
                # View or continue based on --repl flag
                if args.repl:
                    # Get full UUID and continue in REPL mode
                    conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
                    cursor = conn.cursor()
                    cursor.execute("SELECT id FROM ai_chats WHERE id::text LIKE %s LIMIT 1", [f"{chat_uuid}%"])
                    full_uuid = cursor.fetchone()[0]
                    cursor.close()
                    conn.close()
                    
                    # Continue conversation
                    args.chat_id = [str(full_uuid)]
                    args.context_ids = [str(full_uuid)]
                    start_repl(args)
                else:
                    # Just view the chat
                    view_ai_chat(chat_uuid, output_format='text', debug=args.debug, 
                                content_only=args.content_only, show_history=args.history,
                                show_full_history=args.full_history, trace_mode=args.trace,
                                render=args.render)
                
                return True
            
        finally:
            os.unlink(temp_file)
            os.unlink(preview_file)
            if 'PGPASSWORD' in os.environ:
                del os.environ['PGPASSWORD']
    
    except Exception as e:
        print(f"Error in fzf chat search: {e}", file=sys.stderr)
        return False

def fzf_search_contexts(args):
    """Search contexts using fzf with preview."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(args.debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get all contexts with chat count
        cursor.execute("""
            SELECT 
                ctx.context_uuid,
                ctx.name,
                ctx.summary,
                ctx.tags,
                ctx.created_at,
                ctx.updated_at,
                (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count,
                (SELECT MAX(request_timestamp) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as last_chat_time
            FROM ai_chat_contexts ctx
            ORDER BY last_chat_time DESC NULLS LAST, created_at DESC
        """)
        
        contexts = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not contexts:
            print("No contexts found.")
            return False
        
        # Create temporary file with context entries for fzf
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            for ctx in contexts:
                # Format: UUID | name | chat_count | last_chat | summary preview
                uuid_short = str(ctx['context_uuid'])[:8]
                last_chat = ctx['last_chat_time'].strftime('%Y-%m-%d %H:%M') if ctx['last_chat_time'] else 'Never'
                summary_preview = ctx['summary'][:60] + '...' if ctx['summary'] and len(ctx['summary']) > 60 else (ctx['summary'] or 'No summary')
                tags_str = f" [{', '.join(ctx['tags'])}]" if ctx['tags'] else ""
                line = f"{uuid_short} | {ctx['name']}{tags_str} | {ctx['chat_count']} chats | Last: {last_chat} | {summary_preview}"
                f.write(line + '\n')
            temp_file = f.name
        
        # Set PGPASSWORD if available
        if AI_CHATS_DB_CONFIG.get('password'):
            os.environ['PGPASSWORD'] = AI_CHATS_DB_CONFIG['password']
        
        # Create preview script
        preview_script = """#!/bin/bash
export PGPASSWORD='{}'
uuid="${{1%% *}}"
# Find full UUID from partial
full_uuid=$(psql -h {} -p {} -d {} -U {} -t -c "SELECT context_uuid FROM ai_chat_contexts WHERE context_uuid::text LIKE '$uuid%' LIMIT 1" 2>/dev/null | tr -d ' ')
if [ -z "$full_uuid" ]; then
    echo "Context not found"
    exit 1
fi
# Get context details
psql -h {} -p {} -d {} -U {} -t -c "
SELECT 
    '━━━ CONTEXT: ' || name || ' ━━━' || E'\\n' ||
    'UUID: ' || context_uuid || E'\\n' ||
    'Created: ' || created_at || E'\\n' ||
    'Updated: ' || updated_at || E'\\n' ||
    CASE WHEN array_length(tags, 1) > 0 THEN 'Tags: ' || array_to_string(tags, ', ') || E'\\n' ELSE '' END ||
    E'\\n━━━ SUMMARY ━━━\\n' || COALESCE(summary, 'No summary available')
FROM ai_chat_contexts 
WHERE context_uuid = '$full_uuid'::uuid
" 2>/dev/null

echo -e "\\n━━━ RECENT CHATS ━━━"
psql -h {} -p {} -d {} -U {} -t -c "
SELECT 
    '• ' || to_char(request_timestamp, 'YYYY-MM-DD HH24:MI') || ' | ' || 
    provider || '/' || model || ' | ' || 
    LEFT(regexp_replace(prompt, E'[\\n\\r]+', ' ', 'g'), 60) || '...'
FROM ai_chats 
WHERE context_uuid = '$full_uuid'::uuid
ORDER BY request_timestamp DESC
LIMIT 5
" 2>/dev/null
""".format(
            AI_CHATS_DB_CONFIG.get('password', ''),
            AI_CHATS_DB_CONFIG['host'],
            AI_CHATS_DB_CONFIG['port'],
            AI_CHATS_DB_CONFIG['database'],
            AI_CHATS_DB_CONFIG['user'],
            AI_CHATS_DB_CONFIG['host'],
            AI_CHATS_DB_CONFIG['port'],
            AI_CHATS_DB_CONFIG['database'],
            AI_CHATS_DB_CONFIG['user'],
            AI_CHATS_DB_CONFIG['host'],
            AI_CHATS_DB_CONFIG['port'],
            AI_CHATS_DB_CONFIG['database'],
            AI_CHATS_DB_CONFIG['user']
        )
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False) as f:
            f.write(preview_script)
            preview_file = f.name
        
        os.chmod(preview_file, 0o755)
        
        try:
            # Find fzf command
            import shutil
            fzf_path = shutil.which('fzf')
            if not fzf_path:
                print("Error: fzf command not found. Please install fzf.", file=sys.stderr)
                return False
            
            # Run fzf
            fzf_cmd = [
                fzf_path,
                '--preview', f'{preview_file} {{}}',
                '--preview-window', 'down:70%:wrap',
                '--header', 'Select a context (Enter to view/continue, Ctrl-C to cancel)',
                '--bind', 'ctrl-/:toggle-preview'
            ]
            
            result = subprocess.run(fzf_cmd, stdin=open(temp_file), capture_output=True, text=True)
            
            if result.returncode == 0 and result.stdout.strip():
                selected = result.stdout.strip()
                context_uuid_short = selected.split(' | ')[0]
                
                # Find the full UUID from the short one
                conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
                cursor = conn.cursor()
                cursor.execute("SELECT context_uuid FROM ai_chat_contexts WHERE context_uuid::text LIKE %s LIMIT 1", 
                             (f"{context_uuid_short}%",))
                result_uuid = cursor.fetchone()
                cursor.close()
                conn.close()
                
                if not result_uuid:
                    print(f"Error: Could not find full UUID for {context_uuid_short}", file=sys.stderr)
                    return False
                
                full_context_uuid = str(result_uuid[0])
                
                # View or continue based on --repl flag
                if args.repl:
                    # Continue conversation in this context
                    args.context_id = full_context_uuid
                    start_repl(args)
                else:
                    # Just view the context
                    view_context(full_context_uuid, output_format='text', debug=args.debug, 
                                content_only=args.content_only, render=args.render)
                
                return True
            
        finally:
            os.unlink(temp_file)
            os.unlink(preview_file)
            if 'PGPASSWORD' in os.environ:
                del os.environ['PGPASSWORD']
    
    except Exception as e:
        print(f"Error in fzf context search: {e}", file=sys.stderr)
        return False

def review_chats_by_date(date_str, trace_mode=False, content_only=False, output_format='text', debug=False):
    """Review all chats from a specific date."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        # Validate date format
        try:
            datetime.strptime(date_str, "%Y-%m-%d")
        except ValueError:
            print(f"Error: Invalid date format '{date_str}'. Use YYYY-MM-DD", file=sys.stderr)
            return False
        
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Query for all chats from the specified date
        sql = """
        SELECT 
            a.id,
            a.prompt,
            a.response,
            a.provider,
            a.model,
            a.request_timestamp,
            a.duration_ms,
            a.cost_total_usd,
            a.tags,
            a.context_uuid,
            c.name as context_name
        FROM ai_chats a
        JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
        WHERE DATE(a.request_timestamp) = %s
        ORDER BY a.request_timestamp ASC
        """
        
        cursor.execute(sql, [date_str])
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        if output_format == 'json':
            print(json.dumps([dict(r) for r in results], default=str, indent=2))
        else:
            if not results:
                print(f"No chats found for {date_str}.")
                return True
            
            if trace_mode:
                # Show table format with chat-id, context-id, and context title
                print(f"\nChat Review for {date_str} (Trace Mode)")
                print("=" * 80)
                print(f"{'Chat ID':<12} {'Context ID':<12} {'Time':<10} {'Context Title':<30} {'Model':<15}")
                print("-" * 80)
                
                for r in results:
                    chat_id_short = str(r['id'])[:8]
                    context_id_short = str(r['context_uuid'])[:8]
                    time_str = r['request_timestamp'].strftime('%H:%M:%S')
                    context_name = (r['context_name'] or 'Unnamed')[:28]
                    model_name = r['model'][:13]
                    
                    print(f"{chat_id_short:<12} {context_id_short:<12} {time_str:<10} {context_name:<30} {model_name:<15}")
                
                print(f"\nTotal: {len(results)} chats")
            
            else:
                # Show full content format
                if content_only:
                    # Generate markdown content for content-only mode
                    markdown_parts = [f"# Chat Review for {date_str}", ""]
                    
                    for i, r in enumerate(results, 1):
                        chat_id = str(r['id'])[:8]
                        timestamp = r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')
                        markdown_parts.append(f"## {i}. Chat {chat_id} - {r['provider']}/{r['model']}")
                        markdown_parts.append("")
                        markdown_parts.append(f"**Context:** {r['context_name']} ({str(r['context_uuid'])[:8]}...)")
                        markdown_parts.append(f"**Time:** {timestamp}")
                        if r['duration_ms']:
                            markdown_parts.append(f"**Duration:** {r['duration_ms']}ms")
                        if r['cost_total_usd']:
                            markdown_parts.append(f"**Cost:** ${r['cost_total_usd']:.4f}")
                        if r['tags']:
                            markdown_parts.append(f"**Tags:** {', '.join(r['tags'])}")
                        markdown_parts.append("")
                        markdown_parts.append("### Question")
                        markdown_parts.append("")
                        markdown_parts.append(r['prompt'])
                        markdown_parts.append("")
                        markdown_parts.append("### Response")
                        markdown_parts.append("")
                        markdown_parts.append(r['response'])
                        markdown_parts.append("")
                        markdown_parts.append("---")
                        markdown_parts.append("")
                    
                    # Remove the last separator
                    if markdown_parts and markdown_parts[-2] == "---":
                        markdown_parts = markdown_parts[:-2]
                    
                    markdown_parts.append(f"\n**Total:** {len(results)} chats")
                    
                    markdown_content = "\n".join(markdown_parts)
                    rendered_output = render_markdown(markdown_content)
                    if rendered_output:  # Fallback if markdown rendering failed
                        print(rendered_output)
                    else:
                        print(markdown_content)
                else:
                    # Show regular format with snippets
                    print(f"\nChat Review for {date_str}")
                    print("=" * 50)
                    
                    for i, r in enumerate(results, 1):
                        print(f"\n{i}. [{str(r['id'])[:8]}] {r['provider']}/{r['model']}")
                        print(f"   Context: {r['context_name']} ({str(r['context_uuid'])[:8]}...)")
                        print(f"   Time: {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                        if r['duration_ms']:
                            print(f"   Duration: {r['duration_ms']}ms")
                        if r['cost_total_usd']:
                            print(f"   Cost: ${r['cost_total_usd']:.4f}")
                        if r['tags']:
                            print(f"   Tags: {', '.join(r['tags'])}")
                        
                        prompt_snippet = r['prompt'][:200]
                        response_snippet = r['response'][:200]
                        print(f"   Prompt: {prompt_snippet}{'...' if len(r['prompt']) > 200 else ''}")
                        print(f"   Response: {response_snippet}{'...' if len(r['response']) > 200 else ''}")
                    
                    print(f"\nTotal: {len(results)} chats")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error reviewing chats: {e}", file=sys.stderr)
        return False

def view_ai_chat(chat_id, output_format='text', debug=False, show_history=False, show_full_history=False, trace_mode=False, content_only=False, render=False):
    """View full AI chat by UUID with optional conversation history."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Check if chat_id is a partial UUID (8 characters)
        if len(chat_id) == 8:
            # Search for UUIDs starting with this prefix
            cursor.execute("""
                SELECT a.*, c.name as context_name 
                FROM ai_chats a 
                JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid 
                WHERE CAST(a.id AS TEXT) LIKE %s
            """, [f"{chat_id}%"])
            
            results = cursor.fetchall()
            
            if not results:
                cursor.close()
                conn.close()
                print(f"No chat found with ID starting with: {chat_id}")
                return False
            elif len(results) > 1:
                cursor.close()
                conn.close()
                print(f"Multiple chats found with ID starting with '{chat_id}':")
                for r in results:
                    print(f"  - {r['id']} ({r['provider']}/{r['model']}, {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')})")
                print("Please provide the full UUID to view a specific chat.")
                return False
            else:
                result = results[0]
                chat_id = str(result['id'])  # Use full UUID for history functionality
        else:
            # Try to use as full UUID
            cursor.execute("""
                SELECT a.*, c.name as context_name 
                FROM ai_chats a 
                JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid 
                WHERE a.id = %s::uuid
            """, [chat_id])
            
            result = cursor.fetchone()
            
            if not result:
                cursor.close()
                conn.close()
                print(f"No chat found with ID: {chat_id}")
                return False
        
        cursor.close()
        conn.close()
        
        # Handle history reconstruction if requested
        if show_history or show_full_history:
            history = build_conversation_history(chat_id, include_forward=show_full_history, debug=debug)
            if history:
                history_content = format_conversation_history(history, trace_mode=trace_mode, content_only=content_only)
                if content_only:
                    rendered_output = render_markdown(history_content)
                    if rendered_output:  # Fallback if markdown rendering failed
                        print(rendered_output)
                    else:
                        print(history_content)
                else:
                    print(history_content)
                return True
            elif show_history or show_full_history:
                print("No conversation history found. Showing individual chat:")
        
        # Handle content-only format
        if content_only:
            markdown_content = format_chat_content_only(result)
            rendered_output = render_markdown(markdown_content)
            if rendered_output:  # Fallback if markdown rendering failed
                print(rendered_output)
        elif output_format == 'json':
            print(json.dumps(dict(result), default=str, indent=2))
        else:
            print("=" * 80)
            print(f"Chat ID: {result['id']}")
            print(f"Context: {result['context_name']} ({result['context_uuid']})")
            print(f"Provider: {result['provider']}")
            print(f"Model: {result['model']}")
            print(f"Request Time: {result['request_timestamp']}")
            if result['response_timestamp']:
                print(f"Response Time: {result['response_timestamp']}")
            if result['duration_ms']:
                print(f"Duration: {result['duration_ms']}ms")
            if result['tokens_input']:
                print(f"Input Tokens: {result['tokens_input']}")
            if result['tokens_output']:
                print(f"Output Tokens: {result['tokens_output']}")
            if result['cost_total_usd']:
                print(f"Total Cost: ${result['cost_total_usd']:.6f}")
            if result['tags']:
                print(f"Tags: {', '.join(result['tags'])}")
            print("=" * 80)
            print("\nPROMPT:")
            print("-" * 40)
            print(result['prompt'])
            print("\nRESPONSE:")
            print("-" * 40)
            if render:
                render_markdown(result['response'])
            else:
                print(result['response'])
            if result['metadata'] and result['metadata'] != '{}':
                print("\nMETADATA:")
                print("-" * 40)
                metadata = json.loads(result['metadata']) if isinstance(result['metadata'], str) else result['metadata']
                print(json.dumps(metadata, indent=2))
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error viewing AI chat: {e}", file=sys.stderr)
        return False

def delete_ai_chat(chat_id, confirm=False, debug=False):
    """Delete AI chat by UUID."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # First, get the chat details
        cursor.execute("""
            SELECT id, LEFT(prompt, 100) as prompt_snippet, provider, model, request_timestamp
            FROM ai_chats WHERE id = %s::uuid
        """, [chat_id])
        
        result = cursor.fetchone()
        
        if not result:
            print(f"No chat found with ID: {chat_id}")
            cursor.close()
            conn.close()
            return False
        
        # Confirm deletion
        if not confirm:
            print("About to delete chat:")
            print(f"  ID: {result['id']}")
            print(f"  Provider/Model: {result['provider']}/{result['model']}")
            print(f"  Time: {result['request_timestamp']}")
            print(f"  Prompt: {result['prompt_snippet']}{'...' if len(result['prompt_snippet']) >= 100 else ''}")
            response = input("Are you sure? (y/N): ")
            if response.lower() != 'y':
                print("Deletion cancelled.")
                cursor.close()
                conn.close()
                return False
        
        # Delete the chat
        cursor.execute("DELETE FROM ai_chats WHERE id = %s::uuid", [chat_id])
        conn.commit()
        
        cursor.close()
        conn.close()
        
        print("Chat deleted successfully.")
        return True
    
    except psycopg2.Error as e:
        print(f"Error deleting AI chat: {e}", file=sys.stderr)
        return False

def get_chat_context(chat_ids, debug=False):
    """Retrieve chat content by UUIDs (full or partial 8-char) for use as context."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for context retrieval", file=sys.stderr)
        return []
    
    if not check_db_available(debug):
        if debug:
            print("Warning: Could not connect to AI chats database for context", file=sys.stderr)
        return []
    
    if not chat_ids:
        return []
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Resolve partial UUIDs to full UUIDs first
        resolved_ids = []
        for chat_id in chat_ids:
            if len(chat_id) == 8:
                # Search for UUIDs starting with this prefix
                cursor.execute("""
                    SELECT id FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s
                """, [f"{chat_id}%"])
                
                matches = cursor.fetchall()
                
                if not matches:
                    print(f"Error: No chat found with ID starting with: {chat_id}", file=sys.stderr)
                    cursor.close()
                    conn.close()
                    return None  # Signal error to caller
                elif len(matches) > 1:
                    print(f"Error: Multiple chats found with ID starting with '{chat_id}':", file=sys.stderr)
                    for match in matches:
                        cursor.execute("""
                            SELECT provider, model, request_timestamp 
                            FROM ai_chats WHERE id = %s::uuid
                        """, [str(match['id'])])
                        details = cursor.fetchone()
                        print(f"  - {match['id']} ({details['provider']}/{details['model']}, {details['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')})", file=sys.stderr)
                    print("Please provide the full UUID to disambiguate.", file=sys.stderr)
                    cursor.close()
                    conn.close()
                    return None  # Signal error to caller
                else:
                    resolved_ids.append(str(matches[0]['id']))
            else:
                # Use as full UUID
                resolved_ids.append(chat_id)
        
        # Now query for all resolved UUIDs
        placeholders = ', '.join(['%s::uuid'] * len(resolved_ids))
        cursor.execute(f"""
            SELECT id, prompt, response, provider, model, request_timestamp, tags
            FROM ai_chats 
            WHERE id IN ({placeholders})
            ORDER BY request_timestamp ASC
        """, resolved_ids)
        
        results = cursor.fetchall()
        cursor.close()
        conn.close()
        
        context_chats = []
        found_ids = set()
        
        for result in results:
            found_ids.add(str(result['id']))
            context_chats.append({
                'id': str(result['id']),
                'prompt': result['prompt'],
                'response': result['response'],
                'provider': result['provider'],
                'model': result['model'],
                'timestamp': result['request_timestamp'],
                'tags': result['tags'] or []
            })
        
        # Report missing chats
        missing_ids = set(resolved_ids) - found_ids
        if missing_ids and debug:
            print(f"Warning: Could not find chats: {', '.join(missing_ids)}", file=sys.stderr)
        
        return context_chats
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to retrieve chat context: {e}", file=sys.stderr)
        return []

def format_context_for_prompt(context_chats):
    """Format retrieved chats into context text for the prompt."""
    if not context_chats:
        return ""
    
    context_parts = ["=== CONTEXT FROM PREVIOUS CONVERSATIONS ===\n"]
    
    for i, chat in enumerate(context_chats, 1):
        context_parts.append(f"Context {i} (ID: {chat['id'][:8]}..., {chat['provider']}/{chat['model']}):")
        
        if chat['tags']:
            context_parts.append(f"Tags: {', '.join(chat['tags'])}")
        
        context_parts.append(f"User: {chat['prompt']}")
        context_parts.append(f"Assistant: {chat['response']}")
        context_parts.append("")  # Empty line between contexts
    
    context_parts.append("=== END CONTEXT ===\n")
    
    return "\n".join(context_parts)

def build_conversation_history(chat_id, include_forward=False, debug=False):
    """Build conversation history by getting all chats in the same context."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for history reconstruction", file=sys.stderr)
        return []
    
    if not check_db_available(debug):
        if debug:
            print("Warning: Could not connect to AI chats database for history", file=sys.stderr)
        return []
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # First, get the context_uuid for the given chat_id
        # Handle partial UUID (8 characters)
        if len(chat_id) == 8:
            cursor.execute("""
                SELECT context_uuid FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s LIMIT 1
            """, [f"{chat_id}%"])
        else:
            cursor.execute("""
                SELECT context_uuid FROM ai_chats WHERE id = %s::uuid
            """, [chat_id])
        
        result = cursor.fetchone()
        if not result:
            cursor.close()
            conn.close()
            if debug:
                print(f"Warning: Chat ID {chat_id} not found", file=sys.stderr)
            return []
        
        context_uuid = result['context_uuid']
        
        # Get all chats in the same context, ordered by timestamp
        cursor.execute("""
            SELECT a.id, a.prompt, a.response, a.provider, a.model, a.request_timestamp, a.tags, a.metadata, 
                   a.context_uuid, c.name as context_name
            FROM ai_chats a
            JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            WHERE a.context_uuid = %s
            ORDER BY a.request_timestamp ASC
        """, [context_uuid])
        
        all_chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        # Convert to the expected format
        conversation_history = []
        for chat in all_chats:
            conversation_history.append({
                'id': str(chat['id']),
                'prompt': chat['prompt'],
                'response': chat['response'],
                'provider': chat['provider'],
                'model': chat['model'],
                'timestamp': chat['request_timestamp'],
                'tags': chat['tags'] or [],
                'context_uuid': str(chat['context_uuid']),
                'context_name': chat['context_name']
            })
        
        if debug:
            print(f"Found {len(conversation_history)} chats in context {context_uuid}", file=sys.stderr)
        
        return conversation_history
    
    except psycopg2.Error as e:
        if debug:
            print(f"Warning: Failed to build conversation history: {e}", file=sys.stderr)
        return []

def view_context(context_uuid, output_format='text', debug=False, content_only=False, by_chat_uuid=False, render=False):
    """View entire conversation context by context_uuid or by chat_uuid."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # If by_chat_uuid is True, lookup context_uuid from chat_uuid
        if by_chat_uuid:
            chat_uuid = context_uuid  # Rename for clarity
            
            # Handle partial chat UUID (8 characters)
            if len(chat_uuid) == 8:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s
                """, [f"{chat_uuid}%"])
                
                results = cursor.fetchall()
                
                if not results:
                    cursor.close()
                    conn.close()
                    print(f"No chat found with UUID starting with: {chat_uuid}")
                    return False
                elif len(results) > 1:
                    cursor.close()
                    conn.close()
                    print(f"Multiple chats found with UUID starting with '{chat_uuid}':")
                    # Get more details for disambiguation
                    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
                    cursor.execute("""
                        SELECT id, provider, model, request_timestamp 
                        FROM ai_chats WHERE CAST(id AS TEXT) LIKE %s
                        ORDER BY request_timestamp DESC
                    """, [f"{chat_uuid}%"])
                    chat_results = cursor.fetchall()
                    for r in chat_results:
                        print(f"  - {r['id']} ({r['provider']}/{r['model']}, {r['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')})")
                    cursor.close()
                    conn.close()
                    print("Please provide the full chat UUID to view its context.")
                    return False
                else:
                    context_uuid = str(results[0]['context_uuid'])
            else:
                # Full chat UUID
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats WHERE id = %s::uuid
                """, [chat_uuid])
                
                result = cursor.fetchone()
                if not result:
                    cursor.close()
                    conn.close()
                    print(f"No chat found with UUID: {chat_uuid}")
                    return False
                
                context_uuid = str(result['context_uuid'])
            
            if debug:
                print(f"Found context UUID {context_uuid} for chat UUID {chat_uuid}", file=sys.stderr)
        
        # Use updated resolver for context UUID/name
        elif not by_chat_uuid:
            # Close current cursor and use resolve_context_id
            cursor.close()
            resolved_context = resolve_context_id(context_uuid, debug)
            if not resolved_context:
                conn.close()
                return False
            
            context_uuid = resolved_context
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get context info
        cursor.execute("""
            SELECT * FROM ai_chat_contexts WHERE context_uuid = %s::uuid
        """, [context_uuid])
        
        context_info = cursor.fetchone()
        if not context_info:
            cursor.close()
            conn.close()
            print(f"No context found with UUID: {context_uuid}")
            return False
        
        # Get all chats in this context
        cursor.execute("""
            SELECT * FROM ai_chats WHERE context_uuid = %s
            ORDER BY request_timestamp ASC
        """, [context_uuid])
        
        chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not chats:
            print(f"No chats found in context: {context_uuid}")
            return False
        
        if content_only:
            # Show content-only format
            output = []
            output.append(f"# {context_info['name']} ({context_info['context_uuid']})")
            output.append("")
            
            # Add summary as first subsection if available
            if context_info['summary']:
                output.append("## Summary")
                output.append("")
                output.append(context_info['summary'])
                output.append("")
                output.append("---")
                output.append("")
            
            for chat in chats:
                full_id = str(chat['id'])
                output.append(f"## Question ({full_id} - {chat['provider']}/{chat['model']})")
                output.append("")
                output.append(chat['prompt'])
                output.append("")
                output.append(f"## Response ({full_id} - {chat['provider']}/{chat['model']})")
                output.append("")
                output.append(chat['response'])
                output.append("")
                output.append("---")
                output.append("")
            
            # Remove last separator
            if output and output[-2] == "---":
                output = output[:-2]
            
            print("\n".join(output))
        elif output_format == 'json':
            context_data = {
                'context': dict(context_info),
                'chats': [dict(chat) for chat in chats]
            }
            print(json.dumps(context_data, default=str, indent=2))
        else:
            # Regular format
            print("=" * 80)
            print(f"Context: {context_info['name']}")
            print(f"Context UUID: {context_info['context_uuid']}")
            print(f"Created: {context_info['created_at']}")
            print(f"Updated: {context_info['updated_at']}")
            print(f"Total Chats: {len(chats)}")
            print("=" * 80)
            
            for i, chat in enumerate(chats, 1):
                print(f"\n[{i}] {chat['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')} | {chat['provider']}/{chat['model']} | {str(chat['id'])[:8]}...")
                if chat['tags']:
                    print(f"   Tags: {', '.join(chat['tags'])}")
                print("-" * 80)
                print(f"PROMPT:\n{chat['prompt']}")
                print(f"\nRESPONSE:")
                if render:
                    render_markdown(chat['response'])
                else:
                    print(chat['response'])
            
            # Display summary at the bottom if available
            if context_info['summary']:
                print("\n" + "=" * 80)
                print("CONTEXT SUMMARY:")
                print("=" * 80)
                print(context_info['summary'])
                if context_info['summary_last_updated']:
                    print(f"\nLast updated: {context_info['summary_last_updated'].strftime('%Y-%m-%d %H:%M:%S')}")
                if context_info['summary_provider'] and context_info['summary_model']:
                    print(f"Generated by: {context_info['summary_provider']}/{context_info['summary_model']}")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error viewing context: {e}", file=sys.stderr)
        return False

def edit_context(context_identifier, name=None, debug=False):
    """Edit context properties like name."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    if not name:
        print("Error: --name is required when using --edit-context", file=sys.stderr)
        return False
    
    try:
        # Resolve context identifier (UUID, partial UUID, or name)
        resolved_context = resolve_context_id(context_identifier, debug)
        if not resolved_context:
            return False
        
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Update the context name
        cursor.execute("""
            UPDATE ai_chat_contexts SET name = %s WHERE context_uuid = %s::uuid
            RETURNING *
        """, [name, resolved_context])
        
        result = cursor.fetchone()
        if not result:
            cursor.close()
            conn.close()
            print(f"No context found with UUID: {context_uuid}")
            return False
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"Context updated successfully:")
        print(f"  UUID: {result['context_uuid']}")
        print(f"  Name: {result['name']}")
        print(f"  Updated: {result['updated_at']}")
        
        return True
    
    except psycopg2.Error as e:
        print(f"Error editing context: {e}", file=sys.stderr)
        return False

def summarize_context(context_identifier, model=None, debug=False):
    """Generate a summary for a context using the specified model."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-4"  # Default model
    
    # Get provider for the model
    provider = get_provider_for_model(model)
    if not provider:
        print(f"Error: No provider found for model {model}", file=sys.stderr)
        return False
    
    try:
        # Resolve context identifier (UUID, partial UUID, or name)
        resolved_context = resolve_context_id(context_identifier, debug)
        if not resolved_context:
            return False
        
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get all chats in this context
        cursor.execute("""
            SELECT a.*, c.name as context_name
            FROM ai_chats a
            JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            WHERE a.context_uuid = %s
            ORDER BY a.request_timestamp ASC
        """, [resolved_context])
        
        chats = cursor.fetchall()
        
        if not chats:
            cursor.close()
            conn.close()
            print(f"No chats found in context: {resolved_context}")
            return False
        
        # Build conversation text for summarization
        conversation_text = []
        for chat in chats:
            conversation_text.append(f"User: {chat['prompt']}")
            conversation_text.append(f"Assistant: {chat['response']}")
        
        full_conversation = "\n\n".join(conversation_text)
        
        # Create summarization prompt
        summary_prompt = f"""Please provide a comprehensive summary of this conversation. Focus on the main topics discussed, key decisions made, and important information exchanged. Keep the summary informative but concise (2-3 paragraphs maximum).

Conversation:
{full_conversation}

Summary:"""
        
        print(f"Generating summary using {provider}/{model}...")
        
        # Run the summarization using the specified provider/model
        import subprocess
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(summary_prompt)
            temp_file = f.name
        
        try:
            cmd = [provider, "--model", model, "--no-preserve", "-f", temp_file]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode != 0:
                print(f"Error running {provider}: {result.stderr}", file=sys.stderr)
                return False
            
            summary = result.stdout.strip()
            if not summary:
                print("Error: Empty summary generated", file=sys.stderr)
                return False
            
            # Update context with summary
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET summary = %s, summary_provider = %s, summary_model = %s, summary_last_updated = CURRENT_TIMESTAMP
                WHERE context_uuid = %s
                RETURNING *
            """, [summary, provider, model, resolved_context])
            
            result_row = cursor.fetchone()
            conn.commit()
            cursor.close()
            conn.close()
            
            print(f"Summary generated successfully for context: {chats[0]['context_name']}")
            print(f"Context UUID: {resolved_context}")
            print(f"Summary Provider/Model: {provider}/{model}")
            print(f"\nSummary:\n{summary}")
            
            return True
            
        finally:
            import os
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    except psycopg2.Error as e:
        print(f"Error summarizing context: {e}", file=sys.stderr)
        return False
    except subprocess.TimeoutExpired:
        print("Error: Summary generation timed out", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during summarization: {e}", file=sys.stderr)
        return False

def generate_context_name(context_identifier, model=None, debug=False):
    """Generate a name for a context based on its summary."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-4"  # Default model
    
    # Get provider for the model
    provider = get_provider_for_model(model)
    if not provider:
        print(f"Error: No provider found for model {model}", file=sys.stderr)
        return False
    
    try:
        # Resolve context identifier (UUID, partial UUID, or name)
        resolved_context = resolve_context_id(context_identifier, debug)
        if not resolved_context:
            return False
        
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get context info
        cursor.execute("""
            SELECT * FROM ai_chat_contexts WHERE context_uuid = %s
        """, [resolved_context])
        
        context_info = cursor.fetchone()
        if not context_info:
            cursor.close()
            conn.close()
            print(f"No context found with UUID: {resolved_context}")
            return False
        
        # Check if we have a summary
        if not context_info['summary']:
            print(f"No summary found for context. Generating summary first...")
            cursor.close()
            conn.close()
            
            # Generate summary first
            if not summarize_context(resolved_context, model, debug):
                return False
            
            # Re-fetch context info
            conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            cursor.execute("""
                SELECT * FROM ai_chat_contexts WHERE context_uuid = %s
            """, [resolved_context])
            context_info = cursor.fetchone()
        
        summary = context_info['summary']
        
        # Create name generation prompt
        name_prompt = f"""Based on the following conversation summary, generate a concise, descriptive name for this conversation context. The name should be 8 words or less and capture the main topic or purpose of the conversation.

Summary:
{summary}

Generate a name that is:
- 8 words or less
- Descriptive and clear
- Suitable as a conversation title
- Focused on the main topic

Name:"""
        
        print(f"Generating context name using {provider}/{model}...")
        
        # Run the name generation using the specified provider/model
        import subprocess
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(name_prompt)
            temp_file = f.name
        
        try:
            cmd = [provider, "--model", model, "--no-preserve", "-f", temp_file]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            
            if result.returncode != 0:
                print(f"Error running {provider}: {result.stderr}", file=sys.stderr)
                return False
            
            generated_name = result.stdout.strip()
            if not generated_name:
                print("Error: Empty name generated", file=sys.stderr)
                return False
            
            # Clean up the generated name (remove quotes, limit length)
            generated_name = generated_name.strip('"\'').strip()
            words = generated_name.split()
            if len(words) > 8:
                generated_name = ' '.join(words[:8])
            
            # Update context with the new name
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET name = %s
                WHERE context_uuid = %s
                RETURNING *
            """, [generated_name, resolved_context])
            
            result_row = cursor.fetchone()
            conn.commit()
            cursor.close()
            conn.close()
            
            print(f"Name generated successfully for context:")
            print(f"Context UUID: {resolved_context}")
            print(f"New Name: {generated_name}")
            print(f"Generated by: {provider}/{model}")
            
            return True
            
        finally:
            import os
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    except psycopg2.Error as e:
        print(f"Error generating context name: {e}", file=sys.stderr)
        return False
    except subprocess.TimeoutExpired:
        print("Error: Name generation timed out", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during name generation: {e}", file=sys.stderr)
        return False

def generate_context_names_missing(model=None, debug=False):
    """Generate names for all contexts that are currently missing names (marked as '<untitled>')."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-4"  # Default model
    
    # Get provider for the model
    provider = get_provider_for_model(model)
    if not provider:
        print(f"Error: No provider found for model {model}", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Find all contexts that need names (have '<untitled>' or empty names)
        cursor.execute("""
            SELECT context_uuid, name, summary, created_at,
                   (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count
            FROM ai_chat_contexts ctx
            WHERE name IS NULL OR name = '' OR name = '<untitled>'
            ORDER BY created_at DESC
        """)
        
        contexts_to_name = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not contexts_to_name:
            print("No contexts found that need names. All contexts already have names!")
            return True
        
        print(f"Found {len(contexts_to_name)} contexts that need names")
        print(f"Using model: {provider}/{model}")
        print("=" * 60)
        
        success_count = 0
        error_count = 0
        
        for i, context in enumerate(contexts_to_name, 1):
            context_uuid = context['context_uuid']
            chat_count = context['chat_count']
            created_at = context['created_at'].strftime('%Y-%m-%d %H:%M:%S')
            
            print(f"\n{i}/{len(contexts_to_name)}. Processing context {str(context_uuid)[:8]}...")
            print(f"   Created: {created_at}")
            print(f"   Chats: {chat_count}")
            
            # Skip contexts with no chats
            if chat_count == 0:
                print("   ⏭️  Skipping - no chats in this context")
                continue
            
            try:
                # Use the existing generate_context_name function for individual processing
                if generate_context_name(str(context_uuid), model=model, debug=debug):
                    success_count += 1
                    print(f"   ✅ Name generated successfully")
                else:
                    error_count += 1
                    print(f"   ❌ Failed to generate name")
                    
            except Exception as e:
                error_count += 1
                print(f"   ❌ Error: {e}")
                if debug:
                    print(f"   Debug: {e}", file=sys.stderr)
        
        print("\n" + "=" * 60)
        print(f"📊 Summary:")
        print(f"   Total contexts processed: {len(contexts_to_name)}")
        print(f"   ✅ Successfully named: {success_count}")
        print(f"   ❌ Errors: {error_count}")
        print(f"   📈 Success rate: {success_count/(success_count+error_count)*100:.1f}%" if (success_count + error_count) > 0 else "")
        
        return error_count == 0
        
    except psycopg2.Error as e:
        print(f"Error accessing database: {e}", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during bulk name generation: {e}", file=sys.stderr)
        return False

def summarize_contexts_missing(model=None, debug=False):
    """Summarize all contexts that don't have summaries."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-4"  # Default model
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Find contexts without summaries
        cursor.execute("""
            SELECT context_uuid, name FROM ai_chat_contexts 
            WHERE summary IS NULL OR summary = ''
            ORDER BY created_at DESC
        """)
        
        contexts_to_summarize = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not contexts_to_summarize:
            print("No contexts found that need summaries.")
            return True
        
        print(f"Found {len(contexts_to_summarize)} contexts without summaries.")
        print(f"Will summarize using model: {model}")
        
        success_count = 0
        failure_count = 0
        
        for context in contexts_to_summarize:
            context_uuid = str(context['context_uuid'])
            context_name = context['name'] or '<untitled>'
            
            print(f"\nSummarizing context: {context_name} ({context_uuid[:8]}...)")
            
            if summarize_context(context_uuid, model, debug):
                success_count += 1
                print(f"✓ Successfully summarized context {context_uuid[:8]}...")
            else:
                failure_count += 1
                print(f"✗ Failed to summarize context {context_uuid[:8]}...")
        
        print(f"\nSummary complete:")
        print(f"- Successfully summarized: {success_count}")
        print(f"- Failed to summarize: {failure_count}")
        
        return failure_count == 0
        
    except psycopg2.Error as e:
        print(f"Error finding contexts to summarize: {e}", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during batch summarization: {e}", file=sys.stderr)
        return False

def summarize_contexts_outdated(model=None, debug=False):
    """Update summaries for contexts with newer chats than summary_last_updated."""
    if not DB_AVAILABLE:
        print("Error: Database dependencies not available", file=sys.stderr)
        return False
    
    if not check_db_available(debug):
        print("Error: Could not connect to AI chats database", file=sys.stderr)
        return False
    
    # Use default model if not specified
    if not model:
        model = "grok-4"  # Default model
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Find contexts that have chats newer than their summary_last_updated
        cursor.execute("""
            SELECT DISTINCT ctx.context_uuid, ctx.name, ctx.summary_last_updated,
                   MAX(chat.request_timestamp) as latest_chat
            FROM ai_chat_contexts ctx
            JOIN ai_chats chat ON ctx.context_uuid = chat.context_uuid
            WHERE ctx.summary IS NOT NULL 
              AND ctx.summary != ''
              AND (ctx.summary_last_updated IS NULL 
                   OR chat.request_timestamp > ctx.summary_last_updated)
            GROUP BY ctx.context_uuid, ctx.name, ctx.summary_last_updated
            ORDER BY latest_chat DESC
        """)
        
        contexts_to_update = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not contexts_to_update:
            print("No contexts found with outdated summaries.")
            return True
        
        print(f"Found {len(contexts_to_update)} contexts with outdated summaries.")
        print(f"Will update summaries using model: {model}")
        
        success_count = 0
        failure_count = 0
        
        for context in contexts_to_update:
            context_uuid = str(context['context_uuid'])
            context_name = context['name'] or '<untitled>'
            last_updated = context['summary_last_updated']
            latest_chat = context['latest_chat']
            
            print(f"\nUpdating summary for context: {context_name} ({context_uuid[:8]}...)")
            if last_updated:
                print(f"  Last summary update: {last_updated}")
            else:
                print(f"  Last summary update: Never")
            print(f"  Latest chat: {latest_chat}")
            
            if summarize_context(context_uuid, model, debug):
                success_count += 1
                print(f"✓ Successfully updated summary for context {context_uuid[:8]}...")
            else:
                failure_count += 1
                print(f"✗ Failed to update summary for context {context_uuid[:8]}...")
        
        print(f"\nSummary update complete:")
        print(f"- Successfully updated: {success_count}")
        print(f"- Failed to update: {failure_count}")
        
        return failure_count == 0
        
    except psycopg2.Error as e:
        print(f"Error finding contexts to update: {e}", file=sys.stderr)
        return False
    except Exception as e:
        print(f"Error during batch summary update: {e}", file=sys.stderr)
        return False

def resolve_context_id(context_identifier, debug=False):
    """Resolve a context identifier (UUID, partial UUID, or name) to a full context UUID."""
    if not DB_AVAILABLE:
        if debug:
            print("Error: Database dependencies not available", file=sys.stderr)
        return None
    
    if not check_db_available(debug):
        if debug:
            print("Error: Could not connect to AI chats database", file=sys.stderr)
        return None
    
    def is_uuid_like(text):
        """Check if text looks like a UUID (full or partial)."""
        import re
        # Full UUID pattern
        if re.match(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', text, re.IGNORECASE):
            return True
        # Partial UUID pattern (8 hex characters)
        if len(text) == 8 and re.match(r'^[0-9a-f]{8}$', text, re.IGNORECASE):
            return True
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Determine if input is UUID-like or a name
        if is_uuid_like(context_identifier):
            # Handle UUID-based lookup
            if len(context_identifier) == 8:
                # Partial UUID
                cursor.execute("""
                    SELECT context_uuid, name FROM ai_chat_contexts 
                    WHERE CAST(context_uuid AS TEXT) LIKE %s
                """, [f"{context_identifier}%"])
                
                results = cursor.fetchall()
                
                if not results:
                    cursor.close()
                    conn.close()
                    print(f"No context found with UUID starting with: {context_identifier}", file=sys.stderr)
                    return None
                elif len(results) > 1:
                    cursor.close()
                    conn.close()
                    print(f"Multiple contexts found with UUID starting with '{context_identifier}':", file=sys.stderr)
                    for r in results:
                        context_name = r['name'] or '<untitled>'
                        print(f"  - {r['context_uuid']} ({context_name})", file=sys.stderr)
                    print("Please provide the full UUID to specify a unique context.", file=sys.stderr)
                    return None
                else:
                    resolved_uuid = str(results[0]['context_uuid'])
                    if debug:
                        context_name = results[0]['name'] or '<untitled>'
                        print(f"Resolved partial UUID {context_identifier} to {resolved_uuid} ({context_name})", file=sys.stderr)
                    cursor.close()
                    conn.close()
                    return resolved_uuid
            else:
                # Full UUID - validate it exists
                cursor.execute("""
                    SELECT context_uuid, name FROM ai_chat_contexts 
                    WHERE context_uuid = %s::uuid
                """, [context_identifier])
                
                result = cursor.fetchone()
                if not result:
                    cursor.close()
                    conn.close()
                    print(f"No context found with UUID: {context_identifier}", file=sys.stderr)
                    return None
                
                if debug:
                    context_name = result['name'] or '<untitled>'
                    print(f"Validated UUID {context_identifier} ({context_name})", file=sys.stderr)
                
                cursor.close()
                conn.close()
                return context_identifier
        else:
            # Handle name-based lookup
            cursor.execute("""
                SELECT context_uuid, name FROM ai_chat_contexts 
                WHERE name = %s
            """, [context_identifier])
            
            results = cursor.fetchall()
            
            if not results:
                cursor.close()
                conn.close()
                print(f"No context found with name: {context_identifier}", file=sys.stderr)
                return None
            elif len(results) > 1:
                cursor.close()
                conn.close()
                print(f"Multiple contexts found with name '{context_identifier}':", file=sys.stderr)
                for r in results:
                    print(f"  - {r['context_uuid']} ({r['name']})", file=sys.stderr)
                print("Please provide a unique context name or use the UUID.", file=sys.stderr)
                return None
            else:
                resolved_uuid = str(results[0]['context_uuid'])
                if debug:
                    print(f"Resolved name '{context_identifier}' to {resolved_uuid}", file=sys.stderr)
                cursor.close()
                conn.close()
                return resolved_uuid
    
    except psycopg2.Error as e:
        if debug:
            print(f"Error resolving context identifier: {e}", file=sys.stderr)
        return None
    except Exception as e:
        if debug:
            print(f"Error during context identifier resolution: {e}", file=sys.stderr)
        return None

def get_last_chat_context(debug=False):
    """Get the context UUID of the most recently added chat."""
    if not DB_AVAILABLE:
        if debug:
            print("Error: Database dependencies not available", file=sys.stderr)
        return None
    
    if not check_db_available(debug):
        if debug:
            print("Error: Could not connect to AI chats database", file=sys.stderr)
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get the most recent chat's context_uuid
        cursor.execute("""
            SELECT a.context_uuid, c.name as context_name, a.request_timestamp
            FROM ai_chats a
            LEFT JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
            ORDER BY a.request_timestamp DESC
            LIMIT 1
        """)
        
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if not result:
            if debug:
                print("No previous chats found in database", file=sys.stderr)
            return None
        
        context_uuid = str(result['context_uuid'])
        context_name = result['context_name'] or '<untitled>'
        
        if debug:
            print(f"Found last chat context: {context_name} ({context_uuid})", file=sys.stderr)
        
        return context_uuid
        
    except psycopg2.Error as e:
        if debug:
            print(f"Error getting last chat context: {e}", file=sys.stderr)
        return None
    except Exception as e:
        if debug:
            print(f"Error during last chat context lookup: {e}", file=sys.stderr)
        return None

def auto_generate_context_name(context_uuid, model, prompt, response, debug=False):
    """Automatically generate a name for a context based on the first chat."""
    if not DB_AVAILABLE:
        if debug:
            print("Warning: Database not available for auto-naming", file=sys.stderr)
        return False
    
    # Get provider for the model
    provider = get_provider_for_model(model)
    if not provider:
        if debug:
            print(f"Warning: No provider found for model {model} for auto-naming", file=sys.stderr)
        return False
    
    try:
        # Create name generation prompt based on the conversation content
        name_prompt = f"""Based on the following conversation, generate a concise, descriptive name for this conversation context. The name should be 8 words or less and capture the main topic or purpose of the conversation.

User Question:
{prompt}

AI Response:
{response}

Generate a name that is:
- 8 words or less
- Descriptive and clear
- Suitable as a conversation title
- Focused on the main topic

Name:"""
        
        if debug:
            print(f"Auto-generating context name using {provider}/{model}...", file=sys.stderr)
        
        # Run the name generation using the specified provider/model
        import subprocess
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(name_prompt)
            temp_file = f.name
        
        try:
            cmd = [provider, "--model", model, "--no-preserve", "-f", temp_file]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            
            if result.returncode != 0:
                if debug:
                    print(f"Warning: Error running {provider} for auto-naming: {result.stderr}", file=sys.stderr)
                return False
            
            generated_name = result.stdout.strip()
            if not generated_name:
                if debug:
                    print("Warning: Empty name generated for auto-naming", file=sys.stderr)
                return False
            
            # Clean up the generated name (remove quotes, limit length)
            generated_name = generated_name.strip('"\'').strip()
            words = generated_name.split()
            if len(words) > 8:
                generated_name = ' '.join(words[:8])
            
            # Update context with the new name
            conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET name = %s
                WHERE context_uuid = %s
                RETURNING name
            """, [generated_name, context_uuid])
            
            result_row = cursor.fetchone()
            conn.commit()
            cursor.close()
            conn.close()
            
            if debug:
                print(f"Auto-generated context name: '{generated_name}' for {context_uuid}", file=sys.stderr)
            
            return True
            
        finally:
            # Clean up temp file
            import os
            try:
                os.unlink(temp_file)
            except:
                pass
                
    except Exception as e:
        if debug:
            print(f"Warning: Error during auto-naming: {e}", file=sys.stderr)
        return False

def format_chat_content_only(chat_data):
    """Format a single chat in content-only markdown format."""
    # Use full UUID for chat ID
    full_id = str(chat_data['id'])
    context_uuid = str(chat_data['context_uuid'])
    context_name = chat_data.get('context_name', '<untitled>')
    
    output = []
    output.append(f"# {context_name} ({context_uuid})")
    output.append("")
    output.append(f"## Question ({full_id} - {chat_data['provider']}/{chat_data['model']})")
    output.append("")
    output.append(chat_data['prompt'])
    output.append("")
    output.append(f"## Response ({full_id} - {chat_data['provider']}/{chat_data['model']})")
    output.append("")
    output.append(chat_data['response'])
    
    return "\n".join(output)

def format_conversation_history(history, trace_mode=False, content_only=False):
    """Format conversation history for display."""
    if not history:
        return "No conversation history found."
    
    if content_only:
        # Show content-only format for the entire conversation
        output = []
        # Use the first chat's context info for the main header
        if history:
            first_chat = history[0]
            context_name = first_chat.get('context_name', '<untitled>')
            context_uuid = first_chat.get('context_uuid', 'unknown')
            output.append(f"# {context_name} ({context_uuid})")
            output.append("")
        
        for chat in history:
            full_id = chat['id']
            output.append(f"## Question ({full_id} - {chat['provider']}/{chat['model']})")
            output.append("")
            output.append(chat['prompt'])
            output.append("")
            output.append(f"## Response ({full_id} - {chat['provider']}/{chat['model']})")
            output.append("")
            output.append(chat['response'])
            output.append("")
            output.append("---")
            output.append("")
        
        # Remove the last separator
        if output and output[-2] == "---":
            output = output[:-2]
        
        return "\n".join(output)
    elif trace_mode:
        # Show trace format
        output = ["Conversation Flow Trace:"]
        output.append("=" * 40)
        
        for i, chat in enumerate(history):
            timestamp = chat['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            tags_str = f" [{', '.join(chat['tags'])}]" if chat['tags'] else ""
            output.append(f"{i+1:2d}. {chat['id'][:8]}... | {timestamp} | {chat['provider']}/{chat['model']}{tags_str}")
        
        return "\n".join(output)
    else:
        # Show full conversation format
        output = ["Conversation History:"]
        output.append("=" * 80)
        
        for i, chat in enumerate(history):
            timestamp = chat['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            tags_str = f" [{', '.join(chat['tags'])}]" if chat['tags'] else ""
            
            output.append(f"\n[{i+1}] {timestamp} | {chat['provider']}/{chat['model']} | {chat['id'][:8]}...{tags_str}")
            output.append("-" * 80)
            output.append(f"User: {chat['prompt']}")
            output.append(f"\nAssistant: {chat['response']}")
            output.append("")
        
        return "\n".join(output)

def pick_best_response(prompt, model_responses, decision_model):
    """Use a reasoning model to pick the best response from multiple models."""
    # Prepare the decision prompt
    decision_prompt = f"""Original User Query:
{prompt}

I received multiple AI responses to this query. Please analyze them and select the BEST response based on:
1. Accuracy and correctness
2. Completeness and thoroughness
3. Clarity and coherence
4. Relevance to the query
5. Practical usefulness

Here are the responses from different models:

"""
    
    for i, (model, response) in enumerate(model_responses):
        if response.get("success") and response.get("output"):
            decision_prompt += f"=== Response {i+1} from {model} ===\n{response['output']}\n\n"
    
    decision_prompt += """Please analyze all responses and tell me which response number is the BEST (1, 2, 3, etc).
Start your response with "BEST: X" where X is the response number.
Then provide a brief explanation of why that response is superior to the others.
"""
    
    # Get the provider for the decision model
    provider = get_provider_for_model(decision_model)
    
    # Build command for decision model
    cmd = [provider, "--model", decision_model, "--no-preserve", decision_prompt]
    
    try:
        # Run the decision model
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0 and result.stdout:
            # Parse the response to find the best model
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if line.strip().startswith("BEST:"):
                    try:
                        # Extract the number
                        best_num = int(line.split(":")[1].strip().split()[0]) - 1
                        if 0 <= best_num < len(model_responses):
                            best_model = model_responses[best_num][0]
                            reasoning = '\n'.join(lines[1:])  # Rest is explanation
                            return best_model, reasoning.strip()
                    except:
                        pass
            
        # If parsing fails, return the first successful response
        for model, response in model_responses:
            if response.get("success"):
                return model, "Could not determine best response, defaulting to first successful model"
                
    except Exception as e:
        print(f"Error running decision model: {e}", file=sys.stderr)
        # Return first successful response as fallback
        for model, response in model_responses:
            if response.get("success"):
                return model, "Decision model failed, defaulting to first successful model"
    
    return None, "No successful responses to choose from"

def generate_helper_suggestion(args):
    """Use AI to suggest correct aipy command based on user's goal.

    Args:
        args: Parsed command-line arguments

    Returns:
        str: AI-generated command suggestion with explanation
    """
    # Read aipy.md documentation
    aipy_md_path = Path.home() / "Documents/notes/02_areas/repos/dotfiles/aipy.md"
    try:
        with open(aipy_md_path, 'r') as f:
            aipy_docs = f.read()
    except IOError as e:
        print(f"Warning: Could not read aipy.md: {e}", file=sys.stderr)
        aipy_docs = "Documentation not available. Helper will work with limited context."

    # Build context from current arguments
    current_args = []

    # Model
    if args.model and args.model != "grok-4-1-fast-non-reasoning":
        current_args.append(f"--model {args.model}")

    # Files
    if args.files:
        for f in args.files:
            current_args.append(f"-f {f}")

    # Images
    if args.images:
        for img in args.images:
            current_args.append(f"--image {img}")

    # Prompt (positional or --prompt flag)
    if args.prompt:
        current_args.append(f'"{args.prompt}"')
    if args.prompt_flag:
        current_args.append(f'--prompt "{args.prompt_flag}"')

    # Common flags
    if args.tags:
        current_args.append(f'--tags "{args.tags}"')
    if args.context_ids:
        for ctx in args.context_ids:
            current_args.append(f"--chat-id {ctx}")
    if args.debug:
        current_args.append("--debug")
    if args.no_streaming:
        current_args.append("--no-streaming")
    if args.render:
        current_args.append("--render")

    current_args_str = ' '.join(current_args) if current_args else '(none)'

    # Construct prompt for AI
    helper_prompt = f"""You are an expert at using the aipy CLI tool. A user needs help constructing the correct command.

USER'S GOAL:
{args.helper}

ARGUMENTS THEY'VE ALREADY PROVIDED:
{current_args_str}

COMPLETE AIPY DOCUMENTATION:
{aipy_docs}

Based on their goal and the documentation, suggest the complete aipy command they should run.

Format your response EXACTLY as follows:

COMMAND: <the full aipy command>

EXPLANATION:
<2-3 sentence explanation of what this command does and any changes made>

CONSIDERATIONS:
<bullet points of important notes, alternatives, or warnings>

Requirements:
1. The COMMAND line must be copy-pasteable (complete valid command)
2. Start the command with "aipy" (not "./aipy" or path)
3. PRESERVE all their existing arguments (files, images, prompts, etc.)
4. ADD missing flags based on their goal from --helper-prompt
5. IMPROVE their prompt if it's vague or could be more effective (explain improvements in EXPLANATION)
6. If they provided a prompt like "summarize this file", enhance it to be more specific
7. Be concise but thorough
8. If their goal is unclear, provide the most likely interpretation

Important: The user has already specified some args - keep them and add what's missing based on their goal!
"""

    if args.debug:
        print(f"Debug: Helper prompt length: {len(helper_prompt)} chars", file=sys.stderr)

    # Call AI model
    helper_model = "claude-sonnet-4-20250514"
    provider = get_provider_for_model(helper_model)

    cmd = [provider, "--model", helper_model, "--no-preserve", "--no-streaming"]

    try:
        if args.debug:
            print(f"Debug: Calling {provider} with model {helper_model}", file=sys.stderr)

        result = subprocess.run(
            cmd,
            input=helper_prompt,
            capture_output=True,
            text=True,
            timeout=120
        )

        if result.returncode == 0:
            return result.stdout.strip()
        else:
            error_msg = result.stderr.strip() if result.stderr else f"Exit code {result.returncode}"
            return f"Error calling AI helper: {error_msg}"

    except subprocess.TimeoutExpired:
        return "Error: AI helper request timed out (120s limit)"
    except Exception as e:
        return f"Error: {e}"

def save_multi_model_transaction(models, prompt, responses, metadata=None, tags=None, context_ids=None, context_id=None, store_db=True, save_sb_md=False, debug=False, dry_run=False, show_id=False, context_name=None):
    """Save multi-model chat transaction to markdown file organized by date and optionally to database."""
    
    # Save to second-brain markdown file if requested
    if save_sb_md:
        # Create directory structure if it doesn't exist
        base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
        base_path.mkdir(parents=True, exist_ok=True)
        
        # Generate filename based on today's date
        today = datetime.now()
        filename = today.strftime("%Y-%m-%d.md")
        file_path = base_path / filename
        
        # Generate timestamp for section header
        timestamp = today.strftime("%Y-%m-%d %H:%M:%S")
        
        # Prepare content
        content = f"\n# {timestamp} - Multi-Model Query (aipy)\n\n"
        content += f"## Prompt\n\n```\n{prompt}\n```\n\n"
        
        # Add model responses
        content += f"## Model Responses\n\n"
        
        for i, (model, response) in enumerate(zip(models, responses)):
            content += f"### {i+1}. {model}\n\n"
            if response.get("success"):
                content += f"{response.get('output', 'No output captured')}\n\n"
            else:
                content += f"**Error**: {response.get('error', 'Unknown error')}\n\n"
        
        # Add metadata if provided
        if metadata:
            content += f"## Metadata\n\n"
            content += f"- **Provider**: aipy (multi-model wrapper)\n"
            content += f"- **Models**: {', '.join(models)}\n"
            if 'total_time' in metadata:
                content += f"- **Total Time**: {metadata['total_time']:.2f}s\n"
            if 'parallel' in metadata:
                content += f"- **Execution**: {'Parallel' if metadata['parallel'] else 'Sequential'}\n"
            if 'iterations' in metadata:
                content += f"- **Iterations**: {metadata['iterations']}\n"
            if 'best_model' in metadata:
                content += f"- **Best Response**: {metadata['best_model']}\n"
            if 'decision_model' in metadata:
                content += f"- **Decision Model**: {metadata['decision_model']}\n"
            if 'decision_reasoning' in metadata:
                content += f"\n### Decision Reasoning\n\n{metadata['decision_reasoning']}\n"
        
        content += "\n---\n"
        
        # Write to file (append if exists)
        try:
            with open(file_path, 'a', encoding='utf-8') as f:
                f.write(content)
        except Exception as e:
            print(f"Warning: Could not save chat transaction: {e}", file=sys.stderr)
    
    # Store in database if requested and available
    if store_db:
        # For multi-model responses, store each successful response separately
        for i, (model, response) in enumerate(zip(models, responses)):
            if response.get("success") and response.get('output'):
                provider = get_provider_for_model(model)
                
                # Prepare database metadata
                db_metadata = {
                    'request_timestamp': today,
                    'response_timestamp': today,
                    'extra_metadata': {
                        'source': 'aipy_multi_model',
                        'model_index': i,
                        'total_models': len(models),
                        'is_best_response': metadata.get('best_model') == model if metadata else False,
                        'execution_type': 'parallel' if metadata and metadata.get('parallel') else 'sequential',
                        'iterations': metadata.get('iterations', 1) if metadata else 1
                    }
                }
                
                # Add timing if available
                if metadata and 'total_time' in metadata:
                    db_metadata['duration_ms'] = int(metadata['total_time'] * 1000)
                
                # Add decision info if available
                if metadata and metadata.get('best_model') == model:
                    db_metadata['extra_metadata']['decision_model'] = metadata.get('decision_model')
                    db_metadata['extra_metadata']['decision_reasoning'] = metadata.get('decision_reasoning')
                
                new_id = store_chat_in_db(
                    prompt, 
                    response['output'], 
                    provider, 
                    model, 
                    db_metadata,
                    tags=tags,
                    context_ids=context_ids,
                    context_id=context_id,
                    debug=debug, 
                    dry_run=dry_run,
                    show_id=show_id,
                    auto_name=False,  # Not available in this context
                    context_name=context_name
                )
                
                # Store the new ID for summary display
                if new_id and 'new_chat_ids' not in metadata:
                    if not hasattr(metadata, 'get'):
                        metadata = metadata or {}
                    metadata['new_chat_ids'] = []
                if new_id:
                    metadata.setdefault('new_chat_ids', []).append(new_id)

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        prog="aipy",
        description="""
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
                    Universal AI Provider Wrapper
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Intelligently routes requests to appropriate AI providers while maintaining
conversation history, context threading, and advanced search capabilities.

Supported Providers: Claude, Grok, Gemini, OpenAI, Ollama, Perplexity
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
                                EXAMPLES
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

🚀 BASIC USAGE
  aipy "What is quantum computing?"
  aipy --model gpt-4o "Explain machine learning"
  aipy --model claude-3-5-sonnet-latest "Help with coding"

🔀 MULTI-MODEL COMPARISON
  aipy --model "gpt-4o,claude-3-5-sonnet,grok-4" "Compare AI approaches"
  aipy --model "gpt-4o,claude-3-5-sonnet" --pick-best "Complex analysis"

🎨 IMAGE GENERATION
  aipy --image-gen "A beautiful sunset over mountains"
  aipy --times 3 --image-gen "Abstract art in blue tones"
  aipy --image-gen --hd --output sunset.png "Photorealistic sunset"

📁 FILE AND MEDIA INPUT
  aipy -f config.yaml "Explain this configuration"
  aipy -f script.py -f README.md "Review this code project"
  aipy --image screenshot.png "What's shown in this image?"

🔍 DATABASE OPERATIONS
  # Search chat history
  aipy --search "quantum computing"
  aipy --search "python" --provider-filter claudpy
  aipy --search "API" --model-filter gpt-4o
  aipy --search "*" --tags-filter "work,coding"

  # View conversations (supports partial UUIDs)
  aipy --view 3740590a
  aipy --view 3740590a-7356-4136-8247-2657318797b5
  
  # View with conversation history
  aipy --view 3740590a --history          # Show leading conversation
  aipy --view 3740590a --full-history     # Show complete thread
  aipy --view 3740590a --trace            # Show UUID/timestamp flow

  # Delete conversations
  aipy --delete 3740590a
  aipy --delete 3740590a -y               # Skip confirmation

🏷️  CONVERSATION TAGGING
  aipy --tags "work,python,debugging" "Help fix my code"
  aipy --tags "research,ai" "Explain transformers"

🔗 CONTEXT THREADING (supports partial UUIDs)
  aipy --chat-id 3740590a "Continue our discussion"
  aipy --chat-id 3740590a --chat-id 339e0ffc "Build on both conversations"
  aipy --chat-id 3740590a --tags "synthesis" "Synthesize our discussion"
  aipy --continue "Continue from where we left off"
  aipy --auto-name "Start a new conversation with auto-generated name"

📊 TRACKING AND MONITORING
  aipy --show-id "New conversation"        # Shows UUID on stderr
  aipy --summary "Another conversation"    # Shows UUID in summary
  aipy --debug "Test with debug info"      # Detailed execution info
  aipy --dry-run "Preview request"         # Show what would be sent

🛠️  UTILITY COMMANDS
  aipy -L                                  # List all available models
  aipy --search "*" --limit 20            # Show recent 20 conversations
  aipy --view 3740590a --json             # Output in JSON format
  aipy -P "Your question"                 # Include personal preferences with request
  aipy --preferences --model gpt-4o "Complex analysis"  # Use preferences with specific model

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
                           WORKFLOW EXAMPLES
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

📚 Research Workflow:
  1. aipy --search "topic" | head -5           # Find relevant discussions
  2. aipy --chat-id <uuid> --tags "research" "Expand on this topic"
  3. aipy --view <new-uuid> --history          # Review conversation thread

💻 Code Review Workflow:
  1. aipy -f code.py --tags "review" "Review this code"
  2. aipy --chat-id <uuid> --tags "follow-up" "Implement the suggestions"
  3. aipy --view <uuid> --full-history         # See complete review process

🤔 Decision Making:
  aipy --model "gpt-4o,claude-3-5-sonnet,grok-4" --pick-best "Complex decision"

For more information, see: ~/Documents/notes/02_areas/repos/dotfiles/aipy.md
        """
    )
    
    # Positional argument
    parser.add_argument("prompt", nargs="?", 
                        help="The prompt to send to the AI")
    
    # ━━━ CORE OPTIONS ━━━
    core_group = parser.add_argument_group("🚀 Core Options")
    core_group.add_argument("--model", default="grok-4-1-fast-non-reasoning", metavar="MODEL",
                           help="Model(s) to use. Comma-separated for parallel execution (default: grok-4)")
    core_group.add_argument("--provider", choices=["claudpy", "grokpy", "geminpy", "openpy", "ollampy", "perpy"],
                           help="Force specific provider (overrides model-based routing)")
    core_group.add_argument("--prompt", dest="prompt_flag", metavar="TEXT",
                           help="Prompt to prepend to the input")
    core_group.add_argument("--times", type=int, default=1, metavar="N",
                           help="Number of times to repeat request (useful for image generation)")
    core_group.add_argument("-L", "--list-models", action="store_true",
                           help="List all available models and exit")
    core_group.add_argument("--repl", action="store_true",
                           help="Start interactive REPL mode for chat conversation")
    core_group.add_argument("--no-streaming", action="store_true",
                           help="Disable streaming output (wait for complete response)")
    core_group.add_argument("--no-summary", action="store_true",
                           help="Skip displaying context summary in REPL mode")
    core_group.add_argument("--render", action="store_true",
                           help="Render markdown output beautifully (like in REPL mode)")
    
    # ━━━ INPUT OPTIONS ━━━  
    input_group = parser.add_argument_group("📁 Input Options")
    input_group.add_argument("-f", "--file", action="append", dest="files", metavar="FILE",
                            help="Include file content (can be used multiple times)")
    input_group.add_argument("--image", action="append", dest="images", metavar="IMAGE",
                            help="Include image files for vision analysis")
    
    # ━━━ GENERATION MODES ━━━
    gen_group = parser.add_argument_group("🎨 Generation Modes")
    gen_group.add_argument("--image-gen", action="store_true",
                          help="Generate images instead of text")
    gen_group.add_argument("--embedding", action="store_true",
                          help="Generate text embeddings")
    gen_group.add_argument("--regen", action="store_true",
                          help="Regenerate/enhance existing images")
    gen_group.add_argument("--output", metavar="FILE",
                          help="Output filename for generated content")
    gen_group.add_argument("--size", metavar="SIZE",
                          help="Image size (format depends on provider)")
    gen_group.add_argument("--hd", action="store_true",
                          help="Use HD quality for image generation")
    
    # ━━━ MULTI-MODEL OPTIONS ━━━
    multi_group = parser.add_argument_group("🔀 Multi-Model Options")
    multi_group.add_argument("--pick-best", action="store_true",
                            help="Use reasoning model to pick best response from multiple models")
    multi_group.add_argument("--decision-model", default=DEFAULT_DECISION_MODEL, metavar="MODEL",
                            help=f"Model for picking best response (default: {DEFAULT_DECISION_MODEL})")
    
    # ━━━ DATABASE OPERATIONS ━━━
    db_group = parser.add_argument_group("🔍 Database Operations")
    db_group.add_argument("--search", metavar="QUERY",
                         help="Search AI chat history in database")
    db_group.add_argument("--view", metavar="UUID",
                         help="View AI chat by UUID (supports 8-char partial UUIDs)")
    db_group.add_argument("--delete", metavar="UUID",
                         help="Delete AI chat by UUID (supports 8-char partial UUIDs)")
    db_group.add_argument("--content-only", action="store_true",
                         help="When viewing, show only content in clean markdown format")
    db_group.add_argument("--limit", type=int, default=10, metavar="N",
                         help="Limit number of search results (default: 10)")
    db_group.add_argument("--review-today", action="store_true",
                         help="Review all chats from today")
    db_group.add_argument("--review-yesterday", action="store_true",
                         help="Review all chats from yesterday")
    db_group.add_argument("--review-date", metavar="YYYY-MM-DD",
                         help="Review all chats from a specific date")
    
    # ━━━ HISTORY & CONTEXT ━━━
    context_group = parser.add_argument_group("🔗 History & Context")
    context_group.add_argument("--chat-id", action="append", dest="context_ids", metavar="UUID",
                              help="Include previous chat as context (supports 8-char partial UUIDs)")
    context_group.add_argument("--tags", metavar="TAGS",
                              help="Tag conversation with comma-separated tags")
    context_group.add_argument("--history", action="store_true",
                              help="Show conversation history leading to this chat")
    context_group.add_argument("--full-history", action="store_true",
                              help="Show complete conversation thread including forward refs")
    context_group.add_argument("--trace", action="store_true",
                              help="Show conversation flow as UUID/timestamp trace")
    context_group.add_argument("--use-context", action="store_true",
                              help="Include today's chat history")
    context_group.add_argument("--use-context-from", metavar="DATE",
                              help="Include chat history from date (YYYY-MM-DD)")
    
    # ━━━ CONTEXT MANAGEMENT ━━━  
    context_mgmt_group = parser.add_argument_group("🗂️  Context Management")
    context_mgmt_group.add_argument("--view-context", metavar="CONTEXT",
                                   help="View entire conversation context (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--by-chat-uuid", action="store_true",
                                   help="View context by chat UUID (use with --view-context)")
    context_mgmt_group.add_argument("--edit-context", metavar="CONTEXT", 
                                   help="Edit context properties (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--name", metavar="NAME",
                                   help="Set context name (use with --edit-context)")
    context_mgmt_group.add_argument("--summarize-context", metavar="CONTEXT",
                                   help="Generate summary for context (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--generate-context-name", metavar="CONTEXT",
                                   help="Generate name for context based on summary (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--generate-context-name-missing", action="store_true",
                                   help="Generate names for all contexts missing names (currently '<untitled>')")
    context_mgmt_group.add_argument("--summarize-contexts-missing", action="store_true",
                                   help="Summarize all contexts that don't have summaries")
    context_mgmt_group.add_argument("--summarize-contexts-outdated", action="store_true",
                                   help="Update summaries for contexts with newer chats")
    context_mgmt_group.add_argument("--search-contexts", metavar="QUERY",
                                   help="Search contexts by name, summary content, or tags")
    context_mgmt_group.add_argument("--context-id", metavar="CONTEXT",
                                   help="Add new chat to specific context (UUID, 8-char partial UUID, or name)")
    context_mgmt_group.add_argument("--continue", action="store_true",
                                   help="Continue conversation in the context of the most recent chat")
    context_mgmt_group.add_argument("--auto-name", action="store_true",
                                   help="Automatically generate name for new contexts using AI")
    
    # ━━━ SEARCH FILTERS ━━━
    filter_group = parser.add_argument_group("🔎 Search Filters")
    filter_group.add_argument("--provider-filter", metavar="PROVIDER",
                             help="Filter search results by provider")
    filter_group.add_argument("--model-filter", metavar="MODEL",
                             help="Filter search results by model")
    filter_group.add_argument("--tags-filter", metavar="TAGS",
                             help="Filter search results by tags (comma-separated)")
    filter_group.add_argument("--fzf-chat", action="store_true",
                             help="Search chats using fzf with preview")
    filter_group.add_argument("--fzf-context", action="store_true",
                             help="Search contexts using fzf with preview")
    
    # ━━━ OUTPUT & DEBUG ━━━
    output_group = parser.add_argument_group("📊 Output & Debug")
    output_group.add_argument("--json", action="store_true",
                             help="Return response in JSON format")
    output_group.add_argument("--summary", action="store_true",
                             help="Show usage summary after execution")
    output_group.add_argument("--show-id", action="store_true",
                             help="Display UUID of newly created chat")
    output_group.add_argument("--debug", action="store_true",
                             help="Enable detailed debug output")
    output_group.add_argument("--dry-run", action="store_true",
                             help="Show what would be sent without executing")
    output_group.add_argument("--no-color", action="store_true",
                             help="Disable colored output")
    
    # ━━━ ADVANCED OPTIONS ━━━
    advanced_group = parser.add_argument_group("⚙️  Advanced Options")
    advanced_group.add_argument("-P", "--preferences", "--about-me", dest="preferences", action="store_true",
                               help="Auto-include your personal preferences/system prompt from ${HOME}/Documents/notes/02_areas/personal/ai/ai_personal_preferences.md")
    advanced_group.add_argument("--personality", metavar="ROLE",
                               help="Set AI personality/role")
    advanced_group.add_argument("--emojis", nargs="?", const="light", choices=["light", "heavy"],
                               help="Encourage emoji usage in responses. 'light' (default) uses emojis sparingly, 'heavy' uses them frequently.")
    advanced_group.add_argument("--enhance-emojis", action="store_true",
                               help="Post-process the response to add appropriate emojis using a secondary AI call.")
    advanced_group.add_argument("--no-preserve", action="store_true",
                               help="Don't save chat transaction to database or files")
    advanced_group.add_argument("--do-sb-logging", action="store_true",
                               help="Save chat transaction to second-brain markdown file (disabled by default)")
    advanced_group.add_argument("-y", "--yes", action="store_true",
                               help="Skip confirmation prompts (for delete operations)")
    advanced_group.add_argument("--serve", action="store_true",
                               help="Start web interface for browsing contexts and chats")
    advanced_group.add_argument("--endpoint", metavar="URL",
                               help="Ollama API endpoint")
    advanced_group.add_argument("--profile-performance", action="store_true",
                               help="Profile Ollama model performance")
    advanced_group.add_argument("--profile-prompt", metavar="TEXT",
                               help="Custom prompt for performance profiling")
    advanced_group.add_argument("--profile-models", metavar="MODELS",
                               help="Specific models to profile")
    advanced_group.add_argument("--profile-show-output", action="store_true",
                               help="Show model outputs in performance table")

    # ━━━ CHUNKING OPTIONS ━━━
    chunking_group = parser.add_argument_group("🔗 Chunking Options (for large inputs)")
    chunking_group.add_argument("--enable-chunking", action="store_true",
                               help="Enable automatic chunking for prompts exceeding model token limits")
    chunking_group.add_argument("--max-chunk-size", type=int, metavar="TOKENS",
                               help="Maximum tokens per chunk (enables chunking automatically, default: 90%% of model limit)")
    chunking_group.add_argument("--summarize-context-chunks", action="store_true",
                               help="Pre-summarize context and include in all chunks (more coherent but higher cost)")

    # ━━━ AI HELPER ━━━
    helper_group = parser.add_argument_group("🤖 AI Helper")
    helper_group.add_argument("--helper", metavar="GOAL",
                             help="Use AI to suggest the correct aipy command for your goal")

    return parser.parse_args()

def run_provider_command(provider, args, model=None, iteration=None, stdin_input=None):
    """Run a command with the specified provider."""
    cmd = [provider]
    
    # Add model if specified and different from default
    if model and model != "grok-4":
        cmd.extend(["--model", model])
    
    # Add all the flags and options
    if args.debug:
        cmd.append("--debug")
    if args.json:
        cmd.append("--json")
    if args.no_streaming:
        cmd.append("--no-streaming")
    if args.no_color:
        cmd.append("--no-color")
    if args.summary:
        cmd.append("--summary")
    if args.dry_run:
        cmd.append("--dry-run")
    if args.embedding:
        cmd.append("--embedding")
    if args.image_gen:
        cmd.append("--image-gen")
    if args.regen:
        cmd.append("--regen")
    if args.hd:
        cmd.append("--hd")
    if args.use_context:
        cmd.append("--use-context")
    # Always pass --no-preserve to providers since aipy handles preservation
    cmd.append("--no-preserve")
    if args.list_models:
        cmd.append("--list-models")
    if args.profile_performance:
        cmd.append("--profile-performance")
    if args.profile_show_output:
        cmd.append("--profile-show-output")
    
    # Add options with values
    if args.files:
        for file in args.files:
            cmd.extend(["-f", file])
    if args.images:
        for image in args.images:
            cmd.extend(["--image", image])
    if args.output:
        # For multiple iterations, modify output filename
        if iteration is not None and args.times > 1:
            output_path = Path(args.output)
            stem = output_path.stem
            suffix = output_path.suffix
            new_output = f"{stem}-{iteration:03d}{suffix}"
            cmd.extend(["--output", new_output])
        else:
            cmd.extend(["--output", args.output])
    if args.size:
        cmd.extend(["--size", args.size])
    if args.use_context_from:
        cmd.extend(["--use-context-from", args.use_context_from])
    if args.personality:
        cmd.extend(["--personality", args.personality])
    if args.emojis:
        cmd.extend(["--emojis", args.emojis])
    if args.enhance_emojis:
        cmd.extend(["--enhance-emojis"])
    if args.endpoint:
        cmd.extend(["--endpoint", args.endpoint])
    if args.profile_prompt:
        cmd.extend(["--profile-prompt", args.profile_prompt])
    if args.profile_models:
        cmd.extend(["--profile-models", args.profile_models])
    
    # Add --prompt if provided via flag (not positional) and not being passed via stdin
    if args.prompt_flag and not stdin_input and not args.list_models:
        cmd.extend(["--prompt", args.prompt_flag])
    
    return cmd, stdin_input

def execute_command(cmd_and_stdin, model=None, iteration=None, capture_output=False, stream_output=True):
    """Execute a command and return the result."""
    try:
        if isinstance(cmd_and_stdin, tuple):
            cmd, stdin_input = cmd_and_stdin
        else:
            cmd, stdin_input = cmd_and_stdin, None
            
        if iteration is not None:
            print(f"[{model or 'default'} - Iteration {iteration + 1}]", file=sys.stderr)
        elif model:
            print(f"[{model}]", file=sys.stderr)
        
        # Use streaming when enabled
        if stream_output:
            process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE if stdin_input else None,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            stdout_output = []
            stderr_output = []
            
            # Send stdin input if provided
            if stdin_input:
                process.stdin.write(stdin_input)
                process.stdin.close()
            
            # Stream output in real-time using threads
            import threading
            
            def read_stream(stream, output_list, print_func):
                """Read from a stream and both collect and print output."""
                try:
                    for line in iter(stream.readline, ''):
                        if line:
                            print_func(line, end='')
                            output_list.append(line)
                        else:
                            break
                except:
                    pass
                finally:
                    stream.close()
            
            # Start threads to read stdout and stderr
            stdout_thread = threading.Thread(
                target=read_stream, 
                args=(process.stdout, stdout_output, print)
            )
            stderr_thread = threading.Thread(
                target=read_stream, 
                args=(process.stderr, stderr_output, lambda x, **kwargs: print(x, file=sys.stderr, **kwargs))
            )
            
            stdout_thread.start()
            stderr_thread.start()
            
            # Wait for process to complete
            returncode = process.wait()
            
            # Wait for all output to be read
            stdout_thread.join()
            stderr_thread.join()
            
            response = {
                "success": returncode == 0,
                "returncode": returncode,
                "model": model,
                "iteration": iteration,
                "cmd": " ".join(cmd) if isinstance(cmd, list) else str(cmd)
            }
            
            if capture_output:
                response["output"] = ''.join(stdout_output)
                if stderr_output:
                    response["stderr"] = ''.join(stderr_output)
                    
            return response
        
        # Fallback to original non-streaming behavior
        else:
            # If we have stdin input, pass it to the command
            if stdin_input:
                result = subprocess.run(cmd, input=stdin_input, text=True, capture_output=capture_output)
            else:
                result = subprocess.run(cmd, capture_output=capture_output, text=True)
                
            response = {
                "success": result.returncode == 0,
                "returncode": result.returncode,
                "model": model,
                "iteration": iteration,
                "cmd": " ".join(cmd) if isinstance(cmd, list) else str(cmd)
            }
            
            if capture_output:
                response["output"] = result.stdout
                if result.stderr:
                    response["stderr"] = result.stderr
                    
            return response
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "model": model,
            "iteration": iteration,
            "cmd": " ".join(cmd) if isinstance(cmd, (list, tuple)) else str(cmd)
        }

def list_all_models():
    """List models from all providers."""
    providers = ["claudpy", "grokpy", "geminpy", "openpy", "ollampy", "perpy"]
    
    print("🤖 All Available AI Models\n")
    print("=" * 80)
    
    for provider in providers:
        print(f"\n📡 {provider.upper()}")
        print("-" * 40)
        
        try:
            # Run the list-models command for each provider
            result = subprocess.run([provider, "--list-models"], 
                                    capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                # Clean up the output to remove extra formatting
                output = result.stdout.strip()
                if output:
                    print(output)
                else:
                    print("  No models returned")
            else:
                print(f"  Error: {result.stderr.strip()}")
        except subprocess.TimeoutExpired:
            print("  Timeout - provider not responding")
        except FileNotFoundError:
            print(f"  Provider '{provider}' not found in PATH")
        except Exception as e:
            print(f"  Error: {e}")
    
    print("\n" + "=" * 80)
    print("💡 Usage: aipy --model <model-name> \"your prompt\"")
    print("💡 Multiple models: aipy --model model1,model2 \"your prompt\"")
    print("💡 Model routing is automatic based on the model name")

def parse_file_references(text, debug=False):
    """Parse @<file>, @image:<file>, @note, @fimage, @ffile, and @bash:"<cmd>" references in text."""
    
    files = []
    images = []
    
    # Handle @note command - use fzf to select from second brain
    if '@note' in text:
        try:
            notes_dir = os.path.expanduser("~/Documents/notes")
            if os.path.exists(notes_dir):
                # Use find with same exclusions as fopen_note
                cmd = [
                    'find', notes_dir,
                    '-maxdepth', '6',
                    '(', '-name', '.git', '-o', '-name', '*.calendar', '-o', 
                    '-name', '*.books', '-o', '-name', '*.claude', '-o', '-name', 'trees', ')', '-prune',
                    '-o', '-type', 'f', '-name', '*.md', '-print'
                ]
                
                find_result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)
                if find_result.returncode == 0 and find_result.stdout.strip():
                    # Use fzf to select
                    fzf_cmd = ['fzf', '--preview', 'head -100 {}', '--prompt', 'Select note: ']
                    fzf_result = subprocess.run(fzf_cmd, input=find_result.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                    
                    if fzf_result.returncode == 0 and fzf_result.stdout.strip():
                        selected_note = fzf_result.stdout.strip()
                        files.append(selected_note)
                        if debug:
                            print(f"📝 Selected note: {selected_note}", file=sys.stderr)
                    else:
                        if debug:
                            print("📝 No note selected", file=sys.stderr)
                else:
                    print("⚠️  No markdown files found in notes directory", file=sys.stderr)
            else:
                print("⚠️  Notes directory not found", file=sys.stderr)
        except Exception as e:
            print(f"⚠️  Error with @note command: {e}", file=sys.stderr)
    
    # Handle @fimage command - use fzf to select images from PWD
    if '@fimage' in text:
        try:
            # Find image files in current directory and subdirectories
            cmd = [
                'find', '.', '-type', 'f', 
                '(', '-iname', '*.jpg', '-o', '-iname', '*.jpeg', '-o', '-iname', '*.png', 
                '-o', '-iname', '*.gif', '-o', '-iname', '*.bmp', '-o', '-iname', '*.tiff', 
                '-o', '-iname', '*.webp', '-o', '-iname', '*.svg', ')'
            ]
            
            find_result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)
            if find_result.returncode == 0 and find_result.stdout.strip():
                # Use fzf to select
                fzf_cmd = ['fzf', '--prompt', 'Select image: ']
                fzf_result = subprocess.run(fzf_cmd, input=find_result.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                
                if fzf_result.returncode == 0 and fzf_result.stdout.strip():
                    selected_image = fzf_result.stdout.strip()
                    # Convert to absolute path
                    if not os.path.isabs(selected_image):
                        selected_image = os.path.abspath(selected_image)
                    images.append(selected_image)
                    if debug:
                        print(f"📷 Selected image: {selected_image}", file=sys.stderr)
                else:
                    if debug:
                        print("📷 No image selected", file=sys.stderr)
            else:
                print("⚠️  No image files found in current directory", file=sys.stderr)
        except Exception as e:
            print(f"⚠️  Error with @fimage command: {e}", file=sys.stderr)
    
    # Handle @ffile command - use fzf to select any file from PWD
    if '@ffile' in text:
        try:
            # Find all files in current directory and subdirectories
            cmd = ['find', '.', '-type', 'f']
            
            find_result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)
            if find_result.returncode == 0 and find_result.stdout.strip():
                # Use fzf to select
                fzf_cmd = ['fzf', '--preview', 'head -100 {}', '--prompt', 'Select file: ']
                fzf_result = subprocess.run(fzf_cmd, input=find_result.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                
                if fzf_result.returncode == 0 and fzf_result.stdout.strip():
                    selected_file = fzf_result.stdout.strip()
                    # Convert to absolute path
                    if not os.path.isabs(selected_file):
                        selected_file = os.path.abspath(selected_file)
                    files.append(selected_file)
                    if debug:
                        print(f"📄 Selected file: {selected_file}", file=sys.stderr)
                else:
                    if debug:
                        print("📄 No file selected", file=sys.stderr)
            else:
                print("⚠️  No files found in current directory", file=sys.stderr)
        except Exception as e:
            print(f"⚠️  Error with @ffile command: {e}", file=sys.stderr)
    
    # Handle @bash:"<command>" references (process before regular files)
    bash_pattern = r'@bash:"([^"]+)"'
    bash_matches = re.findall(bash_pattern, text)
    
    for command in bash_matches:
        if debug:
            print(f"🖥️  Executing bash command: {command}", file=sys.stderr)
        
        try:
            # Run bash command in interactive shell to source .bashrc
            # Use bash -i to make it interactive (loads .bashrc)
            bash_cmd = ['bash', '-i', '-c', command]
            result = subprocess.run(
                bash_cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE, 
                text=True,
                timeout=30  # 30 second timeout for safety
            )
            
            # Clean up command for display (remove dbhi prefix if present)
            display_command = command
            if command.startswith('dbhi '):
                display_command = command[5:]  # Remove 'dbhi ' prefix
            
            # Format the output to include in context
            command_output = f"""

---

## Bash Command Output

**Command executed:**
```bash
{display_command}
```

**Exit Code:** {result.returncode}

**Command Output:**
```
{result.stdout}
```

**Errors (if any):**
```
{result.stderr}
```

---

"""
            
            # Replace the @bash:"command" with the formatted output
            bash_ref = f'@bash:"{command}"'
            text = text.replace(bash_ref, command_output)
            
            if debug:
                print(f"🖥️  Command completed with exit code: {result.returncode}", file=sys.stderr)
                if result.stdout:
                    print(f"🖥️  Output: {result.stdout[:100]}...", file=sys.stderr)
                if result.stderr:
                    print(f"🖥️  Errors: {result.stderr[:100]}...", file=sys.stderr)
        
        except subprocess.TimeoutExpired:
            # Clean up command for display (remove dbhi prefix if present)
            display_command = command
            if command.startswith('dbhi '):
                display_command = command[5:]  # Remove 'dbhi ' prefix
                
            error_output = f"""

---

## Bash Command Output

**Command executed:**
```bash
{display_command}
```

**Status:** ⚠️ Command timed out after 30 seconds

---

"""
            bash_ref = f'@bash:"{command}"'
            text = text.replace(bash_ref, error_output)
            print(f"⚠️  Bash command timed out: {command}", file=sys.stderr)
        
        except Exception as e:
            # Clean up command for display (remove dbhi prefix if present)
            display_command = command
            if command.startswith('dbhi '):
                display_command = command[5:]  # Remove 'dbhi ' prefix
                
            error_output = f"""

---

## Bash Command Output

**Command executed:**
```bash
{display_command}
```

**Status:** ❌ Error: {str(e)}

---

"""
            bash_ref = f'@bash:"{command}"'
            text = text.replace(bash_ref, error_output)
            print(f"⚠️  Error executing bash command: {command} - {e}", file=sys.stderr)
    
    # Find @image:<file> references
    image_pattern = r'@image:([^\s]+)'
    image_matches = re.findall(image_pattern, text)
    
    for match in image_matches:
        file_path = match.strip()
        # Handle relative paths
        if not os.path.isabs(file_path):
            file_path = os.path.abspath(file_path)
        
        if os.path.exists(file_path):
            images.append(file_path)
            if debug:
                print(f"📷 Found image: {file_path}", file=sys.stderr)
        else:
            print(f"⚠️  Image not found: {file_path}", file=sys.stderr)
    
    # Find @<file> references (excluding @image:, @note, @fimage, @ffile, @bash:)
    file_pattern = r'@(?!image:|note|fimage|ffile|bash:")([^\s]+)'
    file_matches = re.findall(file_pattern, text)
    
    for match in file_matches:
        file_path = match.strip()
        # Handle relative paths
        if not os.path.isabs(file_path):
            file_path = os.path.abspath(file_path)
        
        if os.path.exists(file_path):
            files.append(file_path)
            if debug:
                print(f"📄 Found file: {file_path}", file=sys.stderr)
        else:
            print(f"⚠️  File not found: {file_path}", file=sys.stderr)
    
    # Remove file references from text (bash commands already processed and replaced)
    clean_text = re.sub(r'@image:[^\s]+', '', text)
    clean_text = re.sub(r'@note\b', '', clean_text)
    clean_text = re.sub(r'@fimage\b', '', clean_text)
    clean_text = re.sub(r'@ffile\b', '', clean_text)
    # Don't remove @bash:"..." since we've already processed and replaced them
    clean_text = re.sub(r'@(?!image:|note|fimage|ffile|bash:")[^\s]+', '', clean_text)
    clean_text = ' '.join(clean_text.split())  # Clean up extra whitespace
    
    return clean_text, files, images

def display_context_info(context_uuid=None, chat_id=None, debug=False):
    """Display context information for REPL."""
    if not DB_AVAILABLE or not check_db_available(debug):
        return
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        if chat_id:
            # Get context from specific chat
            if len(chat_id) == 8:
                cursor.execute("""
                    SELECT a.context_uuid, c.name as context_name
                    FROM ai_chats a
                    JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
                    WHERE CAST(a.id as TEXT) LIKE %s
                """, [f"{chat_id}%"])
            else:
                cursor.execute("""
                    SELECT a.context_uuid, c.name as context_name  
                    FROM ai_chats a
                    JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
                    WHERE a.id = %s
                """, [chat_id])
            
            result = cursor.fetchone()
            if result:
                context_uuid = result['context_uuid']
                context_name = result['context_name']
            else:
                print(f"⚠️  Chat {chat_id} not found", file=sys.stderr)
                cursor.close()
                conn.close()
                return
        elif context_uuid:
            # Get context info directly
            if len(context_uuid) == 8:
                cursor.execute("""
                    SELECT context_uuid, name as context_name
                    FROM ai_chat_contexts 
                    WHERE CAST(context_uuid as TEXT) LIKE %s
                """, [f"{context_uuid}%"])
            else:
                cursor.execute("""
                    SELECT context_uuid, name as context_name
                    FROM ai_chat_contexts 
                    WHERE context_uuid = %s
                """, [context_uuid])
            
            result = cursor.fetchone()
            if result:
                context_uuid = result['context_uuid']
                context_name = result['context_name']
            else:
                print(f"⚠️  Context {context_uuid} not found", file=sys.stderr)
                cursor.close()
                conn.close()
                return
        
        # Get recent chats from this context
        cursor.execute("""
            SELECT 
                id,
                LEFT(prompt, 100) as prompt_snippet,
                LEFT(response, 100) as response_snippet,
                model,
                request_timestamp
            FROM ai_chats 
            WHERE context_uuid = %s
            ORDER BY request_timestamp DESC
            LIMIT 5
        """, [context_uuid])
        
        chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        print(f"\n🧠 Context: {context_name}")
        print(f"📝 Context ID: {str(context_uuid)}")
        print(f"💭 Recent conversations ({len(chats)} shown):")
        print("─" * 60)
        
        for i, chat in enumerate(reversed(chats), 1):  # Show oldest first
            timestamp = chat['request_timestamp'].strftime('%H:%M:%S')
            print(f"{i}. [{timestamp}] {chat['model']}")
            print(f"   👤 {chat['prompt_snippet']}{'...' if len(chat['prompt_snippet']) >= 100 else ''}")
            print(f"   🤖 {chat['response_snippet']}{'...' if len(chat['response_snippet']) >= 100 else ''}")
            print()
        
        print("─" * 60)
        
    except psycopg2.Error as e:
        if debug:
            print(f"Error displaying context: {e}", file=sys.stderr)

def display_full_conversation_history(context_uuid=None, chat_id=None, debug=False, no_summary=False):
    """Display the full conversation history for a context in REPL with markdown rendering."""
    if not DB_AVAILABLE or not check_db_available(debug):
        print("⚠️  Database not available", file=sys.stderr)
        return
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # If chat_id provided, resolve to context_uuid
        if chat_id and not context_uuid:
            if len(chat_id) == 8:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats 
                    WHERE CAST(id as TEXT) LIKE %s
                """, [f"{chat_id}%"])
            else:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats 
                    WHERE id = %s
                """, [chat_id])
            
            result = cursor.fetchone()
            if result:
                context_uuid = str(result['context_uuid'])
            else:
                print(f"⚠️  Chat {chat_id[:8]}... not found")
                cursor.close()
                conn.close()
                return
        
        if not context_uuid:
            print("⚠️  No context_uuid or chat_id provided")
            cursor.close()
            conn.close()
            return
        
        # Get context info including summary
        cursor.execute("""
            SELECT name, summary FROM ai_chat_contexts WHERE context_uuid = %s
        """, [context_uuid])
        
        context_result = cursor.fetchone()
        if not context_result:
            print(f"⚠️  Context {context_uuid[:8]}... not found")
            cursor.close()
            conn.close()
            return
        
        context_name = context_result['name'] or '<untitled>'
        context_summary = context_result['summary']
        
        # Get all chats from this context
        cursor.execute("""
            SELECT 
                id,
                prompt,
                response,
                provider,
                model,
                request_timestamp,
                duration_ms,
                cost_total_usd,
                tags
            FROM ai_chats 
            WHERE context_uuid = %s
            ORDER BY request_timestamp ASC
        """, [context_uuid])
        
        chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not chats:
            print(f"📝 Context: {context_name}")
            print(f"🆔 Context ID: {context_uuid}")
            print("💭 No conversations found in this context")
            return
        
        # Generate markdown content for the full conversation
        markdown_parts = [f"# {context_name}", ""]
        markdown_parts.append(f"**Context ID:** {context_uuid}")
        markdown_parts.append(f"**Total Chats:** {len(chats)}")
        markdown_parts.append("")
        
        # Add context summary if available and not disabled
        if context_summary and not no_summary:
            markdown_parts.append("## Summary")
            markdown_parts.append("")
            markdown_parts.append(context_summary)
            markdown_parts.append("")
        
        markdown_parts.append("---")
        markdown_parts.append("")
        
        for i, chat in enumerate(chats, 1):
            chat_id = str(chat['id'])[:8]
            timestamp = chat['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            
            markdown_parts.append(f"## {i}. Chat {chat_id} - {chat['provider']}/{chat['model']}")
            markdown_parts.append("")
            markdown_parts.append(f"**Time:** {timestamp}")
            if chat['duration_ms']:
                markdown_parts.append(f"**Duration:** {chat['duration_ms']}ms")
            if chat['cost_total_usd']:
                markdown_parts.append(f"**Cost:** ${chat['cost_total_usd']:.4f}")
            if chat['tags']:
                markdown_parts.append(f"**Tags:** {', '.join(chat['tags'])}")
            markdown_parts.append("")
            
            markdown_parts.append("### Question")
            markdown_parts.append("")
            markdown_parts.append(chat['prompt'])
            markdown_parts.append("")
            
            markdown_parts.append("### Response")
            markdown_parts.append("")
            markdown_parts.append(chat['response'])
            markdown_parts.append("")
            
            if i < len(chats):  # Don't add separator after last chat
                markdown_parts.append("---")
                markdown_parts.append("")
        
        # Render the markdown
        markdown_content = "\n".join(markdown_parts)
        rendered_output = render_markdown(markdown_content)
        if rendered_output:  # Only print if markdown rendering failed and returned fallback text
            print(rendered_output)
            
    except psycopg2.Error as e:
        print(f"⚠️  Error displaying conversation history: {e}", file=sys.stderr)
        if debug:
            print(f"Debug: {e}", file=sys.stderr)

def repl_search_chats(query, limit=5, debug=False):
    """Search chats in REPL mode with compact output."""
    if not DB_AVAILABLE or not check_db_available(debug):
        print("⚠️  Database not available for search", file=sys.stderr)
        return []
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Simple search query
        sql = """
        SELECT 
            a.id,
            LEFT(a.prompt, 80) as prompt_snippet,
            LEFT(a.response, 80) as response_snippet,
            a.model,
            a.request_timestamp,
            a.context_uuid,
            c.name as context_name
        FROM ai_chats a
        JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
        WHERE (
            to_tsvector('english', a.prompt) @@ plainto_tsquery('english', %s) OR 
            to_tsvector('english', a.response) @@ plainto_tsquery('english', %s)
        )
        ORDER BY a.request_timestamp DESC
        LIMIT %s
        """
        
        cursor.execute(sql, [query, query, limit])
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        if not results:
            print(f"🔍 No chats found for: '{query}'")
            return []
        
        print(f"\n🔍 Found {len(results)} chats for: '{query}'")
        print("─" * 60)
        
        for i, r in enumerate(results, 1):
            chat_id = str(r['id'])[:8]
            timestamp = r['request_timestamp'].strftime('%m-%d %H:%M')
            context_name = (r['context_name'] or 'Unnamed')[:20]
            
            print(f"{i}. [{chat_id}] {timestamp} | {r['model']}")
            print(f"   Context: {context_name}")
            print(f"   👤 {r['prompt_snippet']}{'...' if len(r['prompt_snippet']) >= 80 else ''}")
            print(f"   🤖 {r['response_snippet']}{'...' if len(r['response_snippet']) >= 80 else ''}")
            print()
        
        return [str(r['id']) for r in results]
        
    except psycopg2.Error as e:
        print(f"⚠️  Error searching chats: {e}", file=sys.stderr)
        return []

def repl_search_contexts(query, limit=5, debug=False):
    """Search contexts in REPL mode with compact output."""
    if not DB_AVAILABLE or not check_db_available(debug):
        print("⚠️  Database not available for search", file=sys.stderr)
        return []
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Search contexts
        sql = """
        SELECT 
            context_uuid,
            name,
            LEFT(COALESCE(summary, ''), 100) as summary_snippet,
            tags,
            created_at,
            (SELECT COUNT(*) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as chat_count,
            (SELECT MAX(request_timestamp) FROM ai_chats WHERE context_uuid = ctx.context_uuid) as last_chat_time
        FROM ai_chat_contexts ctx
        WHERE (
            LOWER(name) LIKE LOWER(%s) OR
            to_tsvector('english', COALESCE(summary, '')) @@ plainto_tsquery('english', %s) OR
            %s = ANY(LOWER(tags::text)::text[])
        )
        ORDER BY last_chat_time DESC NULLS LAST
        LIMIT %s
        """
        
        search_term = f"%{query}%"
        cursor.execute(sql, [search_term, query, query.lower(), limit])
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        if not results:
            print(f"🔍 No contexts found for: '{query}'")
            return []
        
        print(f"\n🔍 Found {len(results)} contexts for: '{query}'")
        print("─" * 60)
        
        for i, r in enumerate(results, 1):
            context_id = str(r['context_uuid'])[:8]
            name = (r['name'] or 'Unnamed')[:30]
            chat_count = r['chat_count']
            last_chat = r['last_chat_time'].strftime('%m-%d %H:%M') if r['last_chat_time'] else 'Never'
            
            print(f"{i}. [{context_id}] {name}")
            print(f"   📊 {chat_count} chats | Last: {last_chat}")
            if r['summary_snippet']:
                print(f"   📝 {r['summary_snippet']}{'...' if len(r['summary_snippet']) >= 100 else ''}")
            if r['tags']:
                print(f"   🏷️  {', '.join(r['tags'])}")
            print()
        
        return [str(r['context_uuid']) for r in results]
        
    except psycopg2.Error as e:
        print(f"⚠️  Error searching contexts: {e}", file=sys.stderr)
        return []

def repl_load_chat(chat_id, debug=False):
    """Load a chat's context into the current REPL session."""
    if not DB_AVAILABLE or not check_db_available(debug):
        print("⚠️  Database not available", file=sys.stderr)
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Handle partial UUID
        if len(chat_id) == 8:
            cursor.execute("""
                SELECT a.context_uuid, c.name as context_name
                FROM ai_chats a
                JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
                WHERE CAST(a.id as TEXT) LIKE %s
            """, [f"{chat_id}%"])
        else:
            cursor.execute("""
                SELECT a.context_uuid, c.name as context_name
                FROM ai_chats a
                JOIN ai_chat_contexts c ON a.context_uuid = c.context_uuid
                WHERE a.id = %s
            """, [chat_id])
        
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if result:
            context_uuid = str(result['context_uuid'])
            context_name = result['context_name']
            print(f"✅ Loaded chat context: {context_name}")
            print(f"📝 Context ID: {context_uuid}")
            return context_uuid
        else:
            print(f"⚠️  Chat {chat_id} not found")
            return None
            
    except psycopg2.Error as e:
        print(f"⚠️  Error loading chat: {e}", file=sys.stderr)
        return None

def repl_load_context(context_id, debug=False):
    """Load a context into the current REPL session."""
    if not DB_AVAILABLE or not check_db_available(debug):
        print("⚠️  Database not available", file=sys.stderr)
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Handle partial UUID
        if len(context_id) == 8:
            cursor.execute("""
                SELECT context_uuid, name
                FROM ai_chat_contexts
                WHERE CAST(context_uuid as TEXT) LIKE %s
            """, [f"{context_id}%"])
        else:
            cursor.execute("""
                SELECT context_uuid, name
                FROM ai_chat_contexts
                WHERE context_uuid = %s
            """, [context_id])
        
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if result:
            context_uuid = str(result['context_uuid'])
            context_name = result['name']
            print(f"✅ Loaded context: {context_name}")
            print(f"📝 Context ID: {context_uuid}")
            return context_uuid
        else:
            print(f"⚠️  Context {context_id} not found")
            return None
            
    except psycopg2.Error as e:
        print(f"⚠️  Error loading context: {e}", file=sys.stderr)
        return None

def get_conversation_history(id_value, is_chat_id=False, debug=False):
    """Get conversation history for a chat ID or context ID."""
    if not DB_AVAILABLE or not check_db_available(debug):
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        if is_chat_id:
            # Get context from chat ID, then get conversation history
            if len(id_value) == 8:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats 
                    WHERE CAST(id as TEXT) LIKE %s
                """, [f"{id_value}%"])
            else:
                cursor.execute("""
                    SELECT context_uuid FROM ai_chats 
                    WHERE id = %s
                """, [id_value])
            
            result = cursor.fetchone()
            if not result:
                cursor.close()
                conn.close()
                return None
            
            context_uuid = result['context_uuid']
        else:
            # Use context ID directly
            context_uuid = id_value
        
        # Get conversation history from context
        cursor.execute("""
            SELECT prompt, response, request_timestamp
            FROM ai_chats 
            WHERE context_uuid = %s
            ORDER BY request_timestamp ASC
        """, [context_uuid])
        
        chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not chats:
            return None
        
        # Build conversation history string
        history_parts = ["=== CONTEXT FROM PREVIOUS CONVERSATIONS ==="]
        
        for chat in chats:
            timestamp = chat['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            history_parts.append(f"\n[{timestamp}]")
            history_parts.append(f"Human: {chat['prompt']}")
            history_parts.append(f"Assistant: {chat['response']}")
        
        history_parts.append("\n=== END CONTEXT ===")
        
        return "\n".join(history_parts)
        
    except psycopg2.Error as e:
        if debug:
            print(f"Error getting conversation history: {e}", file=sys.stderr)
        return None

def render_markdown(text, width=None):
    """Render markdown text to terminal with rich formatting."""
    if not RICH_AVAILABLE:
        # Fallback to plain text if rich is not available
        return text
    
    try:
        console = Console(width=width, file=sys.stdout)
        markdown = Markdown(text)
        console.print(markdown)
        return ""  # Return empty string since console.print already outputs
    except Exception:
        # Fallback to plain text if rendering fails
        return text

def repl_set_context_name(context_id, name, debug=False):
    """Set the name for the current context in REPL mode."""
    if not DB_AVAILABLE or not check_db_available(debug):
        if debug:
            print("⚠️  Database not available", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Update the context name
        cursor.execute("""
            UPDATE ai_chat_contexts 
            SET name = %s, updated_at = CURRENT_TIMESTAMP
            WHERE context_uuid = %s
            RETURNING context_uuid, name
        """, [name, context_id])
        
        result = cursor.fetchone()
        conn.commit()
        cursor.close()
        conn.close()
        
        if result:
            if debug:
                print(f"Context {context_id} name updated to: {name}")
            return True
        else:
            if debug:
                print(f"Context {context_id} not found")
            return False
            
    except psycopg2.Error as e:
        if debug:
            print(f"⚠️  Error setting context name: {e}", file=sys.stderr)
        return False

def repl_get_context_name(context_id, debug=False):
    """Get the name of the current context in REPL mode."""
    if not DB_AVAILABLE or not check_db_available(debug):
        if debug:
            print("⚠️  Database not available", file=sys.stderr)
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get the context name
        cursor.execute("""
            SELECT name FROM ai_chat_contexts 
            WHERE context_uuid = %s
        """, [context_id])
        
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if result:
            name = result['name']
            # Don't show '<untitled>' as the name - treat it as no name
            if name == '<untitled>':
                return "Untitled Context"
            return name
        else:
            if debug:
                print(f"Context {context_id} not found")
            return None
            
    except psycopg2.Error as e:
        if debug:
            print(f"⚠️  Error getting context name: {e}", file=sys.stderr)
        return None

def repl_ingest_to_second_brain(context_id, filename, debug=False):
    """Save context to second brain using fzf directory selection."""
    notes_dir = os.environ.get('NOTES_DIR', os.path.expanduser('~/Documents/notes'))
    
    # Validate filename
    if not filename.endswith('.md'):
        print("⚠️  Filename must end with .md")
        return False
    
    # Validate filename doesn't contain path separators
    if '/' in filename or '\\' in filename:
        print("⚠️  Filename should not contain path separators")
        return False
    
    # Get context content as markdown
    if not DB_AVAILABLE or not check_db_available(debug):
        print("⚠️  Database not available")
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get context info
        cursor.execute("""
            SELECT name, summary FROM ai_chat_contexts WHERE context_uuid = %s
        """, [context_id])
        
        context_result = cursor.fetchone()
        if not context_result:
            print(f"⚠️  Context {context_id[:8]}... not found")
            cursor.close()
            conn.close()
            return False
        
        context_name = context_result['name'] or '<untitled>'
        context_summary = context_result['summary']
        
        # Get all chats from this context
        cursor.execute("""
            SELECT 
                id,
                prompt,
                response,
                provider,
                model,
                request_timestamp,
                duration_ms,
                cost_total_usd,
                tags
            FROM ai_chats 
            WHERE context_uuid = %s
            ORDER BY request_timestamp ASC
        """, [context_id])
        
        chats = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not chats:
            print("💭 No conversations found in this context")
            return False
        
        # Generate markdown content
        markdown_parts = [f"# {context_name}", ""]
        markdown_parts.append(f"**Context ID:** {context_id}")
        markdown_parts.append(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        markdown_parts.append(f"**Total Chats:** {len(chats)}")
        markdown_parts.append("")
        
        if context_summary:
            markdown_parts.append("## Summary")
            markdown_parts.append("")
            markdown_parts.append(context_summary)
            markdown_parts.append("")
        
        markdown_parts.append("---")
        markdown_parts.append("")
        
        for i, chat in enumerate(chats, 1):
            chat_id = str(chat['id'])[:8]
            timestamp = chat['request_timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            
            markdown_parts.append(f"## {i}. Chat {chat_id} - {chat['provider']}/{chat['model']}")
            markdown_parts.append("")
            markdown_parts.append(f"**Time:** {timestamp}")
            if chat['duration_ms']:
                markdown_parts.append(f"**Duration:** {chat['duration_ms']}ms")
            if chat['cost_total_usd']:
                markdown_parts.append(f"**Cost:** ${chat['cost_total_usd']:.4f}")
            if chat['tags']:
                markdown_parts.append(f"**Tags:** {', '.join(chat['tags'])}")
            markdown_parts.append("")
            
            markdown_parts.append("### Question")
            markdown_parts.append("")
            markdown_parts.append(chat['prompt'])
            markdown_parts.append("")
            
            markdown_parts.append("### Response")
            markdown_parts.append("")
            markdown_parts.append(chat['response'])
            markdown_parts.append("")
            
            if i < len(chats):  # Don't add separator after last chat
                markdown_parts.append("---")
                markdown_parts.append("")
        
        markdown_content = "\n".join(markdown_parts)
        
        # Find all directories in second brain
        para_dirs = ['00_inbox', '01_projects', '02_areas', '03_resources', '04_archives']
        all_dirs = []
        
        for para_dir in para_dirs:
            para_path = Path(notes_dir) / para_dir
            if para_path.exists():
                # Add the PARA directory itself
                all_dirs.append(para_dir)
                # Add all subdirectories
                for subdir in para_path.rglob('*'):
                    if subdir.is_dir():
                        relative_dir = str(subdir.relative_to(notes_dir))
                        all_dirs.append(relative_dir)
        
        if not all_dirs:
            print("⚠️  No directories found in second brain")
            return False
        
        # Sort directories for better display
        all_dirs.sort()
        
        # Use fzf to select directory
        try:
            proc = subprocess.Popen(
                ['fzf', '--prompt=Select directory: ', '--height=50%', '--reverse'],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                text=True
            )
            stdout, _ = proc.communicate('\n'.join(all_dirs))
            
            if proc.returncode != 0 or not stdout.strip():
                print("❌ Directory selection cancelled")
                return False
            
            selected_dir = stdout.strip()
            
            # Create full file path
            file_path = Path(notes_dir) / selected_dir / filename
            
            # Check if file already exists
            if file_path.exists():
                print(f"⚠️  File already exists: {file_path}")
                response = input("Overwrite? (y/N): ").strip().lower()
                if response != 'y':
                    return False
            
            # Write the file
            file_path.parent.mkdir(parents=True, exist_ok=True)
            file_path.write_text(markdown_content, encoding='utf-8')
            
            print(f"✅ Saved to: {file_path}")
            print(f"📁 Directory: {selected_dir}")
            print(f"📄 Filename: {filename}")
            
            return True
            
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("⚠️  fzf not found. Install fzf to use directory selection")
            return False
        
    except Exception as e:
        print(f"⚠️  Error saving to second brain: {e}")
        if debug:
            print(f"Debug: {e}")
        return False

def repl_add_tags(context_id, tags, debug=False):
    """Add tags to the current context in REPL mode."""
    if not DB_AVAILABLE or not check_db_available(debug):
        if debug:
            print("⚠️  Database not available", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get current tags
        cursor.execute("""
            SELECT tags FROM ai_chat_contexts 
            WHERE context_uuid = %s
        """, [context_id])
        
        result = cursor.fetchone()
        if not result:
            if debug:
                print(f"Context {context_id} not found")
            cursor.close()
            conn.close()
            return False
        
        current_tags = result['tags'] or []
        
        # Add new tags (avoiding duplicates)
        new_tags = [tag.strip() for tag in tags if tag.strip() and tag.strip() not in current_tags]
        if new_tags:
            updated_tags = current_tags + new_tags
            
            # Update context tags
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET tags = %s, updated_at = CURRENT_TIMESTAMP
                WHERE context_uuid = %s
                RETURNING tags
            """, [updated_tags, context_id])
            
            result = cursor.fetchone()
            conn.commit()
            
            if debug:
                print(f"Added tags: {new_tags}")
                print(f"Updated tags: {result['tags']}")
        
        cursor.close()
        conn.close()
        return True
        
    except psycopg2.Error as e:
        if debug:
            print(f"⚠️  Error adding tags: {e}", file=sys.stderr)
        return False

def repl_list_tags(context_id, debug=False):
    """List tags for the current context in REPL mode."""
    if not DB_AVAILABLE or not check_db_available(debug):
        if debug:
            print("⚠️  Database not available", file=sys.stderr)
        return None
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get context tags
        cursor.execute("""
            SELECT tags FROM ai_chat_contexts 
            WHERE context_uuid = %s
        """, [context_id])
        
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if result:
            return result['tags'] or []
        else:
            if debug:
                print(f"Context {context_id} not found")
            return None
            
    except psycopg2.Error as e:
        if debug:
            print(f"⚠️  Error listing tags: {e}", file=sys.stderr)
        return None

def repl_remove_tags(context_id, tags_to_remove, debug=False):
    """Remove tags from the current context in REPL mode."""
    if not DB_AVAILABLE or not check_db_available(debug):
        if debug:
            print("⚠️  Database not available", file=sys.stderr)
        return False
    
    try:
        conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Get current tags
        cursor.execute("""
            SELECT tags FROM ai_chat_contexts 
            WHERE context_uuid = %s
        """, [context_id])
        
        result = cursor.fetchone()
        if not result:
            if debug:
                print(f"Context {context_id} not found")
            cursor.close()
            conn.close()
            return False
        
        current_tags = result['tags'] or []
        
        # Remove specified tags
        tags_to_remove_set = {tag.strip() for tag in tags_to_remove if tag.strip()}
        updated_tags = [tag for tag in current_tags if tag not in tags_to_remove_set]
        removed_tags = [tag for tag in current_tags if tag in tags_to_remove_set]
        
        if removed_tags:
            # Update context tags
            cursor.execute("""
                UPDATE ai_chat_contexts 
                SET tags = %s, updated_at = CURRENT_TIMESTAMP
                WHERE context_uuid = %s
                RETURNING tags
            """, [updated_tags, context_id])
            
            result = cursor.fetchone()
            conn.commit()
            
            if debug:
                print(f"Removed tags: {removed_tags}")
                print(f"Updated tags: {result['tags']}")
        
        cursor.close()
        conn.close()
        return True, removed_tags
        
    except psycopg2.Error as e:
        if debug:
            print(f"⚠️  Error removing tags: {e}", file=sys.stderr)
        return False, []

def start_repl(args):
    """Start interactive REPL mode."""
    print("🚀 Starting AIPy REPL Mode")
    print("─" * 50)
    print(f"🤖 Model: {args.model}")
    
    # Handle context continuation
    context_id = None
    if getattr(args, 'continue', False):
        # Get most recent context
        if DB_AVAILABLE and check_db_available(args.debug):
            try:
                conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
                cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
                cursor.execute("""
                    SELECT context_uuid
                    FROM ai_chats 
                    ORDER BY request_timestamp DESC
                    LIMIT 1
                """)
                result = cursor.fetchone()
                if result:
                    context_id = str(result['context_uuid'])
                cursor.close()
                conn.close()
            except:
                pass
    elif args.context_id:
        context_id = args.context_id
    elif args.context_ids:
        context_id = args.context_ids[0]  # Use first chat's context
        # Need to resolve chat ID to context ID
        if DB_AVAILABLE and check_db_available(args.debug):
            try:
                conn = psycopg2.connect(**AI_CHATS_DB_CONFIG)
                cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
                
                chat_id = args.context_ids[0]
                if len(chat_id) == 8:
                    cursor.execute("""
                        SELECT context_uuid FROM ai_chats 
                        WHERE CAST(id as TEXT) LIKE %s
                    """, [f"{chat_id}%"])
                else:
                    cursor.execute("""
                        SELECT context_uuid FROM ai_chats 
                        WHERE id = %s
                    """, [chat_id])
                
                result = cursor.fetchone()
                if result:
                    context_id = str(result['context_uuid'])
                cursor.close()
                conn.close()
            except:
                pass
    
    # Display context if available
    if context_id:
        display_full_conversation_history(context_uuid=context_id, debug=args.debug, no_summary=args.no_summary)
    elif args.context_ids:
        display_full_conversation_history(chat_id=args.context_ids[0], debug=args.debug, no_summary=args.no_summary)
    
    print("\n💡 Commands:")
    print("   Type your message and press Enter")
    print("   Use @<file> to include files")
    print("   Use @image:<file> to include images")
    print("   Use @note to select notes from second brain")
    print("   Use @fimage to select images with fzf")
    print("   Use @ffile to select any file with fzf")
    print("   Type '/exit' to exit")
    print("   Type '/help' for more commands")
    print("─" * 50)
    
    conversation_count = 0
    last_chat_id = None  # Track the last chat ID for conversation continuity
    
    # Check if we have an initial prompt from command line
    initial_prompt = None
    if hasattr(args, 'prompt') and args.prompt:
        initial_prompt = args.prompt
    elif hasattr(args, 'prompt_flag') and args.prompt_flag:
        initial_prompt = args.prompt_flag
    
    try:
        # Process initial prompt if provided
        if initial_prompt:
            print(f"\n🟢 You [1]: {initial_prompt}")
            user_input = initial_prompt
            # Set to None so we don't process it again
            initial_prompt = None
        else:
            # Get user input normally
            user_input = None
        
        while True:
            try:
                # Get user input if we don't have one from initial prompt
                if user_input is None:
                    user_input = input(f"\n🟢 You [{conversation_count + 1}]: ").strip()
                
                if not user_input:
                    user_input = None
                    continue
                
                # Handle special commands
                if user_input.lower() in ['/exit', '/quit']:
                    print("\n👋 Goodbye!")
                    break
                elif user_input.lower() == '/help':
                    print("\n💡 Available commands:")
                    print("   /exit, /quit - Exit REPL")
                    print("   /help - Show this help")
                    print("   /search-chat <query> - Search chat history")
                    print("   /search-context <query> - Search contexts")
                    print("   /load-chat <chat-id> - Load chat's context")
                    print("   /load-context <context-id> - Load context")
                    print("   /model <model-name> - Change current model")
                    print("   /model-list - List available models")
                    print("   /set-name <name> - Set name for current context")
                    print("   /get-name - Get name of current context")
                    print("   /add-tags <tag1,tag2> - Add tags to current context")
                    print("   /list-tags - List tags for current context")
                    print("   /rm-tags <tag1,tag2> - Remove tags from current context")
                    print("   /serve-url - Get URL to view context in aipy-serve")
                    print("   /serve-url-markdown - Get URL to download context as markdown")
                    print("   /ingest <filename.md> - Save context to second brain")
                    print("   @<file> - Include file content")
                    print("   @image:<file> - Include image")
                    print("   @note - Select notes from second brain with fzf")
                    print("   @fimage - Select images with fzf")
                    print("   @ffile - Select any file with fzf")
                    print('   @bash:"<cmd>" - Execute bash command and include output')
                    print("   Examples:")
                    print("     /search-chat python debugging")
                    print("     /load-chat c18ffd76")
                    print("     /model claude-3-5-sonnet")
                    print("     /set-name My Python Project")
                    print("     /add-tags python,debugging,api")
                    print("     /list-tags")
                    print("     /rm-tags debugging")
                    print("     /ingest important_chat.md")
                    print("     What is this? @script.py")
                    print("     Analyze this: @note")
                    print("     Describe this: @fimage")
                    print('     Check system status: @bash:"df -h"')
                    user_input = None
                    continue
                elif user_input.lower().startswith('/search-chat '):
                    query = user_input[13:].strip()
                    if query:
                        repl_search_chats(query, debug=args.debug)
                    else:
                        print("⚠️  Usage: /search-chat <query>")
                    user_input = None
                    continue
                elif user_input.lower().startswith('/search-context '):
                    query = user_input[16:].strip()
                    if query:
                        repl_search_contexts(query, debug=args.debug)
                    else:
                        print("⚠️  Usage: /search-context <query>")
                    user_input = None
                    continue
                elif user_input.lower().startswith('/load-chat '):
                    chat_id = user_input[11:].strip()
                    if chat_id:
                        new_context = repl_load_chat(chat_id, debug=args.debug)
                        if new_context:
                            context_id = new_context
                            display_full_conversation_history(context_uuid=context_id, debug=args.debug, no_summary=args.no_summary)
                    else:
                        print("⚠️  Usage: /load-chat <chat-id>")
                    user_input = None
                    continue
                elif user_input.lower().startswith('/load-context '):
                    ctx_id = user_input[14:].strip()
                    if ctx_id:
                        new_context = repl_load_context(ctx_id, debug=args.debug)
                        if new_context:
                            context_id = new_context
                            display_full_conversation_history(context_uuid=context_id, debug=args.debug, no_summary=args.no_summary)
                    else:
                        print("⚠️  Usage: /load-context <context-id>")
                    user_input = None
                    continue
                elif user_input.lower().startswith('/model '):
                    new_model = user_input[7:].strip()
                    if new_model:
                        # Validate model exists in our mapping
                        if new_model in MODEL_PROVIDERS or new_model == "grok-4":
                            args.model = new_model
                            print(f"✅ Model changed to: {new_model}")
                        else:
                            print(f"⚠️  Unknown model: {new_model}")
                            print("💡 Use /model-list to see available models")
                    else:
                        print("⚠️  Usage: /model <model-name>")
                    user_input = None
                    continue
                elif user_input.lower() == '/model-list':
                    print("\n📋 Available models:")
                    print("─" * 40)
                    
                    # Group by provider
                    providers = {}
                    for model, provider in MODEL_PROVIDERS.items():
                        if provider not in providers:
                            providers[provider] = []
                        providers[provider].append(model)
                    
                    # Add default model
                    providers.setdefault("grokpy", []).insert(0, "grok-4")
                    
                    for provider, models in sorted(providers.items()):
                        provider_name = provider.replace("py", "").title()
                        print(f"\n🔧 {provider_name}:")
                        for model in sorted(set(models)):
                            current = " (current)" if model == args.model else ""
                            print(f"   • {model}{current}")
                    
                    print(f"\n💡 Current model: {args.model}")
                    print("💡 Use: /model <model-name> to switch")
                    user_input = None
                    continue
                elif user_input.lower().startswith('/set-name '):
                    name = user_input[10:].strip()
                    if name and context_id:
                        if repl_set_context_name(context_id, name, debug=args.debug):
                            print(f"✅ Context name set to: {name}")
                        else:
                            print("❌ Failed to set context name")
                    elif not context_id:
                        print("⚠️  No active context. Start a conversation first.")
                    else:
                        print("⚠️  Usage: /set-name <name>")
                    user_input = None
                    continue
                elif user_input.lower() == '/get-name':
                    if context_id:
                        name = repl_get_context_name(context_id, debug=args.debug)
                        if name:
                            print(f"📝 Current context name: {name}")
                        else:
                            print("❌ Failed to get context name")
                    else:
                        print("⚠️  No active context. Start a conversation first.")
                    user_input = None
                    continue
                elif user_input.lower() == '/serve-url':
                    if context_id:
                        serve_port = os.environ.get('AIPY_SERVE_PORT', '5000')
                        serve_host = os.environ.get('AIPY_SERVE_HOST', '127.0.0.1')
                        url = f"http://{serve_host}:{serve_port}/context/{context_id}"
                        print(f"🌐 View context in aipy-serve: {url}")
                    else:
                        print("⚠️  No active context. Start a conversation first.")
                    user_input = None
                    continue
                elif user_input.lower() == '/serve-url-markdown':
                    if context_id:
                        serve_port = os.environ.get('AIPY_SERVE_PORT', '5000')
                        serve_host = os.environ.get('AIPY_SERVE_HOST', '127.0.0.1')
                        url = f"http://{serve_host}:{serve_port}/context/{context_id}/download?format=md"
                        print(f"💾 Download context as markdown: {url}")
                    else:
                        print("⚠️  No active context. Start a conversation first.")
                    user_input = None
                    continue
                elif user_input.lower().startswith('/ingest '):
                    filename = user_input[8:].strip()
                    if filename and context_id:
                        if repl_ingest_to_second_brain(context_id, filename, debug=args.debug):
                            print(f"✅ Context saved to second brain")
                        else:
                            print("❌ Failed to save context to second brain")
                    elif not context_id:
                        print("⚠️  No active context. Start a conversation first.")
                    else:
                        print("⚠️  Usage: /ingest <filename.md>")
                    user_input = None
                    continue
                elif user_input.lower().startswith('/add-tags '):
                    tags_str = user_input[10:].strip()
                    if tags_str and context_id:
                        tags = [tag.strip() for tag in tags_str.split(',')]
                        if repl_add_tags(context_id, tags, debug=args.debug):
                            print(f"✅ Added tags: {', '.join(tags)}")
                        else:
                            print("❌ Failed to add tags")
                    elif not context_id:
                        print("⚠️  No active context. Start a conversation first.")
                    else:
                        print("⚠️  Usage: /add-tags <tag1,tag2,tag3>")
                    user_input = None
                    continue
                elif user_input.lower() == '/list-tags':
                    if context_id:
                        tags = repl_list_tags(context_id, debug=args.debug)
                        if tags is not None:
                            if tags:
                                print(f"🏷️  Tags: {', '.join(tags)}")
                            else:
                                print("📭 No tags for this context")
                        else:
                            print("❌ Failed to list tags")
                    else:
                        print("⚠️  No active context. Start a conversation first.")
                    user_input = None
                    continue
                elif user_input.lower().startswith('/rm-tags '):
                    tags_str = user_input[9:].strip()
                    if tags_str and context_id:
                        tags = [tag.strip() for tag in tags_str.split(',')]
                        success, removed_tags = repl_remove_tags(context_id, tags, debug=args.debug)
                        if success:
                            if removed_tags:
                                print(f"✅ Removed tags: {', '.join(removed_tags)}")
                            else:
                                print("ℹ️  No matching tags to remove")
                        else:
                            print("❌ Failed to remove tags")
                    elif not context_id:
                        print("⚠️  No active context. Start a conversation first.")
                    else:
                        print("⚠️  Usage: /rm-tags <tag1,tag2,tag3>")
                    user_input = None
                    continue
                
                # Parse file references
                clean_prompt, files, images = parse_file_references(user_input, args.debug)
                
                if not clean_prompt and not files and not images:
                    print("⚠️  Empty prompt after parsing file references")
                    continue
                
                conversation_count += 1
                
                # Build full prompt with conversation history if we have a context
                full_prompt = clean_prompt
                if last_chat_id or context_id:
                    history = get_conversation_history(last_chat_id or context_id, is_chat_id=bool(last_chat_id), debug=args.debug)
                    if history:
                        full_prompt = history + "\n\n" + clean_prompt
                
                # Build command arguments (no context flags - we include history in prompt)
                cmd_args = [
                    sys.argv[0],  # Use current script path
                    "--model", args.model,
                    "--show-id"
                ]
                
                # Add context flag to maintain context continuity
                if context_id:
                    cmd_args.extend(["--context-id", context_id])
                
                # Add files
                for file_path in files:
                    cmd_args.extend(["-f", file_path])
                
                # Add images
                for image_path in images:
                    cmd_args.extend(["--image", image_path])
                
                # Add debug if needed
                if args.debug:
                    cmd_args.append("--debug")
                
                # Add tags if provided
                if args.tags:
                    cmd_args.extend(["--tags", args.tags])
                
                # Add name if provided and this is the first message (no context yet)
                if hasattr(args, 'name') and args.name and not context_id:
                    cmd_args.extend(["--name", args.name])
                
                # Force no-streaming in REPL mode for better markdown rendering
                cmd_args.append("--no-streaming")
                
                # Add the full prompt (with history if available)
                cmd_args.append(full_prompt)
                
                # Debug: print command if in debug mode
                if args.debug:
                    print(f"Debug: Executing command: {' '.join(cmd_args)}", file=sys.stderr)
                
                print(f"\n🤖 {args.model}:")
                print("─" * 40)
                
                # Execute the command - REPL always uses markdown rendering (no streaming)
                result = subprocess.run(cmd_args, capture_output=True, text=True)
                
                # Render the AI response as markdown
                if result.stdout:
                    response_text = result.stdout.strip()
                    if response_text:
                        rendered_output = render_markdown(response_text)
                        if rendered_output:  # Fallback if markdown rendering failed
                            print(rendered_output, end='')
                
                # Check stderr for context UUID and chat ID to maintain context continuity
                current_chat_id = None
                if result.stderr:
                    stderr_lines = result.stderr.strip().split('\n')
                    for line in stderr_lines:
                        if 'Context UUID:' in line:
                            # Extract the context UUID for future messages
                            new_context_id = line.split('Context UUID: ')[-1].strip()
                            if new_context_id:
                                if not context_id:
                                    context_id = new_context_id
                                    if args.debug:
                                        print(f"🔗 Captured new context ID: {context_id[:8]}...", file=sys.stderr)
                                elif context_id != new_context_id:
                                    # This shouldn't normally happen, but log if it does
                                    if args.debug:
                                        print(f"⚠️  Context ID changed from {context_id[:8]}... to {new_context_id[:8]}...", file=sys.stderr)
                                    context_id = new_context_id
                        elif 'New chat ID:' in line:
                            # Capture the new chat ID for conversation continuity
                            new_chat_id = line.split('New chat ID: ')[-1].strip()
                            if new_chat_id:
                                last_chat_id = new_chat_id
                                current_chat_id = new_chat_id
                                if args.debug:
                                    print(f"🔗 Captured chat ID: {last_chat_id[:8]}...", file=sys.stderr)
                        else:
                            # Print other stderr messages normally
                            print(line, file=sys.stderr)
                
                # Always display context and chat IDs after the response
                print("─" * 40)
                if context_id:
                    print(f"🧠 Context: {context_id[:8]}...")
                if current_chat_id:
                    print(f"💬 Chat: {current_chat_id[:8]}...")
                elif last_chat_id:
                    print(f"💬 Chat: {last_chat_id[:8]}...")
                print("─" * 40)
                
                if result.returncode != 0:
                    print(f"⚠️  Error: Command failed with return code {result.returncode}")
                
                print()
                
                # Reset user_input so we ask for it in the next iteration
                user_input = None
                
            except KeyboardInterrupt:
                print("\n\n👋 Interrupted. Type 'exit' to quit or continue chatting.")
                user_input = None
                continue
            except EOFError:
                print("\n👋 Goodbye!")
                break
            except Exception as e:
                print(f"⚠️  Error: {e}", file=sys.stderr)
                continue
    
    except Exception as e:
        print(f"⚠️  REPL error: {e}", file=sys.stderr)

def main():
    args = parse_arguments()
    
    # Handle web server mode - redirect to aipy-serve for backward compatibility
    if args.serve:
        try:
            import shutil
            aipy_serve_path = shutil.which('aipy-serve')
            if aipy_serve_path:
                # Re-exec to aipy-serve with appropriate arguments
                exec_args = ['aipy-serve']
                if args.debug:
                    exec_args.append('--debug')
                # Add any other serve-specific arguments if they exist
                os.execv(aipy_serve_path, exec_args)
            else:
                print("Error: aipy-serve command not found. Please ensure it's installed.", file=sys.stderr)
                return
        except Exception as e:
            print(f"Error launching aipy-serve: {e}", file=sys.stderr)
            print("Web server functionality has been moved to aipy-serve. Please install aipy-serve.", file=sys.stderr)
        return
    
    # Handle FZF search options (before REPL to allow context selection first)
    if args.fzf_chat:
        fzf_search_chats(args)
        return
    
    if args.fzf_context:
        fzf_search_contexts(args)
        return
    
    # Handle REPL mode
    if args.repl:
        start_repl(args)
        return

    # Handle --helper mode
    if args.helper:
        if args.debug:
            print("Debug: Running in helper mode", file=sys.stderr)

        suggestion = generate_helper_suggestion(args)
        print(suggestion)
        sys.exit(0)

    # Handle database operations first
    if args.search:
        tags_filter = None
        if args.tags_filter:
            tags_filter = [tag.strip() for tag in args.tags_filter.split(',') if tag.strip()]
        
        search_ai_chats(
            args.search, 
            limit=args.limit,
            provider_filter=args.provider_filter,
            model_filter=args.model_filter,
            tags_filter=tags_filter,
            output_format='json' if args.json else 'text',
            debug=args.debug
        )
        return
    
    if args.search_contexts:
        search_contexts(
            args.search_contexts,
            limit=args.limit,
            output_format='json' if args.json else 'text',
            debug=args.debug
        )
        return
    
    if args.view:
        view_ai_chat(
            args.view,
            output_format='json' if args.json else 'text',
            debug=args.debug,
            show_history=args.history,
            show_full_history=args.full_history,
            trace_mode=args.trace,
            content_only=args.content_only,
            render=args.render
        )
        return
    
    if args.delete:
        delete_ai_chat(
            args.delete,
            confirm=args.yes,
            debug=args.debug
        )
        return
    
    # Handle review operations
    if args.review_today or args.review_yesterday or args.review_date:
        review_date = None
        if args.review_today:
            review_date = datetime.now().strftime("%Y-%m-%d")
        elif args.review_yesterday:
            yesterday = datetime.now() - timedelta(days=1)
            review_date = yesterday.strftime("%Y-%m-%d")
        elif args.review_date:
            review_date = args.review_date
        
        review_chats_by_date(
            review_date,
            trace_mode=args.trace,
            content_only=args.content_only,
            output_format='json' if args.json else 'text',
            debug=args.debug
        )
        return
    
    if args.view_context:
        view_context(
            args.view_context,
            output_format='json' if args.json else 'text',
            debug=args.debug,
            content_only=args.content_only,
            by_chat_uuid=args.by_chat_uuid,
            render=args.render
        )
        return
    
    if args.edit_context:
        edit_context(
            args.edit_context,
            name=args.name,
            debug=args.debug
        )
        return
    
    if args.summarize_context:
        summarize_context(
            args.summarize_context,
            model=args.model,
            debug=args.debug
        )
        return
    
    if args.generate_context_name:
        generate_context_name(
            args.generate_context_name,
            model=args.model,
            debug=args.debug
        )
        return
    
    if args.generate_context_name_missing:
        generate_context_names_missing(
            model=args.model,
            debug=args.debug
        )
        return
    
    if args.summarize_contexts_missing:
        summarize_contexts_missing(
            model=args.model,
            debug=args.debug
        )
        return
    
    if args.summarize_contexts_outdated:
        summarize_contexts_outdated(
            model=args.model,
            debug=args.debug
        )
        return
    
    # Handle list-models specially
    if args.list_models:
        list_all_models()
        return
    
    # Check for stdin data
    stdin_data = ""
    if not sys.stdin.isatty() or sys.stdin in select.select([sys.stdin], [], [], 0)[0]:
        try:
            stdin_data = sys.stdin.read()
        except:
            pass  # If reading fails, continue without stdin data
    
    # Validate that we have some form of input for non-listing operations
    if not args.prompt and not args.prompt_flag and not stdin_data and not args.list_models:
        print("Error: No prompt provided. Use positional argument, --prompt flag, or stdin.", file=sys.stderr)
        sys.exit(1)
    
    # Handle --continue option
    if getattr(args, 'continue', False):
        last_context_uuid = get_last_chat_context(debug=args.debug)
        if last_context_uuid:
            if args.debug:
                print(f"Using --continue with context: {last_context_uuid}", file=sys.stderr)
            # Set the context_id to the last chat's context
            if args.context_id:
                print("Warning: --continue overrides --context-id option", file=sys.stderr)
            args.context_id = last_context_uuid
        else:
            print("Error: No previous chats found for --continue option", file=sys.stderr)
            sys.exit(1)
    
    # Load context from previous chats if requested
    context_chats = []
    if args.context_ids:
        context_chats = get_chat_context(args.context_ids, debug=args.debug)
        if context_chats is None:
            # Error occurred (likely duplicate partial UUIDs)
            sys.exit(1)
    
    # Combine all input sources like grokpy does
    combined_parts = []

    # Add preferences first if requested
    if args.preferences:
        prefs_content = load_preferences(debug=args.debug)
        if prefs_content:
            combined_parts.append(prefs_content)
        else:
            if args.debug:
                print("Debug: Preferences file not loaded", file=sys.stderr)

    # Add context from previous chats
    if context_chats:
        context_text = format_context_for_prompt(context_chats)
        combined_parts.append(context_text)

    # Add file contents if any
    if args.files:
        for file_path in args.files:
            try:
                with open(file_path, 'r') as f:
                    file_content = f.read()
                    combined_parts.append(f"=== File: {file_path} ===\n{file_content}\n")
            except IOError as e:
                print(f"Warning: Could not read file {file_path}: {e}", file=sys.stderr)

    # Add --prompt flag content if provided
    if args.prompt_flag:
        combined_parts.append(args.prompt_flag)

    # Add positional prompt if provided
    if args.prompt:
        combined_parts.append(args.prompt)

    # Add stdin content if any
    if stdin_data:
        combined_parts.append(stdin_data)

    # Combine all parts
    full_prompt = "\n".join(combined_parts) if combined_parts else ""

    # Store original prompt for --prompt flag
    args.original_prompt = args.prompt

    # ━━━ CHUNKING LOGIC ━━━
    # Check if chunking should be enabled and prompt is too large
    should_chunk = args.enable_chunking or args.max_chunk_size

    if should_chunk and not args.repl and full_prompt and args.times == 1:
        # Count tokens in full prompt
        full_prompt_tokens = count_tokens(full_prompt, args.debug)

        # Get model limits
        models_to_check = [m.strip() for m in args.model.split(",") if m.strip()]
        first_model = models_to_check[0] if models_to_check else args.model
        context_window, max_output = get_model_limits(first_model)

        # Determine token threshold
        if args.max_chunk_size:
            threshold = args.max_chunk_size
        else:
            threshold = int(context_window * 0.9)

        # Check if prompt exceeds threshold
        if full_prompt_tokens > threshold:
            if args.debug:
                print(f"Debug: Prompt ({full_prompt_tokens} tokens) exceeds threshold ({threshold} tokens), enabling chunking", file=sys.stderr)

            # Disable incompatible features
            if args.times and args.times > 1:
                print("Warning: --times not supported with chunking, using times=1", file=sys.stderr)
                args.times = 1

            if "," in args.model:
                print("Warning: Multiple models not fully supported with chunking, using first model only", file=sys.stderr)
                args.model = args.model.split(",")[0]

            # Force no-streaming
            args.no_streaming = True

            # Separate preferences and context from content
            preferences_text = ""
            context_text = ""
            content_only = full_prompt

            # Extract preferences if present
            if args.preferences:
                prefs_content = load_preferences(debug=args.debug)
                if prefs_content:
                    preferences_text = prefs_content
                    # Remove preferences from content_only
                    if content_only.startswith(preferences_text):
                        content_only = content_only[len(preferences_text):].strip()

            if context_chats:
                # Extract context portion
                context_text = format_context_for_prompt(context_chats)
                # Remove context from content_only if present
                if content_only.startswith(context_text):
                    content_only = content_only[len(context_text):].strip()

            # Get provider
            if args.provider:
                provider = args.provider
            else:
                provider = get_provider_for_model(args.model)

            # Process with chunking
            try:
                final_response, chunk_metadata = process_with_chunking(
                    full_prompt, context_text, content_only, provider, args.model,
                    args, args.debug, preferences_text
                )

                # Save transaction if not disabled
                if not args.no_preserve:
                    end_time = time.time()
                    metadata = {
                        'chunking_enabled': True,
                        'num_chunks': len([m for m in chunk_metadata if 'chunk' in m]),
                        'chunk_metadata': chunk_metadata
                    }

                    db_metadata = {
                        'request_timestamp': datetime.fromtimestamp(start_time),
                        'response_timestamp': datetime.fromtimestamp(end_time),
                        'duration_ms': int((end_time - start_time) * 1000),
                        'extra_metadata': metadata,
                        'source': 'aipy_chunked'
                    }

                    if not args.dry_run:
                        store_chat_in_db(
                            prompt=full_prompt,
                            response=final_response,
                            provider=provider,
                            model=args.model,
                            tags=tags,
                            metadata=db_metadata,
                            context_uuid=args.context_id if hasattr(args, 'context_id') else None,
                            debug=args.debug
                        )

                # Output final response
                if args.render and RICH_AVAILABLE:
                    render_markdown(final_response)
                else:
                    print(final_response)

                if args.show_id:
                    print(f"Chat ID: {args.context_id if hasattr(args, 'context_id') else 'N/A'}", file=sys.stderr)

                sys.exit(0)
            except Exception as e:
                print(f"Error during chunked processing: {e}", file=sys.stderr)
                if args.debug:
                    import traceback
                    traceback.print_exc(file=sys.stderr)
                sys.exit(1)

    # Parse tags if provided
    tags = []
    if args.tags:
        tags = [tag.strip() for tag in args.tags.split(',') if tag.strip()]

    # Automatically add 'follow-up' tag when using context
    if args.context_ids:
        if 'follow-up' not in tags:
            tags.append('follow-up')

    # Convert back to None if empty for consistency
    tags = tags if tags else None

    # Parse models (can be comma-separated)
    models = [m.strip() for m in args.model.split(",") if m.strip()]
    
    # Prepare all commands to execute
    commands = []
    
    for model in models:
        # Determine provider
        if args.provider:
            provider = args.provider
        else:
            provider = get_provider_for_model(model)
        
        # Generate commands for multiple iterations
        for iteration in range(args.times):
            cmd_and_stdin = run_provider_command(provider, args, model, 
                                                 iteration if args.times > 1 else None, 
                                                 full_prompt if full_prompt else None)
            commands.append((cmd_and_stdin, model, iteration if args.times > 1 else None))
    
    # Track execution time
    start_time = time.time()
    
    # Execute commands
    if len(commands) == 1 and args.times == 1:
        # Single command - run directly for better interactive experience
        cmd_and_stdin, model, iteration = commands[0]
        cmd, stdin_input = cmd_and_stdin
        
        if args.debug:
            print(f"Debug: Executing: {' '.join(cmd)}", file=sys.stderr)
            if stdin_input:
                print(f"Debug: With stdin input: {len(stdin_input)} characters", file=sys.stderr)
        
        try:
            # Use streaming unless disabled or render mode is active
            stream_output = not args.no_streaming and not args.render
            # Always capture output when preserving chats to database or when rendering
            capture_output = not args.no_preserve or args.render
            result = execute_command(cmd_and_stdin, model, iteration, capture_output=capture_output, stream_output=stream_output)
            
            # Note: For streaming, output is already printed in execute_command
            # For non-streaming or render mode, we need to print the captured output
            if args.no_streaming or args.render:
                if result.get("output"):
                    if args.render:
                        # Use markdown rendering if requested
                        render_markdown(result["output"])
                    else:
                        print(result["output"], end='')
                if result.get("stderr"):
                    print(result["stderr"], file=sys.stderr, end='')
            
            # Convert execute_command result to subprocess-like result for compatibility
            class MockResult:
                def __init__(self, returncode, stdout="", stderr=""):
                    self.returncode = returncode
                    self.stdout = stdout
                    self.stderr = stderr
            
            result_stdout = result.get("output", "")
            result_stderr = result.get("stderr", "")
            result = MockResult(result["returncode"], result_stdout, result_stderr)
            
            # Save transaction if not disabled
            if not args.no_preserve and result.returncode == 0:
                end_time = time.time()
                provider = get_provider_for_model(model)
                new_id = None  # Initialize to handle dry-run case
                
                # For single model, also store directly in DB
                if not args.dry_run:
                    db_metadata = {
                        'request_timestamp': datetime.fromtimestamp(start_time),
                        'response_timestamp': datetime.fromtimestamp(end_time),
                        'duration_ms': int((end_time - start_time) * 1000),
                        'extra_metadata': {
                            'source': 'aipy_single_model',
                            'execution_type': 'single',
                            'iterations': 1
                        }
                    }
                    
                    new_id = store_chat_in_db(
                        full_prompt, 
                        result.stdout, 
                        provider, 
                        model, 
                        db_metadata,
                        tags=tags,
                        context_ids=args.context_ids,
                        context_id=args.context_id,
                        debug=args.debug, 
                        dry_run=args.dry_run,
                        show_id=args.show_id,
                        auto_name=getattr(args, 'auto_name', False),
                        context_name=getattr(args, 'name', None)
                    )
                
                save_multi_model_transaction(
                    [model], 
                    full_prompt, 
                    [{"success": True, "output": result.stdout}],
                    {"total_time": end_time - start_time, "parallel": False, "iterations": 1, "new_chat_ids": [new_id] if new_id else []},
                    tags=tags,
                    context_ids=args.context_ids,
                    context_id=args.context_id,
                    store_db=False,  # Already stored above
                    save_sb_md=args.do_sb_logging,
                    debug=args.debug,
                    dry_run=args.dry_run,
                    show_id=args.show_id,
                    context_name=getattr(args, 'name', None)
                )
                
                # Show summary for single model if requested
                if args.summary and new_id:
                    print(f"\nNew chat ID: {new_id}", file=sys.stderr)
                
            if result.returncode != 0:
                sys.exit(result.returncode)
                
        except KeyboardInterrupt:
            print("\nInterrupted by user", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"Error executing command: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        # Multiple commands - need to capture output for preservation or pick-best
        capture_output = not args.no_preserve or args.pick_best
        
        if len(commands) > 1:
            print(f"Executing {len(commands)} requests in parallel...\n", file=sys.stderr)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(commands), 8)) as executor:
            futures = []
            for cmd_and_stdin, model, iteration in commands:
                cmd, stdin_input = cmd_and_stdin
                if args.debug:
                    print(f"Debug: Queuing: {' '.join(cmd)}", file=sys.stderr)
                    if stdin_input:
                        print(f"Debug: With stdin input: {len(stdin_input)} characters", file=sys.stderr)
                # Disable streaming for parallel execution to avoid confusing output
                stream_output = False
                future = executor.submit(execute_command, cmd_and_stdin, model, iteration, capture_output, stream_output)
                futures.append(future)
            
            # Wait for all to complete
            results = []
            model_order = []
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    model_order.append(result.get("model", "unknown"))
                    
                    # Print output as it arrives (if captured and not using pick-best)
                    if not args.pick_best and result.get("output"):
                        print(f"\n[{result.get('model', 'unknown')}]", file=sys.stderr)
                        if args.render:
                            # Use markdown rendering if requested
                            render_markdown(result["output"])
                        else:
                            print(result["output"], end='')
                    
                    if not result["success"]:
                        error_msg = result.get("error", f"Command failed with return code {result.get('returncode', 'unknown')}")
                        print(f"Error in {result.get('model', 'unknown')}: {error_msg}", file=sys.stderr)
                except Exception as e:
                    print(f"Error executing command: {e}", file=sys.stderr)
        
        # Handle pick-best logic
        best_model = None
        reasoning = None
        if args.pick_best and len(models) > 1:
            # Prepare model responses for pick_best_response
            model_responses = []
            for model in models:
                for result in results:
                    if result.get("model") == model and result.get("success"):
                        model_responses.append((model, result))
                        break
            
            if len(model_responses) > 1:
                print(f"\nAnalyzing {len(model_responses)} responses to pick the best one...", file=sys.stderr)
                best_model, reasoning = pick_best_response(full_prompt, model_responses, args.decision_model)
                
                if best_model:
                    # Find and display the best response
                    for model, result in model_responses:
                        if model == best_model:
                            print(f"\n[Best Response: {best_model}]", file=sys.stderr)
                            if args.render:
                                # Use markdown rendering if requested
                                render_markdown(result["output"])
                            else:
                                print(result["output"], end='')
                            break
                    
                    if args.debug:
                        print(f"\nDecision reasoning: {reasoning}", file=sys.stderr)
                else:
                    print("\nCould not determine best response", file=sys.stderr)
            elif len(model_responses) == 1:
                # Only one successful response, use it
                model, result = model_responses[0]
                print(f"\n[{model}]", file=sys.stderr)
                if args.render:
                    # Use markdown rendering if requested
                    render_markdown(result["output"])
                else:
                    print(result["output"], end='')
        
        # Summary
        successful = sum(1 for r in results if r["success"])
        print(f"\nCompleted: {successful}/{len(commands)} requests successful", file=sys.stderr)
        
        # Show summary with new chat IDs if requested
        if args.summary and metadata and metadata.get('new_chat_ids'):
            print(f"\nNew chat IDs created:", file=sys.stderr)
            for chat_id in metadata.get('new_chat_ids', []):
                print(f"  {chat_id}", file=sys.stderr)
        
        # Save transaction if not disabled and we have successful results
        if not args.no_preserve and successful > 0:
            end_time = time.time()
            
            # Sort results by model order for consistent output
            sorted_results = []
            sorted_models = []
            for model in models:
                for result in results:
                    if result.get("model") == model and result not in sorted_results:
                        sorted_results.append(result)
                        sorted_models.append(model)
                        break
            
            # Prepare metadata
            metadata = {
                "total_time": end_time - start_time, 
                "parallel": len(commands) > 1, 
                "iterations": args.times
            }
            
            # Add pick-best metadata if applicable
            if args.pick_best and len(models) > 1 and best_model:
                metadata["best_model"] = best_model
                metadata["decision_model"] = args.decision_model
                metadata["decision_reasoning"] = reasoning
            
            save_multi_model_transaction(
                sorted_models, 
                full_prompt, 
                sorted_results,
                metadata,
                tags=tags,
                context_ids=args.context_ids,
                context_id=args.context_id,
                store_db=True,
                save_sb_md=args.do_sb_logging,
                debug=args.debug,
                dry_run=args.dry_run,
                show_id=args.show_id,
                context_name=getattr(args, 'name', None)
            )

# ============================================================================
# CHUNKING SYSTEM FOR LARGE INPUTS
# ============================================================================

def count_tokens(text, debug=False):
    """Count tokens using token_count script, fallback to estimation."""
    try:
        result = subprocess.run(
            ['token_count', '--simple', '--only-sum'],
            input=text,
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            return int(result.stdout.strip())
    except Exception as e:
        if debug:
            print(f"Warning: Token counting failed ({e}), using estimation", file=sys.stderr)

    # Fallback to 4 chars per token
    return len(text) // 4

def detect_content_type(text):
    """Detect if content is markdown, code, or plain text."""
    # Markdown: Has headers (# ## ###)
    if re.search(r'^#{1,6}\s+.+$', text, re.MULTILINE):
        return 'markdown'

    # Code: Has function/class definitions, lots of brackets
    code_indicators = [
        r'^(def|class|function|public|private|async)\s+\w+',
        r'^\s*(if|for|while|return|import|from)\s+',
        r'\{.*\}',
    ]
    if any(re.search(pattern, text, re.MULTILINE) for pattern in code_indicators):
        return 'code'

    return 'text'

def split_content_semantically(text, max_tokens, content_type, debug=False):
    """Split content on semantic boundaries."""

    if content_type == 'markdown':
        # Split hierarchy: headers → paragraphs → sentences
        splitters = [
            (r'\n#{1,6}\s+', 'headers'),
            (r'\n\n+', 'paragraphs'),
            (r'[.!?]\s+', 'sentences'),
        ]
    elif content_type == 'code':
        # Split on function/class boundaries → empty lines
        splitters = [
            (r'\n(def|class|function|public|private|async)\s+', 'functions'),
            (r'\n\n+', 'empty_lines'),
            (r'\n', 'lines'),
        ]
    else:  # plain text
        splitters = [
            (r'\n\n+', 'paragraphs'),
            (r'[.!?]\s+', 'sentences'),
            (r'\s+', 'words'),
        ]

    chunks = []
    current_chunk = ""

    for pattern, level in splitters:
        parts = re.split(pattern, text)

        for part in parts:
            candidate = current_chunk + part
            candidate_tokens = count_tokens(candidate, debug)

            if candidate_tokens <= max_tokens:
                current_chunk = candidate
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = part

        if current_chunk and count_tokens(current_chunk, debug) <= max_tokens:
            break  # Successfully split at this level

    if current_chunk:
        chunks.append(current_chunk)

    if debug:
        print(f"Debug: Split into {len(chunks)} chunks using {content_type} strategy", file=sys.stderr)

    return chunks

def summarize_context(context_text, max_tokens=10000, provider="claudpy", model="claude-sonnet-4-5", debug=False):
    """Summarize conversation context to fit within token budget."""
    current_tokens = count_tokens(context_text, debug)

    if current_tokens <= max_tokens:
        return context_text  # No summarization needed

    if debug:
        print(f"Debug: Summarizing context ({current_tokens} → {max_tokens} tokens)", file=sys.stderr)

    summary_prompt = f"""Summarize the following conversation context into approximately {max_tokens} tokens while preserving:
1. Key facts, decisions, and conclusions
2. Important technical details
3. User preferences and requirements
4. Conversation flow and logic

Context to summarize:
{context_text}

Provide a concise summary below:"""

    # Call provider to generate summary
    cmd = [provider, "--model", model, "--no-streaming", "--no-preserve"]

    try:
        proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE, text=True)
        summary, _ = proc.communicate(input=summary_prompt, timeout=120)

        if proc.returncode == 0:
            summary_tokens = count_tokens(summary, debug)
            if debug:
                print(f"Debug: Context summarized to {summary_tokens} tokens", file=sys.stderr)
            return summary.strip()
    except Exception as e:
        if debug:
            print(f"Warning: Context summarization failed ({e}), using truncation", file=sys.stderr)

    # Fallback: truncate to max_tokens
    target_chars = max_tokens * 4
    return context_text[:target_chars] + "\n\n[... context truncated ...]"

def process_with_chunking(full_prompt, context_text, content_only, provider, model, args, debug=False, preferences_text=""):
    """Process large prompt using map-reduce chunking."""

    # Get model limits
    context_window, max_output = get_model_limits(model)
    usable_tokens = int(context_window * 0.9)

    if debug:
        print(f"Debug: Chunking enabled", file=sys.stderr)
        print(f"Debug:   Model: {model} (context: {context_window}, output: {max_output})", file=sys.stderr)

    # Determine token budgets
    chunk_overhead = 500

    if args.summarize_context and context_text:
        # Summarize context for all chunks
        context_summary = summarize_context(context_text, max_tokens=10000,
                                           provider=provider, model=model, debug=debug)
        context_tokens = count_tokens(context_summary, debug)
        chunk_budget = usable_tokens - context_tokens - max_output - chunk_overhead
        context_for_chunks = context_summary
    else:
        # Full context in first chunk only
        context_tokens = count_tokens(context_text, debug) if context_text else 0
        first_chunk_budget = usable_tokens - context_tokens - max_output - chunk_overhead
        subsequent_chunk_budget = usable_tokens - max_output - chunk_overhead
        chunk_budget = subsequent_chunk_budget  # Use smaller for splitting
        context_for_chunks = None

    # Detect content type and split
    content_type = detect_content_type(content_only)
    chunks = split_content_semantically(content_only, chunk_budget, content_type, debug)

    if debug:
        print(f"Debug: Split content into {len(chunks)} chunks", file=sys.stderr)
        for i, chunk in enumerate(chunks, 1):
            chunk_tokens = count_tokens(chunk, debug)
            print(f"Debug:   Chunk {i}: {chunk_tokens} tokens", file=sys.stderr)

    # Process each chunk
    chunk_responses = []
    chunk_metadata = []

    for i, chunk in enumerate(chunks, 1):
        if debug:
            print(f"Debug: Processing chunk {i}/{len(chunks)}...", file=sys.stderr)

        # Build chunk prompt
        chunk_parts = []
        if preferences_text:
            chunk_parts.append(preferences_text)
        if i == 1 and not args.summarize_context and context_text:
            chunk_parts.append(context_text)
        elif context_for_chunks:
            chunk_parts.append(context_for_chunks)
        chunk_parts.append(f"=== CONTENT CHUNK {i}/{len(chunks)} ===\n\n{chunk}")
        chunk_prompt = "\n\n".join(chunk_parts)

        # Process chunk (call provider)
        cmd = [provider, "--model", model, "--no-streaming", "--no-preserve"]

        try:
            proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE,
                                  stderr=subprocess.PIPE, text=True)
            response, stderr = proc.communicate(input=chunk_prompt, timeout=300)

            if proc.returncode == 0:
                chunk_responses.append(response.strip())
                chunk_metadata.append({
                    "chunk": i,
                    "error": False,
                    "tokens_input": count_tokens(chunk_prompt, debug),
                    "tokens_output": count_tokens(response, debug)
                })
            else:
                error_msg = f"[Chunk {i} processing failed: {stderr[:200]}]"
                chunk_responses.append(error_msg)
                chunk_metadata.append({"chunk": i, "error": True, "error_msg": stderr.strip()[:200]})
                if debug:
                    print(f"Warning: Chunk {i} failed: {stderr[:200]}", file=sys.stderr)
        except Exception as e:
            error_msg = f"[Chunk {i} processing error: {str(e)[:100]}]"
            chunk_responses.append(error_msg)
            chunk_metadata.append({"chunk": i, "error": True, "error_msg": str(e)[:100]})
            if debug:
                print(f"Warning: Chunk {i} error: {e}", file=sys.stderr)

    # Check if too many chunks failed
    failed_chunks = sum(1 for m in chunk_metadata if m.get("error"))
    if len(chunks) > 0 and failed_chunks / len(chunks) > 0.5:
        print(f"Error: {failed_chunks}/{len(chunks)} chunks failed, aborting summary", file=sys.stderr)
        return "\n\n".join(chunk_responses), chunk_metadata

    # Generate map-reduce summary
    if debug:
        print(f"Debug: Generating final summary...", file=sys.stderr)

    summary_prompt = ""

    if preferences_text:
        summary_prompt += preferences_text + "\n\n"

    if context_text:
        summary_prompt += context_text + "\n\n"

    summary_prompt += f"""I received responses to {len(chunks)} chunks of a larger query.
Please provide a cohesive, unified response that:
1. Synthesizes information from all chunks
2. Eliminates redundancy
3. Maintains logical flow
4. Provides a complete answer to the original question

"""

    for i, response in enumerate(chunk_responses, 1):
        summary_prompt += f"\n=== RESPONSE TO CHUNK {i}/{len(chunks)} ===\n{response}\n"

    summary_prompt += "\nPlease provide the final unified response below:"

    # Call provider for summary
    cmd = [provider, "--model", model, "--no-streaming", "--no-preserve"]

    try:
        proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE, text=True)
        final_response, _ = proc.communicate(input=summary_prompt, timeout=300)

        if proc.returncode == 0:
            summary_metadata = {
                "tokens_input": count_tokens(summary_prompt, debug),
                "tokens_output": count_tokens(final_response, debug)
            }
            chunk_metadata.append({"summary_call": summary_metadata})

            if debug:
                total_input = sum(m.get("tokens_input", 0) for m in chunk_metadata if "chunk" in m or "summary_call" in m)
                total_output = sum(m.get("tokens_output", 0) for m in chunk_metadata if "chunk" in m or "summary_call" in m)
                print(f"Debug: Total tokens: input={total_input}, output={total_output}", file=sys.stderr)

            return final_response.strip(), chunk_metadata
    except Exception as e:
        if debug:
            print(f"Warning: Summary generation failed ({e}), returning concatenated chunks", file=sys.stderr)

    # Fallback: concatenate chunk responses
    return "\n\n".join(chunk_responses), chunk_metadata

if __name__ == "__main__":
    main()
