#!/usr/bin/env python3

# dotfiles - Personal configuration files and scripts
# Copyright (C) 2025  Zach Podbielniak
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import argparse
import hashlib
import os
import sys
import shutil
import zlib
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional

VERSION = "2.0.0"

class DuplicateFinder:
    def __init__(self, args):
        self.args = args
        self.total_files = 0
        self.initial_matches = 0
        
    def get_file_size(self, filepath: Path) -> int:
        """Get file size in bytes."""
        try:
            return filepath.stat().st_size
        except (OSError, IOError):
            return 0
    
    def get_file_mtime(self, filepath: Path) -> float:
        """Get file modification time."""
        try:
            return filepath.stat().st_mtime
        except (OSError, IOError):
            return 0.0
    
    def calc_crc32(self, filepath: Path) -> Optional[str]:
        """Calculate CRC32 checksum of a file."""
        try:
            crc = 0
            with open(filepath, 'rb') as f:
                while chunk := f.read(65536):  # 64KB chunks
                    crc = zlib.crc32(chunk, crc)
            return f"{crc & 0xffffffff:08x}"
        except (OSError, IOError):
            return None
    
    def calc_sha256(self, filepath: Path) -> Optional[str]:
        """Calculate SHA256 checksum of a file."""
        try:
            sha256_hash = hashlib.sha256()
            with open(filepath, 'rb') as f:
                while chunk := f.read(65536):  # 64KB chunks
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()
        except (OSError, IOError):
            return None
    
    def collect_files(self, paths: List[str]) -> List[Path]:
        """Collect all files to process based on arguments and options."""
        files = []
        
        # Read from stdin if available
        if not sys.stdin.isatty():
            for line in sys.stdin:
                line = line.strip()
                if line:
                    files.append(Path(line))
        
        # Add command line arguments
        for path_str in paths:
            path = Path(path_str)
            if path.is_file() and not path.is_symlink():
                files.append(path)
            elif path.is_dir() and self.args.recursive:
                files.extend(p for p in path.rglob('*') if p.is_file() and not p.is_symlink())
            elif path.is_dir() and not self.args.recursive:
                print(f"Warning: '{path}' is a directory. Use -r/--recursive to process its contents.", file=sys.stderr)
            elif not path.exists():
                print(f"Warning: '{path}' does not exist, skipping.", file=sys.stderr)
        
        # Remove duplicates and sort
        return sorted(set(files))
    
    def find_duplicates_by_size(self, files: List[Path]) -> Dict[str, List[Tuple[Path, float]]]:
        """Find duplicates using file size first, then SHA256."""
        print("Phase 1: Calculating file sizes...", file=sys.stderr)
        
        # Group files by size
        size_groups = defaultdict(list)
        for filepath in files:
            if filepath.is_file() and not filepath.is_symlink():
                size = self.get_file_size(filepath)
                if size > 0:  # Skip empty files
                    mtime = self.get_file_mtime(filepath)
                    size_groups[size].append((filepath, mtime))
                    self.total_files += 1
                    if self.args.dry_run:
                        print(f"[DRY-RUN] Would calculate file size for: {filepath}", file=sys.stderr)
        
        print(f"Processed {self.total_files} files.", file=sys.stderr)
        
        # Find size duplicates (groups with more than one file)
        size_duplicates = {size: paths for size, paths in size_groups.items() if len(paths) > 1}
        
        if not size_duplicates:
            return {}
        
        print("Phase 2: Finding size matches...", file=sys.stderr)
        print(f"Found {sum(len(paths) for paths in size_duplicates.values())} files with duplicate sizes.", file=sys.stderr)
        
        # Calculate SHA256 for files with duplicate sizes
        print("Phase 3: Verifying duplicates with SHA256...", file=sys.stderr)
        
        sha_groups = defaultdict(list)
        
        def process_file(filepath_mtime):
            filepath, mtime = filepath_mtime
            sha = self.calc_sha256(filepath)
            if sha:
                if self.args.dry_run:
                    print(f"[DRY-RUN] Would calculate SHA256 for: {filepath}", file=sys.stderr)
                return sha, filepath, mtime
            return None
        
        # Process files in parallel
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            futures = []
            for paths in size_duplicates.values():
                for filepath_mtime in paths:
                    futures.append(executor.submit(process_file, filepath_mtime))
            
            for future in as_completed(futures):
                result = future.result()
                if result:
                    sha, filepath, mtime = result
                    sha_groups[sha].append((filepath, mtime))
                    self.initial_matches += 1
        
        print(f"Verified {self.initial_matches} potential duplicates.", file=sys.stderr)
        
        # Return only groups with actual duplicates
        return {sha: paths for sha, paths in sha_groups.items() if len(paths) > 1}
    
    def find_duplicates_by_crc32(self, files: List[Path]) -> Dict[str, List[Tuple[Path, float]]]:
        """Find duplicates using CRC32 first, then SHA256."""
        print("Phase 1: Calculating CRC32 checksums...", file=sys.stderr)
        
        # Calculate CRC32 for all files
        crc_groups = defaultdict(list)
        
        def process_file_crc(filepath):
            if filepath.is_file() and not filepath.is_symlink():
                crc = self.calc_crc32(filepath)
                if crc:
                    mtime = self.get_file_mtime(filepath)
                    if self.args.dry_run:
                        print(f"[DRY-RUN] Would calculate CRC32 for: {filepath}", file=sys.stderr)
                    return crc, filepath, mtime
            return None
        
        # Process files in parallel
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            futures = [executor.submit(process_file_crc, filepath) for filepath in files]
            
            for future in as_completed(futures):
                result = future.result()
                if result:
                    crc, filepath, mtime = result
                    crc_groups[crc].append((filepath, mtime))
                    self.total_files += 1
        
        print(f"Processed {self.total_files} files.", file=sys.stderr)
        
        # Find CRC32 duplicates
        crc_duplicates = {crc: paths for crc, paths in crc_groups.items() if len(paths) > 1}
        
        if not crc_duplicates:
            return {}
        
        print("Phase 2: Finding CRC32 matches...", file=sys.stderr)
        print(f"Found {sum(len(paths) for paths in crc_duplicates.values())} files with duplicate CRC32s.", file=sys.stderr)
        
        # Verify with SHA256
        print("Phase 3: Verifying duplicates with SHA256...", file=sys.stderr)
        
        sha_groups = defaultdict(list)
        
        def process_file_sha(filepath_mtime):
            filepath, mtime = filepath_mtime
            sha = self.calc_sha256(filepath)
            if sha:
                if self.args.dry_run:
                    print(f"[DRY-RUN] Would calculate SHA256 for: {filepath}", file=sys.stderr)
                return sha, filepath, mtime
            return None
        
        # Process files in parallel
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            futures = []
            for paths in crc_duplicates.values():
                for filepath_mtime in paths:
                    futures.append(executor.submit(process_file_sha, filepath_mtime))
            
            for future in as_completed(futures):
                result = future.result()
                if result:
                    sha, filepath, mtime = result
                    sha_groups[sha].append((filepath, mtime))
                    self.initial_matches += 1
        
        print(f"Verified {self.initial_matches} potential duplicates.", file=sys.stderr)
        
        # Return only groups with actual duplicates
        return {sha: paths for sha, paths in sha_groups.items() if len(paths) > 1}
    
    def handle_duplicate_group(self, sha: str, files_with_mtime: List[Tuple[Path, float]]):
        """Handle a group of duplicate files."""
        if len(files_with_mtime) < 2:
            return
        
        # Sort by modification time (oldest first)
        files_with_mtime.sort(key=lambda x: x[1])
        
        print(f"Duplicate set (SHA256: {sha}):")
        for i, (filepath, mtime) in enumerate(files_with_mtime):
            date_str = datetime.fromtimestamp(mtime).strftime("%a %b %d %I:%M:%S %p %Z %Y")
            print(f"  [{i}] {filepath} (modified: {date_str})")
        
        # Determine which file to keep
        keep_index = 0  # Default to oldest
        if self.args.keep_strategy == "oldest":
            keep_index = 0
        elif self.args.keep_strategy == "newest":
            keep_index = len(files_with_mtime) - 1
        elif self.args.keep_strategy == "ask":
            if not self.args.dry_run:
                try:
                    keep_index = int(input(f"Which file would you like to keep? (0-{len(files_with_mtime)-1}): "))
                    if keep_index < 0 or keep_index >= len(files_with_mtime):
                        print("Invalid selection, keeping oldest by default")
                        keep_index = 0
                except (ValueError, KeyboardInterrupt):
                    print("Invalid selection, keeping oldest by default")
                    keep_index = 0
            else:
                print("[DRY-RUN] Would ask which file to keep")
                keep_index = 0
        
        keep_file = files_with_mtime[keep_index][0]
        print(f"Keeping: {keep_file}")
        
        # Handle other files
        for i, (filepath, _) in enumerate(files_with_mtime):
            if i != keep_index:
                self.handle_file(filepath)
        print()
    
    def handle_file(self, filepath: Path):
        """Handle a single file (delete or archive)."""
        if self.args.delete:
            if self.args.dry_run:
                print(f"[DRY-RUN] Would delete: {filepath}", file=sys.stderr)
                print(f"[DRY-RUN] Command: rm \"{filepath}\"", file=sys.stderr)
            else:
                if self.args.no_confirm:
                    print(f"Deleting: {filepath}")
                    try:
                        filepath.unlink()
                    except OSError as e:
                        print(f"Error deleting {filepath}: {e}", file=sys.stderr)
                else:
                    response = input(f"Delete {filepath}? (y/n): ")
                    if response.lower().startswith('y'):
                        print(f"Deleting: {filepath}")
                        try:
                            filepath.unlink()
                        except OSError as e:
                            print(f"Error deleting {filepath}: {e}", file=sys.stderr)
                    else:
                        print(f"Skipped: {filepath}")
        
        elif self.args.archive:
            archive_dir = Path(self.args.archive)
            dest_file = archive_dir / filepath.name
            counter = 1
            
            # Handle filename conflicts
            while dest_file.exists():
                stem = filepath.stem
                suffix = filepath.suffix
                if suffix:
                    dest_file = archive_dir / f"{stem}.{counter}{suffix}"
                else:
                    dest_file = archive_dir / f"{filepath.name}.{counter}"
                counter += 1
            
            if self.args.dry_run:
                print(f"[DRY-RUN] Would archive: {filepath} -> {dest_file}", file=sys.stderr)
                print(f"[DRY-RUN] Command: mv \"{filepath}\" \"{dest_file}\"", file=sys.stderr)
            else:
                if self.args.no_confirm:
                    print(f"Archiving: {filepath} -> {dest_file}")
                    try:
                        shutil.move(str(filepath), str(dest_file))
                    except OSError as e:
                        print(f"Error archiving {filepath}: {e}", file=sys.stderr)
                else:
                    response = input(f"Archive {filepath}? (y/n): ")
                    if response.lower().startswith('y'):
                        print(f"Archiving: {filepath} -> {dest_file}")
                        try:
                            shutil.move(str(filepath), str(dest_file))
                        except OSError as e:
                            print(f"Error archiving {filepath}: {e}", file=sys.stderr)
                    else:
                        print(f"Skipped: {filepath}")
    
    def run(self, paths: List[str]):
        """Main execution function."""
        print("Collecting files...", file=sys.stderr)
        files = self.collect_files(paths)
        
        if not files:
            print("No files to process.", file=sys.stderr)
            return 0
        
        # Find duplicates
        if self.args.crc32_mode:
            duplicates = self.find_duplicates_by_crc32(files)
        else:
            duplicates = self.find_duplicates_by_size(files)
        
        if not duplicates:
            print("No duplicate files found.", file=sys.stderr)
            return 0
        
        # Process duplicates
        if self.args.delete or self.args.archive:
            print(file=sys.stderr)
            print("Processing duplicates...", file=sys.stderr)
            for sha, files_with_mtime in duplicates.items():
                self.handle_duplicate_group(sha, files_with_mtime)
        else:
            # Just display duplicates
            print(file=sys.stderr)
            print("Duplicate files found:", file=sys.stderr)
            for sha, files_with_mtime in duplicates.items():
                files_with_mtime.sort(key=lambda x: x[1])  # Sort by mtime
                print(f"Duplicate set (SHA256: {sha}):")
                for filepath, mtime in files_with_mtime:
                    date_str = datetime.fromtimestamp(mtime).strftime("%a %b %d %I:%M:%S %p %Z %Y")
                    print(f"  {filepath} (modified: {date_str})")
                print()
        
        return 0


def main():
    parser = argparse.ArgumentParser(
        description="Find duplicate files using filesize for initial detection and SHA256 for verification.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
EXAMPLES:
    # Find duplicates in specific files
    find_dupes.py file1.txt file2.txt file3.txt

    # Find duplicates recursively in directories
    find_dupes.py -r /path/to/dir1 /path/to/dir2

    # Find duplicates from file list
    find . -name "*.jpg" | find_dupes.py

    # Delete duplicates, keeping oldest
    find_dupes.py -d -r /path/to/photos

    # Archive duplicates for review
    find_dupes.py -a /tmp/duplicates -r /path/to/files

    # Interactive mode - ask which to keep
    find_dupes.py -d --keep-ask -r /path/to/documents

    # See what would be deleted without actually deleting
    find_dupes.py -d --dry-run -r /path/to/media

NOTES:
    - Filesize is used for fast initial detection (default)
    - CRC32 can be used instead with --crc32-mode
    - SHA256 verifies actual duplicates (prevents false positives)
    - File modification time determines age (oldest/newest)
    - Symlinks are skipped
    - Hidden files are included by default
    - Uses parallel processing for better performance
        """
    )
    
    parser.add_argument('-v', '--version', action='version', version=f'find_dupes.py version {VERSION}')
    parser.add_argument('-d', '--delete', action='store_true', help='Delete duplicate files (keeps oldest by default)')
    parser.add_argument('-a', '--archive', metavar='DIR', help='Move duplicates to DIR instead of deleting')
    parser.add_argument('-r', '--recursive', action='store_true', help='Process directories recursively')
    parser.add_argument('--keep-oldest', action='store_const', const='oldest', dest='keep_strategy', default='oldest',
                        help='Keep oldest file when deleting (default)')
    parser.add_argument('--keep-newest', action='store_const', const='newest', dest='keep_strategy',
                        help='Keep newest file when deleting')
    parser.add_argument('--keep-ask', action='store_const', const='ask', dest='keep_strategy',
                        help='Ask which file to keep for each duplicate set')
    parser.add_argument('--no-confirm', action='store_true', help="Don't ask for confirmation before operations")
    parser.add_argument('--dry-run', action='store_true', help='Show what would be done without doing it')
    parser.add_argument('--crc32-mode', action='store_true', help='Use CRC32 for initial detection instead of filesize')
    parser.add_argument('paths', nargs='*', help='Files or directories to process')
    
    args = parser.parse_args()
    
    # Validate options
    if args.delete and args.archive:
        parser.error("Cannot specify both --delete and --archive")
    
    if args.archive:
        archive_path = Path(args.archive)
        if not archive_path.exists():
            if args.dry_run:
                print(f"[DRY-RUN] Would create archive directory: {archive_path}", file=sys.stderr)
            else:
                print(f"Creating archive directory: {archive_path}", file=sys.stderr)
                try:
                    archive_path.mkdir(parents=True, exist_ok=True)
                except OSError as e:
                    parser.error(f"Cannot create archive directory: {e}")
        elif not archive_path.is_dir():
            parser.error(f"Archive path exists but is not a directory: {archive_path}")
        elif not os.access(archive_path, os.W_OK):
            parser.error(f"Archive directory is not writable: {archive_path}")
    
    finder = DuplicateFinder(args)
    return finder.run(args.paths)


if __name__ == "__main__":
    sys.exit(main())