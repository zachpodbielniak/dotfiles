#!/usr/bin/python3

from os import environ
from subprocess import run
from sys import argv, exit, stdin, stderr
import argparse
import json
import requests
import sys
import os
import base64
import mimetypes
import time

ctr_id: str|None = ""
api_key: str|None = ""

if ("CONTAINER_ID" in environ):
    ctr_id = environ.get("CONTAINER_ID")

# if we are not in the 'dev' distrobox re-exec the script
# inside of the 'dev' distrobox
if ("dev" != ctr_id):
    cmd: list[str] = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *argv
    ]

    run(cmd)
    exit(0)

if ("OPENAI_API_KEY" in environ):
    api_key = environ.get("OPENAI_API_KEY")
    if ("" == api_key):
        print("OPENAI_API_KEY is empty")
        exit(1)
else:
    print("OPENAI_API_KEY is not set")
    exit(1)

# Parse arguments
parser = argparse.ArgumentParser(description="Query OpenAI API")
parser.add_argument("--prompt", help="Prompt to prepend to the input")
parser.add_argument("--model", default="gpt-4-turbo",
                    help="Model to use (default: gpt-4-turbo)")
parser.add_argument("--debug", action="store_true", help="Enable debug mode (shows request details)")
parser.add_argument("--json", action="store_true", help="Return a clean JSON response without streaming")
parser.add_argument("-S", "--no-streaming", action="store_true", help="Disable streaming mode for cleaner output capture")
parser.add_argument("--embedding", action="store_true", help="Generate an embedding vector instead of a text response")
parser.add_argument("-f", "--file", action="append", dest="files",
                    help="Include file content in the context (can be specified multiple times)")
parser.add_argument("-L", "--list-models", action="store_true",
                    help="List available models from the OpenAI API")
parser.add_argument("--no-color", action="store_true",
                    help="Disable colored output")
parser.add_argument("--image-gen", action="store_true",
                    help="Generate an image instead of text response")
parser.add_argument("--output", help="Output filename for image generation (default: auto-numbered output-NNNN.png)")
parser.add_argument("--image", action="append", dest="images",
                    help="Include image file(s) for vision analysis (can be specified multiple times)")
parser.add_argument("--summary", action="store_true",
                    help="Show usage summary with token counts, timing, and costs in a formatted table")
args = parser.parse_args()

def get_auto_numbered_filename(base_name="output", extension=".png"):
    """Generate an auto-numbered filename that doesn't exist yet."""
    counter = 0
    while True:
        filename = f"{base_name}-{counter:04d}{extension}"
        if not os.path.exists(filename):
            return filename
        counter += 1

def encode_image_to_base64(image_path):
    """Encode an image file to base64 with proper MIME type detection."""
    try:
        with open(image_path, 'rb') as image_file:
            image_data = image_file.read()
            encoded_string = base64.b64encode(image_data).decode('utf-8')
            
            # Get MIME type
            mime_type, _ = mimetypes.guess_type(image_path)
            if not mime_type or not mime_type.startswith('image/'):
                # Default to common image types
                if image_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp')):
                    ext = image_path.lower().split('.')[-1]
                    if ext == 'jpg':
                        ext = 'jpeg'
                    mime_type = f'image/{ext}'
                else:
                    mime_type = 'image/jpeg'  # fallback
            
            return f"data:{mime_type};base64,{encoded_string}"
    except Exception as e:
        print(f"Error reading image {image_path}: {e}", file=sys.stderr)
        exit(1)

def calculate_cost(model, input_tokens, output_tokens):
    """Calculate cost based on OpenAI pricing (as of 2024)."""
    # Check if this is an image generation model
    if model in ["dall-e-2", "dall-e-3"]:
        # Image generation pricing (per image, not per token)
        image_pricing = {
            "dall-e-2": 0.020,  # $0.020 per image (1024x1024)
            "dall-e-3": 0.040,  # $0.040 per image (1024x1024 standard)
        }
        total_cost = image_pricing.get(model, 0.040)  # Default to DALL-E 3 pricing
        return {
            "input_cost": 0.0,  # No token-based input cost for images
            "output_cost": total_cost,  # Treat image cost as "output"
            "total_cost": total_cost
        }
    
    # Pricing per 1M tokens (in dollars) for text models
    pricing = {
        # GPT-4 models
        "gpt-4-turbo": {"input": 10.00, "output": 30.00},
        "gpt-4-turbo-2024-04-09": {"input": 10.00, "output": 30.00},
        "gpt-4-turbo-preview": {"input": 10.00, "output": 30.00},
        "gpt-4": {"input": 30.00, "output": 60.00},
        "gpt-4-32k": {"input": 60.00, "output": 120.00},
        "gpt-4o": {"input": 5.00, "output": 15.00},
        "gpt-4o-2024-05-13": {"input": 5.00, "output": 15.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-4o-mini-2024-07-18": {"input": 0.15, "output": 0.60},
        # GPT-3.5 models
        "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
        "gpt-3.5-turbo-16k": {"input": 3.00, "output": 4.00},
        "gpt-3.5-turbo-0125": {"input": 0.50, "output": 1.50},
        "gpt-3.5-turbo-1106": {"input": 1.00, "output": 2.00},
        # Embedding models
        "text-embedding-3-small": {"input": 0.02, "output": 0},
        "text-embedding-3-large": {"input": 0.13, "output": 0},
        "text-embedding-ada-002": {"input": 0.10, "output": 0},
    }
    
    # Get pricing for the model, default to GPT-4 Turbo if not found
    model_pricing = pricing.get(model, pricing.get("gpt-4-turbo"))
    
    # Calculate costs (convert from per million to actual tokens)
    input_cost = (input_tokens / 1_000_000) * model_pricing["input"]
    output_cost = (output_tokens / 1_000_000) * model_pricing["output"]
    total_cost = input_cost + output_cost
    
    return {
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost
    }

def print_summary_table(model, input_tokens, output_tokens, total_time, first_token_time, cost_info, is_estimated=False, use_color=True):
    """Print a formatted summary table with color support."""
    # ANSI color codes (conditionally set based on use_color)
    if use_color:
        CYAN = '\033[96m'
        GREEN = '\033[92m'
        YELLOW = '\033[93m'
        BLUE = '\033[94m'
        MAGENTA = '\033[95m'
        BOLD = '\033[1m'
        END = '\033[0m'
    else:
        CYAN = GREEN = YELLOW = BLUE = MAGENTA = BOLD = END = ''
    
    # Calculate total tokens
    total_tokens = input_tokens + output_tokens
    
    # Table width (inner content width)
    width = 50
    
    # Helper to strip ANSI codes
    import re
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    
    def strip_ansi(text):
        return ansi_escape.sub('', text)
    
    # Helper to create a padded line
    def make_line(left, right, left_width=22):
        """Create a line with left and right parts, properly padded."""
        left_plain = strip_ansi(left)
        right_plain = strip_ansi(right)
        
        # Total inner content must be exactly 48 characters
        # Left part: left_text + padding to reach left_width
        left_part = left + ' ' * (left_width - len(left_plain))
        
        # Right part: right_text + padding to fill remaining space
        remaining_width = 48 - left_width - 1  # -1 for space between left and right
        right_part = right + ' ' * (remaining_width - len(right_plain))
        
        # Combine: left_part + space + right_part = exactly 48 chars
        content = f"{left_part} {right_part}"
        
        return f"{CYAN}║{END} {content} {CYAN}║{END}"
    
    # Build the table
    print(f"\n{BOLD}{CYAN}╔{'═' * width}╗{END}", file=sys.stderr)
    
    # Title
    title = "API Usage Summary"
    title_pad_left = (width - len(title)) // 2
    title_pad_right = width - len(title) - title_pad_left
    print(f"{BOLD}{CYAN}║{' ' * title_pad_left}{title}{' ' * title_pad_right}║{END}", file=sys.stderr)
    
    print(f"{BOLD}{CYAN}╠{'═' * width}╣{END}", file=sys.stderr)
    
    # Model info
    model_display = model if len(model) <= 30 else model[:27] + "..."
    print(make_line(f"{BOLD}Model:{END}", f"{GREEN}{model_display}{END}"), file=sys.stderr)
    print(f"{CYAN}╠{'─' * width}╣{END}", file=sys.stderr)
    
    # Token usage
    print(make_line(f"{BOLD}Token Usage:{END}", ""), file=sys.stderr)
    
    est_marker = " (est)" if is_estimated else ""
    print(make_line("  Input:", f"{YELLOW}{input_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Output:", f"{YELLOW}{output_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Total:", f"{BOLD}{YELLOW}{total_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(f"{CYAN}╠{'─' * width}╣{END}", file=sys.stderr)
    
    # Performance
    print(make_line(f"{BOLD}Performance:{END}", ""), file=sys.stderr)
    
    if first_token_time:
        print(make_line("  Time to first token:", f"{BLUE}{first_token_time:.2f}s{END}"), file=sys.stderr)
    
    print(make_line("  Total time:", f"{BLUE}{total_time:.2f}s{END}"), file=sys.stderr)
    
    if output_tokens > 0 and total_time > 0:
        tokens_per_sec = output_tokens / total_time
        print(make_line("  Tokens/second:", f"{BLUE}{tokens_per_sec:.1f}{END}"), file=sys.stderr)
    
    print(f"{CYAN}╠{'─' * width}╣{END}", file=sys.stderr)
    
    # Cost
    print(make_line(f"{BOLD}Cost Breakdown:{END}", ""), file=sys.stderr)
    
    print(make_line("  Input cost:", f"{MAGENTA}${cost_info['input_cost']:.6f}{END}"), file=sys.stderr)
    print(make_line("  Output cost:", f"{MAGENTA}${cost_info['output_cost']:.6f}{END}"), file=sys.stderr)
    print(make_line(f"  {BOLD}Total cost:{END}", f"{BOLD}{MAGENTA}${cost_info['total_cost']:.6f}{END}"), file=sys.stderr)
    
    print(f"{BOLD}{CYAN}╚{'═' * width}╝{END}", file=sys.stderr)

# Since outside of the distrobox we may not have these modules
# quietly ignore the fact that they may not exist
try:
    # Handle list-models option first
    if args.list_models:
        # Check if colors should be disabled
        use_colors = not args.no_color and environ.get("NO_COLOR", "").lower() not in ("1", "true")
        
        # ANSI color codes (conditionally set based on use_colors)
        class Colors:
            HEADER = '\033[95m' if use_colors else ''
            BLUE = '\033[94m' if use_colors else ''
            CYAN = '\033[96m' if use_colors else ''
            GREEN = '\033[92m' if use_colors else ''
            YELLOW = '\033[93m' if use_colors else ''
            RED = '\033[91m' if use_colors else ''
            BOLD = '\033[1m' if use_colors else ''
            UNDERLINE = '\033[4m' if use_colors else ''
            END = '\033[0m' if use_colors else ''
        
        # Query OpenAI API for available models
        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            response = requests.get("https://api.openai.com/v1/models", headers=headers)
            
            if response.status_code != 200:
                print(f"Error querying models: {response.status_code}")
                print(response.text)
                exit(1)
            
            models_data = response.json()
            
            # Extract and sort models by ID
            models = []
            for model in models_data.get("data", []):
                model_id = model.get("id", "")
                # Skip fine-tuned models (they usually have a colon in the ID)
                if ":" not in model_id:
                    # Categorize models for better descriptions
                    if "gpt-4" in model_id:
                        if "turbo" in model_id:
                            desc = "GPT-4 Turbo model"
                        elif "gpt-4o" in model_id:
                            if "mini" in model_id:
                                desc = "Affordable and intelligent small model"
                            else:
                                desc = "Multimodal flagship model"
                        else:
                            desc = "GPT-4 model"
                    elif "gpt-3.5" in model_id:
                        if "16k" in model_id:
                            desc = "Extended context version"
                        else:
                            desc = "Fast and efficient model"
                    elif "embedding" in model_id:
                        if "large" in model_id:
                            desc = "Most capable embedding model"
                        elif "small" in model_id:
                            desc = "Efficient embedding model"
                        else:
                            desc = "Embedding model"
                    elif "dall-e" in model_id:
                        if "3" in model_id:
                            desc = "Most advanced image generation"
                        else:
                            desc = "Image generation model"
                    elif "whisper" in model_id:
                        desc = "Speech recognition model"
                    elif "tts" in model_id:
                        if "hd" in model_id:
                            desc = "High quality text-to-speech"
                        else:
                            desc = "Text-to-speech model"
                    else:
                        desc = "AI model"
                    
                    models.append((model_id, model_id, desc))
            
            # Sort models by ID
            models.sort(key=lambda x: x[0])
            
            if not models:
                print(f"{Colors.YELLOW}No models found. Check your API key permissions.{Colors.END}")
                exit(0)
            
            # Calculate column widths
            max_model = max(len(model[0]) for model in models)
            max_name = max(len(model[1]) for model in models)
            max_desc = max(len(model[2]) for model in models)
            
            # Ensure minimum widths
            max_model = max(max_model, len("Model ID"))
            max_name = max(max_name, len("Name"))
            max_desc = max(max_desc, len("Description"))
            
            # Print header
            print(f"\n{Colors.BOLD}{Colors.CYAN}Available OpenAI Models:{Colors.END}")
            print(f"{Colors.BOLD}{'─' * (max_model + max_name + max_desc + 6)}{Colors.END}")
            
            # Print table header
            print(f"{Colors.BOLD}{Colors.HEADER}{'Model ID':<{max_model}} {'Name':<{max_name}} {'Description':<{max_desc}}{Colors.END}")
            print(f"{Colors.BOLD}{'─' * max_model} {'─' * max_name} {'─' * max_desc}{Colors.END}")
            
            # Print table rows
            for i, (model_id, name, description) in enumerate(models):
                # Alternate row colors
                color = Colors.CYAN if i % 2 == 0 else Colors.BLUE
                name_color = Colors.GREEN
                desc_color = Colors.YELLOW
                
                print(f"{color}{model_id:<{max_model}}{Colors.END} "
                      f"{name_color}{name:<{max_name}}{Colors.END} "
                      f"{desc_color}{description:<{max_desc}}{Colors.END}")
            
            print(f"{Colors.BOLD}{'─' * (max_model + max_name + max_desc + 6)}{Colors.END}")
            print(f"{Colors.BOLD}Total models: {Colors.GREEN}{len(models)}{Colors.END}\n")
            
        except Exception as e:
            print(f"{Colors.RED}Error querying models: {e}{Colors.END}")
            print(f"{Colors.YELLOW}Falling back to known models list...{Colors.END}")
            
            # Fallback to static list if API fails
            models = [
                ("gpt-4-turbo", "GPT-4 Turbo", "Most capable GPT-4 model"),
                ("gpt-4o", "GPT-4o", "Multimodal flagship model"),
                ("gpt-4o-mini", "GPT-4o Mini", "Affordable and intelligent small model"),
                ("gpt-4", "GPT-4", "Original GPT-4 model"),
                ("gpt-3.5-turbo", "GPT-3.5 Turbo", "Fast and efficient model"),
                ("dall-e-3", "DALL-E 3", "Most advanced image generation"),
                ("whisper-1", "Whisper v1", "Speech recognition model"),
            ]
            
            # Calculate and display as before
            max_model = max(len(model[0]) for model in models)
            max_name = max(len(model[1]) for model in models)
            max_desc = max(len(model[2]) for model in models)
            
            max_model = max(max_model, len("Model ID"))
            max_name = max(max_name, len("Name"))
            max_desc = max(max_desc, len("Description"))
            
            print(f"\n{Colors.BOLD}{Colors.CYAN}Known OpenAI Models:{Colors.END}")
            print(f"{Colors.BOLD}{'─' * (max_model + max_name + max_desc + 6)}{Colors.END}")
            
            print(f"{Colors.BOLD}{Colors.HEADER}{'Model ID':<{max_model}} {'Name':<{max_name}} {'Description':<{max_desc}}{Colors.END}")
            print(f"{Colors.BOLD}{'─' * max_model} {'─' * max_name} {'─' * max_desc}{Colors.END}")
            
            for i, (model_id, name, description) in enumerate(models):
                color = Colors.CYAN if i % 2 == 0 else Colors.BLUE
                name_color = Colors.GREEN
                desc_color = Colors.YELLOW
                
                print(f"{color}{model_id:<{max_model}}{Colors.END} "
                      f"{name_color}{name:<{max_name}}{Colors.END} "
                      f"{desc_color}{description:<{max_desc}}{Colors.END}")
            
            print(f"{Colors.BOLD}{'─' * (max_model + max_name + max_desc + 6)}{Colors.END}")
            print(f"{Colors.BOLD}Total models: {Colors.GREEN}{len(models)}{Colors.END}\n")
        
        exit(0)
    
    # Read file contents if any files were specified
    file_contents = []
    if args.files:
        for file_path in args.files:
            try:
                with open(file_path, 'r') as f:
                    file_content = f.read()
                    file_contents.append(f"=== File: {file_path} ===\n{file_content}\n")
            except IOError as e:
                print(f"Warning: Could not read file {file_path}: {e}", file=stderr)
    
    # Read from standard input
    query = stdin.read()

    # Combine file contents with query
    combined_parts = []
    
    # Add file contents first if any
    if file_contents:
        combined_parts.extend(file_contents)
    
    # Add prompt if provided
    if args.prompt:
        combined_parts.append(args.prompt)
    
    # Add stdin content if any
    if query:
        combined_parts.append(query)
    
    # Combine all parts
    if combined_parts:
        query = "\n".join(combined_parts)
    else:
        query = ""

    # Set up API call
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Set the API endpoint based on the request type
    if args.embedding:
        # Use the embeddings API endpoint for OpenAI
        url = "https://api.openai.com/v1/embeddings"
        data = {
            "model": args.model if "embedding" in args.model else "text-embedding-3-small",
            "input": query
        }
    elif args.image_gen:
        # Use the image generation API endpoint for OpenAI
        url = "https://api.openai.com/v1/images/generations"
        image_model = args.model if args.model in ["dall-e-2", "dall-e-3"] else "dall-e-3"
        data = {
            "model": image_model,
            "prompt": query,
            "n": 1,
            "size": "1024x1024",
            "response_format": "b64_json"
        }
    else:
        # Use the chat completions API endpoint for regular requests
        url = "https://api.openai.com/v1/chat/completions"
        
        # Construct message content (text + images if provided)
        message_content = []
        
        # Add text content
        if query:
            message_content.append({
                "type": "text",
                "text": query
            })
        
        # Add image content if provided
        if args.images:
            for image_path in args.images:
                if not os.path.exists(image_path):
                    print(f"Error: Image file not found: {image_path}", file=sys.stderr)
                    exit(1)
                
                image_url = encode_image_to_base64(image_path)
                message_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": image_url
                    }
                })
        
        # If no content was added, add empty text
        if not message_content:
            message_content.append({
                "type": "text",
                "text": ""
            })
        
        data = {
            "model": args.model,
            "max_tokens": 16384,
            "messages": [
                {
                    "role": "user",
                    "content": message_content
                }
            ],
            # Don't stream if JSON output is requested or --no-streaming
            "stream": not (args.json or args.no_streaming)
        }

    # Print debug info if requested
    if args.debug:
        print(f"Debug: API URL: {url}", file=sys.stderr)
        print(f"Debug: Headers (API key partially hidden): {{'Authorization': 'Bearer {api_key[:5]}...{api_key[-4:]}', 'Content-Type': 'application/json'}}", file=sys.stderr)
        print(f"Debug: Data: {json.dumps(data)}", file=sys.stderr)
        if args.image_gen:
            output_file = args.output if args.output else get_auto_numbered_filename()
            print(f"Debug: Image will be saved to: {output_file}", file=sys.stderr)
        if args.images:
            print(f"Debug: Processing {len(args.images)} image(s): {', '.join(args.images)}", file=sys.stderr)

    # Track timing if summary mode
    start_time = time.time() if args.summary else None
    first_token_time = None
    
    # Send request to OpenAI API
    response = requests.post(url, headers=headers, json=data, stream=data.get("stream", False))

    if response.status_code != 200:
        print(f"Error: {response.status_code}")
        response_data = response.json() if response.headers.get('content-type', '').startswith('application/json') else {}
        
        # Provide more helpful error messages for common issues
        if response.status_code == 400 and args.image_gen:
            error_info = response_data.get("error", {})
            error_type = error_info.get("type", "")
            
            if error_type == "image_generation_user_error":
                print("Image generation failed - this is usually due to:")
                print("  • Content policy violation (violence, adult content, etc.)")
                print("  • Prompt safety filters")
                print("  • Try rephrasing your prompt to be more neutral/safe")
            else:
                print(f"Image generation error: {error_info.get('message', 'Unknown error')}")
        else:
            print(response.text)
        exit(1)
    
    # Handle the response based on request type
    if args.embedding:
        # Process embeddings response (never streamed)
        response_data = response.json()
        if "data" in response_data and len(response_data["data"]) > 0:
            # Extract and output just the embedding vector in JSON format
            embedding = response_data["data"][0]["embedding"]
            print(json.dumps({"embedding": embedding}))
            
            # Summary output for embeddings
            if args.summary and "usage" in response_data:
                end_time = time.time()
                usage = response_data["usage"]
                total_time = end_time - start_time
                
                # Add newline before summary
                print(file=sys.stderr)
                
                # Calculate cost
                cost_info = calculate_cost(
                    args.model if "embedding" in args.model else "text-embedding-3-small",
                    usage.get('prompt_tokens', 0),
                    0  # Embeddings have no output tokens
                )
                
                # Print summary table
                print_summary_table(
                    model=args.model if 'embedding' in args.model else 'text-embedding-3-small',
                    input_tokens=usage.get('prompt_tokens', 0),
                    output_tokens=0,
                    total_time=total_time,
                    first_token_time=None,
                    cost_info=cost_info,
                    use_color=not args.no_color
                )
        else:
            print(json.dumps(response_data))
    elif args.image_gen:
        # Process image generation response
        response_data = response.json()
        if "data" in response_data and len(response_data["data"]) > 0:
            # Get the base64 image data
            image_data = response_data["data"][0]["b64_json"]
            
            # Determine output filename
            if args.output:
                output_filename = args.output
            else:
                output_filename = get_auto_numbered_filename()
            
            # Decode and save the image
            try:
                image_bytes = base64.b64decode(image_data)
                with open(output_filename, 'wb') as f:
                    f.write(image_bytes)
                print(f"Image saved to: {output_filename}")
                
                # Summary output for image generation
                if args.summary:
                    end_time = time.time()
                    total_time = end_time - start_time
                    
                    # Add newline before summary
                    print(file=sys.stderr)
                    
                    # Image generation typically doesn't provide token counts
                    # Estimate input tokens from prompt length
                    estimated_input_tokens = len(query) // 4
                    estimated_output_tokens = 0  # Images don't have text output tokens
                    
                    # Calculate cost for image generation
                    cost_info = calculate_cost(
                        args.model,  # Use the actual model for pricing
                        estimated_input_tokens,
                        estimated_output_tokens
                    )
                    
                    # Print summary table
                    print_summary_table(
                        model=args.model,
                        input_tokens=estimated_input_tokens,
                        output_tokens=estimated_output_tokens,
                        total_time=total_time,
                        first_token_time=None,
                        cost_info=cost_info,
                        is_estimated=True,
                        use_color=not args.no_color
                    )
            except Exception as e:
                print(f"Error saving image: {e}", file=sys.stderr)
                exit(1)
        else:
            print(f"Error: No image data in response: {json.dumps(response_data)}", file=sys.stderr)
            exit(1)
    elif args.json or args.no_streaming:
        # Process standard JSON response (not streamed)
        response_data = response.json()
        # For chat completions, extract the message content
        if "choices" in response_data and len(response_data["choices"]) > 0:
            message = response_data["choices"][0]["message"]
            print(message["content"])
            
            # Summary output
            if args.summary and "usage" in response_data:
                end_time = time.time()
                usage = response_data["usage"]
                total_time = end_time - start_time
                
                # Add newline before summary
                print(file=sys.stderr)
                
                # Calculate cost
                cost_info = calculate_cost(
                    args.model,
                    usage.get('prompt_tokens', 0),
                    usage.get('completion_tokens', 0)
                )
                
                # Print summary table
                print_summary_table(
                    model=args.model,
                    input_tokens=usage.get('prompt_tokens', 0),
                    output_tokens=usage.get('completion_tokens', 0),
                    total_time=total_time,
                    first_token_time=None,
                    cost_info=cost_info,
                    use_color=not args.no_color
                )
        else:
            # Just print the raw response as fallback
            if args.json:
                print(json.dumps(response_data))
            else:
                print(f"Unexpected response format: {json.dumps(response_data)}", file=sys.stderr)
    else:
        # Process streaming response
        accumulated_content = ""
        token_count = 0
        input_tokens = None
        output_tokens = None
        
        for line in response.iter_lines():
            if line:
                # Skip the initial "data: " prefix
                line_text = line.decode('utf-8')
                if line_text.startswith("data: "):
                    line_json = line_text[6:]
                    
                    # The last message is usually "data: [DONE]"
                    if line_json == "[DONE]":
                        break
                        
                    try:
                        event = json.loads(line_json)
                        # Extract content delta if it exists
                        if "choices" in event and len(event["choices"]) > 0:
                            choice = event["choices"][0]
                            if "delta" in choice and "content" in choice["delta"]:
                                content = choice["delta"]["content"]
                                print(content, end="", flush=True)
                                
                                # Track first token time
                                if args.summary and first_token_time is None and content:
                                    first_token_time = time.time()
                                
                                accumulated_content += content
                                token_count += 1
                        
                        # Some models provide usage in streaming
                        if args.summary and "usage" in event:
                            usage = event["usage"]
                            input_tokens = usage.get("prompt_tokens", input_tokens)
                            output_tokens = usage.get("completion_tokens", output_tokens)
                    except json.JSONDecodeError:
                        continue
        
        # Summary output for streaming
        if args.summary:
            end_time = time.time()
            total_time = end_time - start_time
            
            # Add newline before summary
            print(file=sys.stderr)
            
            # For streaming, we might not get exact token counts from the API
            # We can estimate based on the response
            is_estimated = False
            if input_tokens is None or output_tokens is None:
                # Rough estimation: ~4 characters per token
                estimated_output_tokens = len(accumulated_content) // 4
                estimated_input_tokens = len(query) // 4
                input_tokens = input_tokens or estimated_input_tokens
                output_tokens = output_tokens or estimated_output_tokens
                is_estimated = True
            
            # Calculate cost
            cost_info = calculate_cost(
                args.model,
                input_tokens,
                output_tokens
            )
            
            # Calculate time to first token
            ttft = (first_token_time - start_time) if first_token_time else None
            
            # Print summary table
            print_summary_table(
                model=args.model,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                total_time=total_time,
                first_token_time=ttft,
                cost_info=cost_info,
                is_estimated=is_estimated,
                use_color=not args.no_color
            )

except (ImportError, Exception) as e:
    print(f"Error: {e}")
    exit(1)