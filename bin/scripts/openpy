#!/usr/bin/python3

# dotfiles - Personal configuration files and scripts
# Copyright (C) 2025  Zach Podbielniak
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.


from os import environ
from subprocess import run
from sys import argv, exit, stdin, stderr
import argparse
import json
import requests
import sys
import os
import base64
import mimetypes
import time
import select
from datetime import datetime
from pathlib import Path

ctr_id: str|None = ""
api_key: str|None = ""

if ("CONTAINER_ID" in environ):
    ctr_id = environ.get("CONTAINER_ID")

# Check if distrobox check should be skipped
no_dbox_check = environ.get("NO_DBOX_CHECK", "").lower() in ("1", "true")

# if we are not in the 'dev' distrobox re-exec the script
# inside of the 'dev' distrobox
if not no_dbox_check and ("dev" != ctr_id):
    cmd: list[str] = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *argv
    ]

    run(cmd)
    exit(0)

if ("OPENAI_API_KEY" in environ):
    api_key = environ.get("OPENAI_API_KEY")
    if ("" == api_key):
        print("OPENAI_API_KEY is empty")
        exit(1)
else:
    print("OPENAI_API_KEY is not set")
    exit(1)

# Parse arguments
parser = argparse.ArgumentParser(description="Query OpenAI API")
parser.add_argument("--prompt", help="Prompt to prepend to the input")
parser.add_argument("--model", default="gpt-5",
                    help="Model to use (default: gpt-5)")
parser.add_argument("--debug", action="store_true", help="Enable debug mode (shows request details)")
parser.add_argument("--json", action="store_true", help="Return a clean JSON response without streaming")
parser.add_argument("-S", "--no-streaming", action="store_true", help="Disable streaming mode for cleaner output capture")
parser.add_argument("--embedding", action="store_true", help="Generate an embedding vector instead of a text response")
parser.add_argument("-f", "--file", action="append", dest="files",
                    help="Include file content in the context (can be specified multiple times)")
parser.add_argument("-L", "--list-models", action="store_true",
                    help="List available models from the OpenAI API")
parser.add_argument("--no-color", action="store_true",
                    help="Disable colored output")
parser.add_argument("--image-gen", action="store_true",
                    help="Generate an image instead of text response")
parser.add_argument("--regen", action="store_true",
                    help="Regenerate/enhance an image provided via --image (uses GPT-4 vision + DALL-E)")
parser.add_argument("--output", help="Output filename for image generation (default: auto-numbered output-NNNN.png)")
parser.add_argument("--hd", action="store_true",
                    help="Use HD quality for DALL-E-3 image generation (creates images with finer details, higher cost)")
parser.add_argument("--size", default="1024x1024",
                    help="Image size (DALL-E-3: 1024x1024, 1024x1792, 1792x1024; DALL-E-2: 256x256, 512x512, 1024x1024)")
parser.add_argument("--image", action="append", dest="images",
                    help="Include image file(s) for vision analysis (can be specified multiple times)")
parser.add_argument("--summary", action="store_true",
                    help="Show usage summary with token counts, timing, and costs in a formatted table")
parser.add_argument("--dry-run", action="store_true",
                    help="Show what would be sent to the API without actually making the request")
parser.add_argument("--no-preserve", action="store_true",
                    help="Don't save the chat transaction to the second-brain markdown file")
parser.add_argument("--use-context", action="store_true",
                    help="Include today's chat history as context for the conversation")
parser.add_argument("--use-context-from", 
                    help="Include chat history from a specific date (YYYY-MM-DD format) as context")
parser.add_argument("--personality", 
                    help="Set a personality/role for the AI. Available: cfp, swe, teacher, doctor, lawyer, chef, therapist, scientist, historian, artist, coach, journalist, philosopher, librarian, tutor, consultant, translator, critic, comedian, mentor, investor, cybersecurity. You can also use custom descriptions.")
parser.add_argument("--emojis", nargs="?", const="light", choices=["light", "heavy"],
                    help="Encourage emoji usage in responses. 'light' (default) uses emojis sparingly, 'heavy' uses them frequently.")
parser.add_argument("--enhance-emojis", action="store_true", 
                    help="Post-process the response to add appropriate emojis using a secondary AI call.")
parser.add_argument("--show-limits", action="store_true",
                    help="Display token limits for all OpenAI models")
args = parser.parse_args()

# Personality definitions
PERSONALITIES = {
    "cfp": "You are a Certified Financial Planner (CFP) with extensive knowledge of personal finance, investment strategies, tax planning, retirement planning, and estate planning. Provide professional, ethical financial advice while always reminding users to consult with their own financial advisor for personalized guidance.",
    
    "swe": "You are a Senior Software Engineer with deep expertise in software architecture, design patterns, code quality, and best practices. Focus on writing clean, maintainable, and efficient code. Consider scalability, security, and performance in your recommendations.",
    
    "teacher": "You are a patient and encouraging teacher who excels at breaking down complex topics into understandable concepts. Use analogies, examples, and step-by-step explanations. Adapt your teaching style to the learner's level and encourage questions.",
    
    "doctor": "You are a knowledgeable medical professional who provides general health information and guidance. Always emphasize that your advice is educational only and users should consult with healthcare providers for medical diagnosis and treatment.",
    
    "lawyer": "You are a legal expert who provides general legal information and explains legal concepts clearly. Always clarify that this is not legal advice and users should consult with an attorney for specific legal matters.",
    
    "chef": "You are a professional chef with expertise in various cuisines, cooking techniques, and food science. Share recipes, cooking tips, and culinary knowledge with enthusiasm. Consider dietary restrictions and preferences when making suggestions.",
    
    "therapist": "You are a supportive mental health professional who provides emotional support and coping strategies. Always encourage users to seek professional help for serious mental health concerns while offering compassionate guidance.",
    
    "scientist": "You are a research scientist with broad knowledge across multiple scientific disciplines. Explain scientific concepts clearly, cite evidence-based information, and maintain scientific accuracy while making complex topics accessible.",
    
    "historian": "You are a historian with deep knowledge of world history, cultures, and historical analysis. Provide context, multiple perspectives, and help users understand how past events connect to the present.",
    
    "artist": "You are a creative artist with expertise in various art forms, techniques, and art history. Encourage creativity, provide constructive feedback, and help users explore their artistic expression.",
    
    "coach": "You are a life coach focused on helping people achieve their goals, overcome obstacles, and maximize their potential. Use motivational techniques, ask powerful questions, and help users create actionable plans.",
    
    "journalist": "You are an investigative journalist skilled in research, fact-checking, and clear communication. Help users understand complex issues, identify reliable sources, and think critically about information.",
    
    "philosopher": "You are a philosopher who explores deep questions about existence, ethics, knowledge, and reality. Engage in thoughtful dialogue, present multiple philosophical perspectives, and encourage critical thinking.",
    
    "librarian": "You are a research librarian with expertise in finding, evaluating, and organizing information. Help users with research strategies, source evaluation, and information literacy.",
    
    "tutor": "You are a subject-specific tutor who helps students understand difficult concepts, complete assignments, and prepare for exams. Focus on building understanding rather than just providing answers.",
    
    "consultant": "You are a business consultant with expertise in strategy, operations, and organizational development. Provide practical business advice, analyze problems systematically, and offer actionable recommendations.",
    
    "translator": "You are a professional translator and linguist with expertise in multiple languages and cultures. Help with translations, explain linguistic nuances, and provide cultural context.",
    
    "critic": "You are a thoughtful critic who analyzes literature, films, art, and media with depth and insight. Provide balanced critiques that consider both strengths and weaknesses while respecting different perspectives.",
    
    "comedian": "You are a witty comedian who uses humor appropriately to lighten conversations while remaining helpful. Balance entertainment with usefulness, and be sensitive to context and audience.",
    
    "mentor": "You are a wise mentor who guides others based on experience and wisdom. Offer perspective, share relevant experiences, and help mentees navigate challenges while encouraging their growth.",
    
    "investor": "You are an experienced investment professional with deep knowledge of financial markets, portfolio management, risk assessment, and investment strategies. Provide insights on stocks, bonds, real estate, alternative investments, and market analysis while emphasizing the importance of diversification and risk management.",
    
    "cybersecurity": "You are a cybersecurity expert with extensive knowledge of information security, threat analysis, network security, and digital privacy. Help users understand security best practices, identify vulnerabilities, and implement protective measures while staying current with emerging threats and security technologies."
}

def get_model_limits(model_name):
    """Get input context window and output token limits for OpenAI models."""
    # Model limits based on current OpenAI specifications (December 2025)
    limits = {
        # GPT-5 Series (Latest - December 2025)
        "gpt-5": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "November 2024"},
        "gpt-5-thinking": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "November 2024"},
        "gpt-5-thinking-pro": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "November 2024"},
        "gpt-5-codex": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "November 2024"},

        # Reasoning Models (New December 2025)
        "o4-mini": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "November 2024"},
        "o3-mini": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "November 2024"},

        # Open-weight Models (New December 2025)
        "gpt-oss-120b": {"context_window": 4096, "max_output": 2048, "knowledge_cutoff": "November 2024"},
        "gpt-oss-20b": {"context_window": 4096, "max_output": 2048, "knowledge_cutoff": "November 2024"},

        # GPT-4.1 Series (Current generation)
        "gpt-4.1": {"context_window": 1000000, "max_output": 32768, "knowledge_cutoff": "October 2024"},
        "gpt-4.1-nano": {"context_window": 1000000, "max_output": 32768, "knowledge_cutoff": "October 2024"},

        # GPT-4o Series (Multimodal flagship)
        "gpt-4o": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "October 2024"},
        "gpt-4o-2024-05-13": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "October 2023"},
        "gpt-4o-2024-08-06": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "October 2023"},
        "gpt-4o-2024-11-20": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "October 2024"},
        "gpt-4o-mini": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "October 2024"},
        "gpt-4o-mini-2024-07-18": {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "October 2023"},

        # GPT-4 Turbo Series (Legacy)
        "gpt-4-turbo": {"context_window": 128000, "max_output": 4096, "knowledge_cutoff": "December 2023"},
        "gpt-4-turbo-2024-04-09": {"context_window": 128000, "max_output": 4096, "knowledge_cutoff": "December 2023"},
        "gpt-4-turbo-preview": {"context_window": 128000, "max_output": 4096, "knowledge_cutoff": "December 2023"},
        "gpt-4-0125-preview": {"context_window": 128000, "max_output": 4096, "knowledge_cutoff": "December 2023"},
        "gpt-4-1106-preview": {"context_window": 128000, "max_output": 4096, "knowledge_cutoff": "April 2023"},

        # GPT-4 Original Series (Legacy)
        "gpt-4": {"context_window": 8192, "max_output": 4096, "knowledge_cutoff": "September 2021"},
        "gpt-4-0613": {"context_window": 8192, "max_output": 4096, "knowledge_cutoff": "September 2021"},
        "gpt-4-32k": {"context_window": 32768, "max_output": 4096, "knowledge_cutoff": "September 2021"},
        "gpt-4-32k-0613": {"context_window": 32768, "max_output": 4096, "knowledge_cutoff": "September 2021"},

        # GPT-3.5 Turbo Series (Legacy)
        "gpt-3.5-turbo": {"context_window": 16385, "max_output": 4096, "knowledge_cutoff": "September 2021"},
        "gpt-3.5-turbo-0125": {"context_window": 16385, "max_output": 4096, "knowledge_cutoff": "September 2021"},
        "gpt-3.5-turbo-1106": {"context_window": 16385, "max_output": 4096, "knowledge_cutoff": "September 2021"},
        "gpt-3.5-turbo-16k": {"context_window": 16385, "max_output": 4096, "knowledge_cutoff": "September 2021"},
        "gpt-3.5-turbo-instruct": {"context_window": 4096, "max_output": 4096, "knowledge_cutoff": "September 2021"},

        # Embedding Models (no output tokens, only input processing)
        "text-embedding-3-small": {"context_window": 8191, "max_output": 0, "knowledge_cutoff": "September 2021"},
        "text-embedding-3-large": {"context_window": 8191, "max_output": 0, "knowledge_cutoff": "September 2021"},
        "text-embedding-ada-002": {"context_window": 8191, "max_output": 0, "knowledge_cutoff": "September 2021"},

        # Image Generation Models (different token model - primarily prompt-based)
        "dall-e-2": {"context_window": 1000, "max_output": 0, "knowledge_cutoff": "N/A"},
        "dall-e-3": {"context_window": 4000, "max_output": 0, "knowledge_cutoff": "N/A"},

        # Audio Models
        "whisper-1": {"context_window": 0, "max_output": 0, "knowledge_cutoff": "N/A"},
        "tts-1": {"context_window": 4096, "max_output": 0, "knowledge_cutoff": "N/A"},
        "tts-1-hd": {"context_window": 4096, "max_output": 0, "knowledge_cutoff": "N/A"},
    }

    # Return the limits for the specified model, or default to GPT-5 if not found
    return limits.get(model_name, limits.get("gpt-5", {"context_window": 128000, "max_output": 16384, "knowledge_cutoff": "November 2024"}))

def estimate_tokens(text):
    """Rough estimation of token count (~4 characters per token)."""
    if not text:
        return 0
    return len(str(text)) // 4

def check_context_limits(model_name, estimated_input_tokens, debug=False):
    """Check if estimated input tokens exceed model's context window."""
    limits = get_model_limits(model_name)
    context_limit = limits["context_window"]
    
    if estimated_input_tokens > context_limit:
        print(f"Warning: Estimated input tokens ({estimated_input_tokens:,}) exceed {model_name}'s context window ({context_limit:,} tokens)", file=sys.stderr)
        print(f"This request may fail or be truncated. Consider reducing input length.", file=sys.stderr)
        return False
    elif estimated_input_tokens > (context_limit * 0.8):  # Warn at 80% capacity
        if debug:
            print(f"Debug: Input tokens ({estimated_input_tokens:,}) approaching {model_name}'s context limit ({context_limit:,} tokens)", file=sys.stderr)
    
    return True

def display_model_limits():
    """Display token limits for all OpenAI models."""
    # Check if colors should be disabled
    use_colors = not args.no_color and environ.get("NO_COLOR", "").lower() not in ("1", "true")
    
    # ANSI color codes (conditionally set based on use_colors)
    class Colors:
        HEADER = '\033[95m' if use_colors else ''
        BLUE = '\033[94m' if use_colors else ''
        CYAN = '\033[96m' if use_colors else ''
        GREEN = '\033[92m' if use_colors else ''
        YELLOW = '\033[93m' if use_colors else ''
        RED = '\033[91m' if use_colors else ''
        BOLD = '\033[1m' if use_colors else ''
        UNDERLINE = '\033[4m' if use_colors else ''
        END = '\033[0m' if use_colors else ''
    
    # Get model limits
    gpt4_1_models = ["gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"]
    gpt4o_models = ["gpt-4o", "gpt-4o-2024-05-13", "gpt-4o-2024-08-06", "gpt-4o-2024-11-20", "gpt-4o-mini", "gpt-4o-mini-2024-07-18"]
    gpt4_turbo_models = ["gpt-4-turbo", "gpt-4-turbo-2024-04-09", "gpt-4-turbo-preview", "gpt-4-0125-preview", "gpt-4-1106-preview"]
    gpt4_models = ["gpt-4", "gpt-4-0613", "gpt-4-32k", "gpt-4-32k-0613"]
    gpt35_models = ["gpt-3.5-turbo", "gpt-3.5-turbo-0125", "gpt-3.5-turbo-1106", "gpt-3.5-turbo-16k", "gpt-3.5-turbo-instruct"]
    embedding_models = ["text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"]
    
    def format_number(num):
        """Format large numbers with K/M suffix."""
        if num >= 1000000:
            return f"{num/1000000:.1f}M"
        elif num >= 1000:
            return f"{num/1000:.0f}K"
        else:
            return str(num)
    
    def display_model_group(title, models):
        print(f"\n{Colors.BOLD}{Colors.CYAN}{title}{Colors.END}")
        print(f"{Colors.BOLD}{'‚îÄ' * 80}{Colors.END}")
        print(f"{Colors.BOLD}{Colors.HEADER}{'Model':<35} {'Context Window':<15} {'Max Output':<12} {'Knowledge Cutoff':<15}{Colors.END}")
        print(f"{Colors.BOLD}{'‚îÄ' * 35} {'‚îÄ' * 15} {'‚îÄ' * 12} {'‚îÄ' * 15}{Colors.END}")
        
        for model in models:
            limits = get_model_limits(model)
            context_formatted = format_number(limits["context_window"])
            output_formatted = format_number(limits["max_output"]) if limits["max_output"] > 0 else "N/A"
            
            print(f"{Colors.GREEN}{model:<35}{Colors.END} "
                  f"{Colors.YELLOW}{context_formatted:<15}{Colors.END} "
                  f"{Colors.BLUE}{output_formatted:<12}{Colors.END} "
                  f"{Colors.CYAN}{limits['knowledge_cutoff']:<15}{Colors.END}")
    
    print(f"\n{Colors.BOLD}{Colors.CYAN}OpenAI Model Token Limits{Colors.END}")
    print(f"{Colors.BOLD}{'‚ïê' * 82}{Colors.END}")
    
    display_model_group("GPT-4.1 Series (Latest - 1M Context Window)", gpt4_1_models)
    display_model_group("GPT-4o Series (Multimodal Flagship)", gpt4o_models)
    display_model_group("GPT-4 Turbo Series", gpt4_turbo_models)
    display_model_group("GPT-4 Original Series", gpt4_models)
    display_model_group("GPT-3.5 Turbo Series", gpt35_models)
    display_model_group("Embedding Models", embedding_models)
    
    print(f"\n{Colors.BOLD}{Colors.YELLOW}Notes:{Colors.END}")
    print(f"‚Ä¢ Context Window: Maximum total tokens (input + output combined)")
    print(f"‚Ä¢ Max Output: Maximum tokens in a single response")
    print(f"‚Ä¢ Image generation and audio models have different token models")
    print(f"‚Ä¢ All limits are subject to OpenAI's current API specifications")
    print(f"‚Ä¢ Use --debug to see per-request limit information\n")

def get_personality_prompt(personality_key):
    """Get the personality prompt for the given key, or return empty string if not found."""
    if not personality_key:
        return ""
    
    personality_key = personality_key.lower()
    if personality_key in PERSONALITIES:
        return PERSONALITIES[personality_key] + "\n\n"
    else:
        # If not a predefined personality, treat it as a custom personality description
        return f"You are {personality_key}.\n\n"

def get_emoji_prompt(emoji_mode):
    """Get the emoji prompt for the given mode."""
    if not emoji_mode:
        return ""
    
    emoji_prompts = {
        "light": "Feel free to use relevant emojis to enhance your response where appropriate.",
        "heavy": "Use emojis frequently throughout your response to make it more engaging and expressive. Include emojis to highlight key points, convey emotions, and make the content more visually appealing."
    }
    
    return emoji_prompts.get(emoji_mode, emoji_prompts["light"]) + "\n\n"

def enhance_with_emojis(text, model="gpt-4o-mini"):
    """Enhance text with emojis using a secondary AI call."""
    if not text or not text.strip():
        return text
    
    enhancement_prompt = f"""Please add appropriate emojis to the following text to make it more engaging and expressive. Follow these guidelines:

1. Add emojis that are relevant and enhance understanding
2. Don't overdo it - use emojis strategically
3. Maintain the original meaning and tone
4. Place emojis at the BEGINNING of section headings, titles, and key points (e.g., "üöÄ Getting Started" not "Getting Started üöÄ")
5. For bullet points and lists, place emojis at the start of each item
6. Use emojis to emphasize important concepts, but place them before the text they're emphasizing
7. Return only the enhanced text with emojis, no additional commentary

Original text:
{text}"""

    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "max_tokens": len(text.split()) * 2 + 1000,  # Generous token limit
            "messages": [
                {
                    "role": "user",
                    "content": enhancement_prompt
                }
            ]
        }
        
        response = requests.post("https://api.openai.com/v1/chat/completions", 
                               headers=headers, 
                               json=data, 
                               timeout=30)
        
        if response.status_code == 200:
            response_data = response.json()
            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"].strip()
        
        # Return original text if enhancement fails
        return text
        
    except Exception as e:
        print(f"Warning: Emoji enhancement failed ({e}), returning original text", file=sys.stderr)
        return text

def get_auto_numbered_filename(base_name="output", extension=".png"):
    """Generate an auto-numbered filename that doesn't exist yet."""
    counter = 0
    while True:
        filename = f"{base_name}-{counter:04d}{extension}"
        if not os.path.exists(filename):
            return filename
        counter += 1

def encode_image_to_base64(image_path):
    """Encode an image file to base64 with proper MIME type detection."""
    try:
        with open(image_path, 'rb') as image_file:
            image_data = image_file.read()
            encoded_string = base64.b64encode(image_data).decode('utf-8')
            
            # Get MIME type
            mime_type, _ = mimetypes.guess_type(image_path)
            if not mime_type or not mime_type.startswith('image/'):
                # Default to common image types
                if image_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp')):
                    ext = image_path.lower().split('.')[-1]
                    if ext == 'jpg':
                        ext = 'jpeg'
                    mime_type = f'image/{ext}'
                else:
                    mime_type = 'image/jpeg'  # fallback
            
            return f"data:{mime_type};base64,{encoded_string}"
    except Exception as e:
        print(f"Error reading image {image_path}: {e}", file=sys.stderr)
        exit(1)

def load_context_from_file(date_str):
    """Load chat history from a specific date's markdown file."""
    base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
    filename = f"{date_str}.md"
    file_path = base_path / filename
    
    if not file_path.exists():
        if args.debug:
            print(f"Debug: No context file found at {file_path}", file=sys.stderr)
        return None
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if args.debug:
            print(f"Debug: Loaded {len(content)} characters of context from {filename}", file=sys.stderr)
        
        return f"=== Previous Chat History from {date_str} ===\n\n{content}\n\n=== End of Previous Chat History ===\n\n"
    except Exception as e:
        print(f"Warning: Could not load context from {file_path}: {e}", file=sys.stderr)
        return None

def calculate_cost(model, input_tokens, output_tokens):
    """Calculate cost based on OpenAI pricing (as of 2024)."""
    # Check if this is an image generation model
    if model in ["dall-e-2", "dall-e-3"]:
        # Image generation pricing (per image, not per token)
        # Note: input_tokens contains encoded size/quality info for images
        # Format: size_idx * 10 + quality (0=standard, 1=hd)
        encoded_info = input_tokens
        size_idx = encoded_info // 10
        is_hd = (encoded_info % 10) == 1
        
        if model == "dall-e-2":
            # DALL-E-2 pricing (size doesn't affect price)
            total_cost = 0.020  # $0.020 per image for all sizes
        else:  # dall-e-3
            # DALL-E-3 pricing based on size and quality
            # size_idx: 0 = 1024x1024, 1 = 1024x1792, 2 = 1792x1024
            if size_idx == 0:  # 1024x1024
                total_cost = 0.080 if is_hd else 0.040
            else:  # 1024x1792 or 1792x1024
                total_cost = 0.120 if is_hd else 0.080
        
        return {
            "input_cost": 0.0,  # No token-based input cost for images
            "output_cost": total_cost,  # Treat image cost as "output"
            "total_cost": total_cost
        }
    
    # Pricing per 1M tokens (in dollars) for text models - Updated December 2024
    pricing = {
        # GPT-4.1 Series (Latest - December 2024)
        "gpt-4.1": {"input": 30.00, "output": 120.00},  # Premium pricing for latest model
        "gpt-4.1-mini": {"input": 15.00, "output": 60.00},
        "gpt-4.1-nano": {"input": 5.00, "output": 20.00},
        
        # GPT-4o Series (Updated pricing as of December 2024)
        "gpt-4o": {"input": 2.50, "output": 10.00},  # Further reduced pricing
        "gpt-4o-2024-05-13": {"input": 5.00, "output": 15.00},
        "gpt-4o-2024-08-06": {"input": 2.50, "output": 10.00},  # More recent version with better pricing
        "gpt-4o-2024-11-20": {"input": 2.50, "output": 10.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-4o-mini-2024-07-18": {"input": 0.15, "output": 0.60},
        
        # GPT-4 Turbo Series
        "gpt-4-turbo": {"input": 10.00, "output": 30.00},
        "gpt-4-turbo-2024-04-09": {"input": 10.00, "output": 30.00},
        "gpt-4-turbo-preview": {"input": 10.00, "output": 30.00},
        "gpt-4-0125-preview": {"input": 10.00, "output": 30.00},
        "gpt-4-1106-preview": {"input": 10.00, "output": 30.00},
        
        # GPT-4 Original Series
        "gpt-4": {"input": 30.00, "output": 60.00},
        "gpt-4-0613": {"input": 30.00, "output": 60.00},
        "gpt-4-32k": {"input": 60.00, "output": 120.00},
        "gpt-4-32k-0613": {"input": 60.00, "output": 120.00},
        
        # GPT-3.5 Turbo Series
        "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
        "gpt-3.5-turbo-0125": {"input": 0.50, "output": 1.50},
        "gpt-3.5-turbo-1106": {"input": 1.00, "output": 2.00},
        "gpt-3.5-turbo-16k": {"input": 3.00, "output": 4.00},
        "gpt-3.5-turbo-instruct": {"input": 1.50, "output": 2.00},
        
        # Embedding models
        "text-embedding-3-small": {"input": 0.02, "output": 0},
        "text-embedding-3-large": {"input": 0.13, "output": 0},
        "text-embedding-ada-002": {"input": 0.10, "output": 0},
    }
    
    # Get pricing for the model, default to GPT-4 Turbo if not found
    model_pricing = pricing.get(model, pricing.get("gpt-4o"))
    
    # Calculate costs (convert from per million to actual tokens)
    input_cost = (input_tokens / 1_000_000) * model_pricing["input"]
    output_cost = (output_tokens / 1_000_000) * model_pricing["output"]
    total_cost = input_cost + output_cost
    
    return {
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost
    }

def print_summary_table(model, input_tokens, output_tokens, total_time, first_token_time, cost_info, is_estimated=False, use_color=True):
    """Print a formatted summary table with color support."""
    # ANSI color codes (conditionally set based on use_color)
    if use_color:
        CYAN = '\033[96m'
        GREEN = '\033[92m'
        YELLOW = '\033[93m'
        BLUE = '\033[94m'
        MAGENTA = '\033[95m'
        BOLD = '\033[1m'
        END = '\033[0m'
    else:
        CYAN = GREEN = YELLOW = BLUE = MAGENTA = BOLD = END = ''
    
    # Calculate total tokens
    total_tokens = input_tokens + output_tokens
    
    # Table width (inner content width)
    width = 50
    
    # Helper to strip ANSI codes
    import re
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    
    def strip_ansi(text):
        return ansi_escape.sub('', text)
    
    # Helper to create a padded line
    def make_line(left, right, left_width=22):
        """Create a line with left and right parts, properly padded."""
        left_plain = strip_ansi(left)
        right_plain = strip_ansi(right)
        
        # Total inner content must be exactly 48 characters
        # Left part: left_text + padding to reach left_width
        left_part = left + ' ' * (left_width - len(left_plain))
        
        # Right part: right_text + padding to fill remaining space
        remaining_width = 48 - left_width - 1  # -1 for space between left and right
        right_part = right + ' ' * (remaining_width - len(right_plain))
        
        # Combine: left_part + space + right_part = exactly 48 chars
        content = f"{left_part} {right_part}"
        
        return f"{CYAN}‚ïë{END} {content} {CYAN}‚ïë{END}"
    
    # Build the table
    print(f"\n{BOLD}{CYAN}‚ïî{'‚ïê' * width}‚ïó{END}", file=sys.stderr)
    
    # Title
    title = "API Usage Summary"
    title_pad_left = (width - len(title)) // 2
    title_pad_right = width - len(title) - title_pad_left
    print(f"{BOLD}{CYAN}‚ïë{' ' * title_pad_left}{title}{' ' * title_pad_right}‚ïë{END}", file=sys.stderr)
    
    print(f"{BOLD}{CYAN}‚ï†{'‚ïê' * width}‚ï£{END}", file=sys.stderr)
    
    # Model info
    model_display = model if len(model) <= 30 else model[:27] + "..."
    print(make_line(f"{BOLD}Model:{END}", f"{GREEN}{model_display}{END}"), file=sys.stderr)
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Token usage
    print(make_line(f"{BOLD}Token Usage:{END}", ""), file=sys.stderr)
    
    est_marker = " (est)" if is_estimated else ""
    print(make_line("  Input:", f"{YELLOW}{input_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Output:", f"{YELLOW}{output_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Total:", f"{BOLD}{YELLOW}{total_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Performance
    print(make_line(f"{BOLD}Performance:{END}", ""), file=sys.stderr)
    
    if first_token_time:
        print(make_line("  Time to first token:", f"{BLUE}{first_token_time:.2f}s{END}"), file=sys.stderr)
    
    print(make_line("  Total time:", f"{BLUE}{total_time:.2f}s{END}"), file=sys.stderr)
    
    if output_tokens > 0 and total_time > 0:
        tokens_per_sec = output_tokens / total_time
        print(make_line("  Tokens/second:", f"{BLUE}{tokens_per_sec:.1f}{END}"), file=sys.stderr)
    
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Cost
    print(make_line(f"{BOLD}Cost Breakdown:{END}", ""), file=sys.stderr)
    
    print(make_line("  Input cost:", f"{MAGENTA}${cost_info['input_cost']:.6f}{END}"), file=sys.stderr)
    print(make_line("  Output cost:", f"{MAGENTA}${cost_info['output_cost']:.6f}{END}"), file=sys.stderr)
    print(make_line(f"  {BOLD}Total cost:{END}", f"{BOLD}{MAGENTA}${cost_info['total_cost']:.6f}{END}"), file=sys.stderr)
    
    print(f"{BOLD}{CYAN}‚ïö{'‚ïê' * width}‚ïù{END}", file=sys.stderr)

def save_chat_transaction(provider, model, prompt, response, metadata=None):
    """Save chat transaction to markdown file organized by date."""
    # Create directory structure if it doesn't exist
    base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
    base_path.mkdir(parents=True, exist_ok=True)
    
    # Generate filename based on today's date
    today = datetime.now()
    filename = today.strftime("%Y-%m-%d.md")
    file_path = base_path / filename
    
    # Generate timestamp for section header
    timestamp = today.strftime("%Y-%m-%d %H:%M:%S")
    
    # Prepare content
    content = f"\n# {timestamp}\n\n"
    content += f"## Prompt\n\n```\n{prompt}\n```\n\n"
    content += f"## Response\n\n{response}\n\n"
    
    # Add metadata if provided
    if metadata:
        content += f"## Metadata\n\n"
        content += f"- **Provider**: {provider}\n"
        content += f"- **Model**: {model}\n"
        if 'input_tokens' in metadata:
            content += f"- **Input Tokens**: {metadata['input_tokens']:,}\n"
        if 'output_tokens' in metadata:
            content += f"- **Output Tokens**: {metadata['output_tokens']:,}\n"
        if 'total_tokens' in metadata:
            content += f"- **Total Tokens**: {metadata['total_tokens']:,}\n"
        if 'cost_info' in metadata:
            cost_info = metadata['cost_info']
            content += f"- **Input Cost**: ${cost_info['input_cost']:.6f}\n"
            content += f"- **Output Cost**: ${cost_info['output_cost']:.6f}\n"
            content += f"- **Total Cost**: ${cost_info['total_cost']:.6f}\n"
        if 'total_time' in metadata:
            content += f"- **Total Time**: {metadata['total_time']:.2f}s\n"
        if 'first_token_time' in metadata and metadata['first_token_time']:
            content += f"- **Time to First Token**: {metadata['first_token_time']:.2f}s\n"
        if 'stream_mode' in metadata:
            content += f"- **Streaming**: {metadata['stream_mode']}\n"
        if 'images' in metadata and metadata['images']:
            content += f"- **Images**: {len(metadata['images'])} included\n"
        if 'image_generated' in metadata and metadata['image_generated']:
            content += f"- **Image Generated**: {metadata['image_generated']}\n"
    
    content += "\n---\n"
    
    # Write to file (append if exists)
    try:
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        print(f"Warning: Could not save chat transaction: {e}", file=sys.stderr)

# Since outside of the distrobox we may not have these modules
# quietly ignore the fact that they may not exist
try:
    # Handle show-limits option first
    if args.show_limits:
        display_model_limits()
        exit(0)
    
    # Handle list-models option first
    if args.list_models:
        # Check if colors should be disabled
        use_colors = not args.no_color and environ.get("NO_COLOR", "").lower() not in ("1", "true")
        
        # ANSI color codes (conditionally set based on use_colors)
        class Colors:
            HEADER = '\033[95m' if use_colors else ''
            BLUE = '\033[94m' if use_colors else ''
            CYAN = '\033[96m' if use_colors else ''
            GREEN = '\033[92m' if use_colors else ''
            YELLOW = '\033[93m' if use_colors else ''
            RED = '\033[91m' if use_colors else ''
            BOLD = '\033[1m' if use_colors else ''
            UNDERLINE = '\033[4m' if use_colors else ''
            END = '\033[0m' if use_colors else ''
        
        # Query OpenAI API for available models
        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            response = requests.get("https://api.openai.com/v1/models", headers=headers)
            
            if response.status_code != 200:
                print(f"Error querying models: {response.status_code}")
                print(response.text)
                exit(1)
            
            models_data = response.json()
            
            # Extract and sort models by ID
            models = []
            for model in models_data.get("data", []):
                model_id = model.get("id", "")
                # Skip fine-tuned models (they usually have a colon in the ID)
                if ":" not in model_id:
                    # Categorize models for better descriptions
                    if "gpt-4" in model_id:
                        if "turbo" in model_id:
                            desc = "GPT-4 Turbo model"
                        elif "gpt-4o" in model_id:
                            if "mini" in model_id:
                                desc = "Affordable and intelligent small model"
                            else:
                                desc = "Multimodal flagship model"
                        else:
                            desc = "GPT-4 model"
                    elif "gpt-3.5" in model_id:
                        if "16k" in model_id:
                            desc = "Extended context version"
                        else:
                            desc = "Fast and efficient model"
                    elif "embedding" in model_id:
                        if "large" in model_id:
                            desc = "Most capable embedding model"
                        elif "small" in model_id:
                            desc = "Efficient embedding model"
                        else:
                            desc = "Embedding model"
                    elif "dall-e" in model_id:
                        if "3" in model_id:
                            desc = "Most advanced image generation"
                        else:
                            desc = "Image generation model"
                    elif "whisper" in model_id:
                        desc = "Speech recognition model"
                    elif "tts" in model_id:
                        if "hd" in model_id:
                            desc = "High quality text-to-speech"
                        else:
                            desc = "Text-to-speech model"
                    else:
                        desc = "AI model"
                    
                    models.append((model_id, model_id, desc))
            
            # Sort models by ID
            models.sort(key=lambda x: x[0])
            
            if not models:
                print(f"{Colors.YELLOW}No models found. Check your API key permissions.{Colors.END}")
                exit(0)
            
            # Calculate column widths
            max_model = max(len(model[0]) for model in models)
            max_name = max(len(model[1]) for model in models)
            max_desc = max(len(model[2]) for model in models)
            
            # Ensure minimum widths
            max_model = max(max_model, len("Model ID"))
            max_name = max(max_name, len("Name"))
            max_desc = max(max_desc, len("Description"))
            
            # Print header
            print(f"\n{Colors.BOLD}{Colors.CYAN}Available OpenAI Models:{Colors.END}")
            print(f"{Colors.BOLD}{'‚îÄ' * (max_model + max_name + max_desc + 6)}{Colors.END}")
            
            # Print table header
            print(f"{Colors.BOLD}{Colors.HEADER}{'Model ID':<{max_model}} {'Name':<{max_name}} {'Description':<{max_desc}}{Colors.END}")
            print(f"{Colors.BOLD}{'‚îÄ' * max_model} {'‚îÄ' * max_name} {'‚îÄ' * max_desc}{Colors.END}")
            
            # Print table rows
            for i, (model_id, name, description) in enumerate(models):
                # Alternate row colors
                color = Colors.CYAN if i % 2 == 0 else Colors.BLUE
                name_color = Colors.GREEN
                desc_color = Colors.YELLOW
                
                print(f"{color}{model_id:<{max_model}}{Colors.END} "
                      f"{name_color}{name:<{max_name}}{Colors.END} "
                      f"{desc_color}{description:<{max_desc}}{Colors.END}")
            
            print(f"{Colors.BOLD}{'‚îÄ' * (max_model + max_name + max_desc + 6)}{Colors.END}")
            print(f"{Colors.BOLD}Total models: {Colors.GREEN}{len(models)}{Colors.END}\n")
            
        except Exception as e:
            print(f"{Colors.RED}Error querying models: {e}{Colors.END}")
            print(f"{Colors.YELLOW}Falling back to known models list...{Colors.END}")
            
            # Fallback to static list if API fails
            models = [
                ("gpt-4-turbo", "GPT-4 Turbo", "Most capable GPT-4 model"),
                ("gpt-4o", "GPT-4o", "Multimodal flagship model"),
                ("gpt-4o-mini", "GPT-4o Mini", "Affordable and intelligent small model"),
                ("gpt-4", "GPT-4", "Original GPT-4 model"),
                ("gpt-3.5-turbo", "GPT-3.5 Turbo", "Fast and efficient model"),
                ("dall-e-3", "DALL-E 3", "Most advanced image generation"),
                ("whisper-1", "Whisper v1", "Speech recognition model"),
            ]
            
            # Calculate and display as before
            max_model = max(len(model[0]) for model in models)
            max_name = max(len(model[1]) for model in models)
            max_desc = max(len(model[2]) for model in models)
            
            max_model = max(max_model, len("Model ID"))
            max_name = max(max_name, len("Name"))
            max_desc = max(max_desc, len("Description"))
            
            print(f"\n{Colors.BOLD}{Colors.CYAN}Known OpenAI Models:{Colors.END}")
            print(f"{Colors.BOLD}{'‚îÄ' * (max_model + max_name + max_desc + 6)}{Colors.END}")
            
            print(f"{Colors.BOLD}{Colors.HEADER}{'Model ID':<{max_model}} {'Name':<{max_name}} {'Description':<{max_desc}}{Colors.END}")
            print(f"{Colors.BOLD}{'‚îÄ' * max_model} {'‚îÄ' * max_name} {'‚îÄ' * max_desc}{Colors.END}")
            
            for i, (model_id, name, description) in enumerate(models):
                color = Colors.CYAN if i % 2 == 0 else Colors.BLUE
                name_color = Colors.GREEN
                desc_color = Colors.YELLOW
                
                print(f"{color}{model_id:<{max_model}}{Colors.END} "
                      f"{name_color}{name:<{max_name}}{Colors.END} "
                      f"{desc_color}{description:<{max_desc}}{Colors.END}")
            
            print(f"{Colors.BOLD}{'‚îÄ' * (max_model + max_name + max_desc + 6)}{Colors.END}")
            print(f"{Colors.BOLD}Total models: {Colors.GREEN}{len(models)}{Colors.END}\n")
        
        exit(0)
    
    # Read file contents if any files were specified
    file_contents = []
    if args.files:
        for file_path in args.files:
            try:
                with open(file_path, 'r') as f:
                    file_content = f.read()
                    file_contents.append(f"=== File: {file_path} ===\n{file_content}\n")
            except IOError as e:
                print(f"Warning: Could not read file {file_path}: {e}", file=stderr)
    
    # Read from standard input only if there's data available
    query = ""
    # Check if stdin has data (not a terminal and has content)
    if not stdin.isatty() or stdin in select.select([stdin], [], [], 0)[0]:
        query = stdin.read()
    # If no stdin and no prompt/files provided, and not doing image generation or regen, show error
    elif not args.prompt and not args.files and not args.image_gen and not args.regen and not args.images:
        print("Error: No input provided. Use --prompt, -f/--file, or pipe input via stdin", file=stderr)
        exit(1)

    # Combine file contents with query
    combined_parts = []
    
    # Add file contents first if any
    if file_contents:
        combined_parts.extend(file_contents)
    
    # Add prompt if provided
    if args.prompt:
        combined_parts.append(args.prompt)
    
    # Add stdin content if any
    if query:
        combined_parts.append(query)
    
    # Combine all parts
    if combined_parts:
        query = "\n".join(combined_parts)
    else:
        query = ""

    # Load context if requested
    context_parts = []
    
    # Load today's context if --use-context is specified
    if args.use_context:
        today_str = datetime.now().strftime("%Y-%m-%d")
        context = load_context_from_file(today_str)
        if context:
            context_parts.append(context)
    
    # Load specific date's context if --use-context-from is specified
    if args.use_context_from:
        try:
            # Validate date format
            datetime.strptime(args.use_context_from, "%Y-%m-%d")
            context = load_context_from_file(args.use_context_from)
            if context:
                context_parts.append(context)
        except ValueError:
            print(f"Error: Invalid date format '{args.use_context_from}'. Use YYYY-MM-DD format.", file=stderr)
            exit(1)
    
    # Prepend context to query if any context was loaded
    if context_parts:
        context_str = "\n".join(context_parts)
        if query:
            query = context_str + "\nCurrent Query:\n" + query
        else:
            query = context_str
    
    # Prepend personality and emoji prompts if specified
    prompt_parts = []
    
    # Add personality prompt if specified
    if args.personality:
        personality_prompt = get_personality_prompt(args.personality)
        if personality_prompt:
            prompt_parts.append(personality_prompt)
    
    # Add emoji prompt if specified
    if args.emojis:
        emoji_prompt = get_emoji_prompt(args.emojis)
        if emoji_prompt:
            prompt_parts.append(emoji_prompt)
    
    # Combine prompts with original query
    if prompt_parts:
        combined_prompt = "".join(prompt_parts)
        if query:
            query = combined_prompt + query
        else:
            query = combined_prompt.rstrip()  # Remove trailing newlines if no other content

    # Set up API call
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Set the API endpoint based on the request type
    if args.embedding:
        # Use the embeddings API endpoint for OpenAI
        url = "https://api.openai.com/v1/embeddings"
        data = {
            "model": args.model if "embedding" in args.model else "text-embedding-3-small",
            "input": query
        }
    elif args.regen:
        # Note: OpenAI doesn't have direct image-to-image generation
        # This would require analyzing with GPT-4V then generating with DALL-E
        print("Error: Image regeneration is not currently supported for OpenAI.", file=sys.stderr)
        print("OpenAI requires a two-step process: vision analysis + DALL-E generation.", file=sys.stderr)
        print("Consider using 'openpy --image image.png --prompt \"regenerate this\"' with GPT-4V instead.", file=sys.stderr)
        exit(1)
        
    elif args.image_gen:
        # Use the image generation API endpoint for DALL-E
        url = "https://api.openai.com/v1/images/generations"
        
        # Determine which DALL-E model to use
        image_model = "dall-e-3"  # Default to DALL-E-3
        if args.model and "dall-e" in args.model.lower():
            image_model = args.model.lower()
        
        # Validate size based on model
        valid_sizes = {
            "dall-e-2": ["256x256", "512x512", "1024x1024"],
            "dall-e-3": ["1024x1024", "1024x1792", "1792x1024"]
        }
        
        if args.size not in valid_sizes[image_model]:
            print(f"Error: Invalid size '{args.size}' for {image_model}", file=sys.stderr)
            print(f"Valid sizes for {image_model}: {', '.join(valid_sizes[image_model])}", file=sys.stderr)
            exit(1)
        
        data = {
            "model": image_model,
            "prompt": query,
            "n": 1,
            "size": args.size,
            "response_format": "b64_json"
        }
        # Add quality parameter for DALL-E-3 if --hd is specified
        if args.hd and image_model == "dall-e-3":
            data["quality"] = "hd"
    else:
        # Use the chat completions API endpoint for regular requests
        url = "https://api.openai.com/v1/chat/completions"
        
        # Construct message content (text + images if provided)
        message_content = []
        
        # Add text content
        if query:
            message_content.append({
                "type": "text",
                "text": query
            })
        
        # Add image content if provided
        if args.images:
            for image_path in args.images:
                if not os.path.exists(image_path):
                    print(f"Error: Image file not found: {image_path}", file=sys.stderr)
                    exit(1)
                
                image_url = encode_image_to_base64(image_path)
                message_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": image_url
                    }
                })
        
        # If no content was added, add empty text
        if not message_content:
            message_content.append({
                "type": "text",
                "text": ""
            })
        
        # Get model-specific limits
        model_limits = get_model_limits(args.model)
        max_output_tokens = model_limits["max_output"]
        
        data = {
            "model": args.model,
            "max_tokens": max_output_tokens,
            "messages": [
                {
                    "role": "user",
                    "content": message_content
                }
            ],
            # Don't stream if JSON output is requested or --no-streaming
            "stream": not (args.json or args.no_streaming)
        }

    # Check input length and warn if it might exceed context limits
    if not args.embedding and not args.image_gen and query:
        estimated_input_tokens = estimate_tokens(query)
        check_context_limits(args.model, estimated_input_tokens, args.debug)
    
    # Print debug info if requested
    if args.debug:
        print(f"Debug: API URL: {url}", file=sys.stderr)
        print(f"Debug: Headers (API key partially hidden): {{'Authorization': 'Bearer {api_key[:5]}...{api_key[-4:]}', 'Content-Type': 'application/json'}}", file=sys.stderr)
        print(f"Debug: Data: {json.dumps(data)}", file=sys.stderr)
        
        # Show model limits in debug mode
        if not args.embedding and not args.image_gen:
            model_limits = get_model_limits(args.model)
            print(f"Debug: Model limits for {args.model}: Context Window: {model_limits['context_window']:,} tokens, Max Output: {model_limits['max_output']:,} tokens", file=sys.stderr)
            if query:
                estimated_input_tokens = estimate_tokens(query)
                print(f"Debug: Estimated input tokens: {estimated_input_tokens:,}", file=sys.stderr)
        if args.image_gen:
            output_file = args.output if args.output else get_auto_numbered_filename()
            print(f"Debug: Image will be saved to: {output_file}", file=sys.stderr)
            if args.hd and image_model == "dall-e-3":
                print(f"Debug: Using HD quality for DALL-E-3", file=sys.stderr)
        if args.images:
            print(f"Debug: Processing {len(args.images)} image(s): {', '.join(args.images)}", file=sys.stderr)

    # Handle dry-run mode
    if args.dry_run:
        print("=== DRY-RUN MODE ===", file=sys.stderr)
        print(f"API endpoint: {url}", file=sys.stderr)
        print("HTTP method: POST", file=sys.stderr)
        print(f"Headers: {{'Authorization': 'Bearer {api_key[:5]}...{api_key[-4:]}', 'Content-Type': 'application/json'}}", file=sys.stderr)
        
        # Format request payload (truncate long content)
        request_data = data.copy()
        if "messages" in request_data and request_data["messages"]:
            for msg in request_data["messages"]:
                if isinstance(msg.get("content"), list):
                    # Handle multi-modal content
                    for content_item in msg["content"]:
                        if content_item.get("type") == "text" and len(content_item.get("text", "")) > 200:
                            content_item["text"] = content_item["text"][:200] + "... [truncated]"
                        elif content_item.get("type") == "image_url":
                            content_item["image_url"]["url"] = content_item["image_url"]["url"][:100] + "... [truncated]"
                elif isinstance(msg.get("content"), str) and len(msg["content"]) > 200:
                    msg["content"] = msg["content"][:200] + "... [truncated]"
        elif "prompt" in request_data and len(request_data["prompt"]) > 200:
            request_data["prompt"] = request_data["prompt"][:200] + "... [truncated]"
        elif "input" in request_data and len(request_data["input"]) > 200:
            request_data["input"] = request_data["input"][:200] + "... [truncated]"
        
        print(f"Request payload: {json.dumps(request_data, indent=2)}", file=sys.stderr)
        
        # Show full prompt if not too long
        if query and len(query) <= 1000:
            print(f"\nFull prompt:\n{query}", file=sys.stderr)
        elif query:
            print(f"\nFull prompt (truncated):\n{query[:1000]}... [truncated]", file=sys.stderr)
        
        # Mode-specific information
        if args.embedding:
            print("\nMode: Embedding generation", file=sys.stderr)
            print(f"Embedding model: {args.model if 'embedding' in args.model else 'text-embedding-3-small'}", file=sys.stderr)
        elif args.image_gen:
            print("\nMode: Image generation", file=sys.stderr)
            print(f"Image model: {image_model}", file=sys.stderr)
            print(f"Image size: {args.size}", file=sys.stderr)
            if args.hd and image_model == "dall-e-3":
                print("Quality: HD", file=sys.stderr)
            else:
                print("Quality: Standard", file=sys.stderr)
            output_file = args.output if args.output else get_auto_numbered_filename()
            print(f"Output file: {output_file}", file=sys.stderr)
        elif args.images:
            print("\nMode: Vision analysis", file=sys.stderr)
            print(f"Images: {', '.join(args.images)}", file=sys.stderr)
        else:
            print("\nMode: Text generation", file=sys.stderr)
            model_limits = get_model_limits(args.model)
            print(f"Max tokens: {model_limits['max_output']:,}", file=sys.stderr)
            print(f"Context window: {model_limits['context_window']:,}", file=sys.stderr)
            print(f"Streaming: {not (args.json or args.no_streaming)}", file=sys.stderr)
        
        print("=== END DRY-RUN ===", file=sys.stderr)
        exit(0)

    # Track timing if summary mode
    start_time = time.time() if args.summary else None
    first_token_time = None
    
    # Send request to OpenAI API
    response = requests.post(url, headers=headers, json=data, stream=data.get("stream", False))

    if response.status_code != 200:
        print(f"Error: {response.status_code}")
        response_data = response.json() if response.headers.get('content-type', '').startswith('application/json') else {}
        
        # Provide more helpful error messages for common issues
        if response.status_code == 400 and args.image_gen:
            error_info = response_data.get("error", {})
            error_type = error_info.get("type", "")
            
            if error_type == "image_generation_user_error":
                print("Image generation failed - this is usually due to:")
                print("  ‚Ä¢ Content policy violation (violence, adult content, etc.)")
                print("  ‚Ä¢ Prompt safety filters")
                print("  ‚Ä¢ Try rephrasing your prompt to be more neutral/safe")
            else:
                print(f"Image generation error: {error_info.get('message', 'Unknown error')}")
        else:
            print(response.text)
        exit(1)
    
    # Handle the response based on request type
    if args.embedding:
        # Process embeddings response (never streamed)
        response_data = response.json()
        if "data" in response_data and len(response_data["data"]) > 0:
            # Extract and output just the embedding vector in JSON format
            embedding = response_data["data"][0]["embedding"]
            print(json.dumps({"embedding": embedding}))
            
            # Summary output for embeddings
            if args.summary and "usage" in response_data:
                end_time = time.time()
                usage = response_data["usage"]
                total_time = end_time - start_time
                
                # Add newline before summary
                print(file=sys.stderr)
                
                # Calculate cost
                cost_info = calculate_cost(
                    args.model if "embedding" in args.model else "text-embedding-3-small",
                    usage.get('prompt_tokens', 0),
                    0  # Embeddings have no output tokens
                )
                
                # Print summary table
                print_summary_table(
                    model=args.model if 'embedding' in args.model else 'text-embedding-3-small',
                    input_tokens=usage.get('prompt_tokens', 0),
                    output_tokens=0,
                    total_time=total_time,
                    first_token_time=None,
                    cost_info=cost_info,
                    use_color=not args.no_color
                )
        else:
            print(json.dumps(response_data))
    elif args.image_gen:
        # Process image generation response
        response_data = response.json()
        if "data" in response_data and len(response_data["data"]) > 0:
            # Get the base64 image data
            image_data = response_data["data"][0]["b64_json"]
            
            # Determine output filename
            if args.output:
                output_filename = args.output
            else:
                output_filename = get_auto_numbered_filename()
            
            # Decode and save the image
            try:
                image_bytes = base64.b64decode(image_data)
                with open(output_filename, 'wb') as f:
                    f.write(image_bytes)
                print(f"Image saved to: {output_filename}")
                
                # Summary output for image generation
                if args.summary:
                    end_time = time.time()
                    total_time = end_time - start_time
                    
                    # Add newline before summary
                    print(file=sys.stderr)
                    
                    # Image generation typically doesn't provide token counts
                    # Estimate input tokens from prompt length
                    estimated_input_tokens = len(query) // 4
                    estimated_output_tokens = 0  # Images don't have text output tokens
                    
                    # Calculate cost for image generation
                    # Encode size and quality info for cost calculation
                    # Map size to index: 0 = 1024x1024, 1 = 1024x1792, 2 = 1792x1024
                    size_map = {
                        "1024x1024": 0,
                        "1024x1792": 1, 
                        "1792x1024": 2,
                        "256x256": 0,   # DALL-E-2 sizes
                        "512x512": 0
                    }
                    size_idx = size_map.get(args.size, 0)
                    quality_flag = 1 if args.hd and image_model == "dall-e-3" else 0
                    encoded_info = size_idx * 10 + quality_flag
                    
                    cost_info = calculate_cost(
                        image_model,  # Use the actual model for pricing
                        encoded_info,  # Size and quality info encoded
                        0  # Not used for image generation
                    )
                    
                    # Print summary table
                    print_summary_table(
                        model=image_model,
                        input_tokens=estimated_input_tokens,
                        output_tokens=estimated_output_tokens,
                        total_time=total_time,
                        first_token_time=None,
                        cost_info=cost_info,
                        is_estimated=True,
                        use_color=not args.no_color
                    )
                
                # Save chat transaction for image generation if not disabled
                if not args.no_preserve:
                    # Prepare metadata
                    metadata = {
                        'stream_mode': False,
                        'images': args.images,
                        'image_generated': output_filename,
                        'total_time': total_time,
                        'cost_info': cost_info
                    }
                    
                    save_chat_transaction("OpenAI", image_model, query, f"Image generated and saved to: {output_filename}", metadata)
            except Exception as e:
                print(f"Error saving image: {e}", file=sys.stderr)
                exit(1)
        else:
            print(f"Error: No image data in response: {json.dumps(response_data)}", file=sys.stderr)
            exit(1)
    elif args.json or args.no_streaming:
        # Process standard JSON response (not streamed)
        response_data = response.json()
        # For chat completions, extract the message content
        if "choices" in response_data and len(response_data["choices"]) > 0:
            message = response_data["choices"][0]["message"]
            response_text = message["content"]
            
            # Enhance with emojis if requested
            if args.enhance_emojis and response_text:
                print("Enhancing response with emojis...", file=sys.stderr)
                response_text = enhance_with_emojis(response_text)
            
            print(response_text)
            
            # Summary output
            if args.summary and "usage" in response_data:
                end_time = time.time()
                usage = response_data["usage"]
                total_time = end_time - start_time
                
                # Add newline before summary
                print(file=sys.stderr)
                
                # Calculate cost
                cost_info = calculate_cost(
                    args.model,
                    usage.get('prompt_tokens', 0),
                    usage.get('completion_tokens', 0)
                )
                
                # Print summary table
                print_summary_table(
                    model=args.model,
                    input_tokens=usage.get('prompt_tokens', 0),
                    output_tokens=usage.get('completion_tokens', 0),
                    total_time=total_time,
                    first_token_time=None,
                    cost_info=cost_info,
                    use_color=not args.no_color
                )
            
            # Save chat transaction if not disabled
            if not args.no_preserve and not args.embedding:
                # Prepare metadata
                metadata = {
                    'stream_mode': False,
                    'images': args.images,
                    'input_tokens': usage.get('prompt_tokens', 0),
                    'output_tokens': usage.get('completion_tokens', 0),
                    'total_tokens': usage.get('prompt_tokens', 0) + usage.get('completion_tokens', 0),
                    'cost_info': cost_info,
                    'total_time': total_time
                }
                
                save_chat_transaction("OpenAI", args.model, query, message["content"], metadata)
        else:
            # Just print the raw response as fallback
            if args.json:
                print(json.dumps(response_data))
            else:
                print(f"Unexpected response format: {json.dumps(response_data)}", file=sys.stderr)
    else:
        # Process streaming response
        accumulated_content = ""
        token_count = 0
        input_tokens = None
        output_tokens = None
        
        for line in response.iter_lines():
            if line:
                # Skip the initial "data: " prefix
                line_text = line.decode('utf-8')
                if line_text.startswith("data: "):
                    line_json = line_text[6:]
                    
                    # The last message is usually "data: [DONE]"
                    if line_json == "[DONE]":
                        break
                        
                    try:
                        event = json.loads(line_json)
                        # Extract content delta if it exists
                        if "choices" in event and len(event["choices"]) > 0:
                            choice = event["choices"][0]
                            if "delta" in choice and "content" in choice["delta"]:
                                content = choice["delta"]["content"]
                                
                                # For emoji enhancement, we don't stream but show a progress indicator  
                                if args.enhance_emojis:
                                    # Just accumulate without printing during streaming
                                    pass
                                else:
                                    print(content, end="", flush=True)
                                
                                # Track first token time
                                if args.summary and first_token_time is None and content:
                                    first_token_time = time.time()
                                
                                accumulated_content += content
                                token_count += 1
                        
                        # Some models provide usage in streaming
                        if args.summary and "usage" in event:
                            usage = event["usage"]
                            input_tokens = usage.get("prompt_tokens", input_tokens)
                            output_tokens = usage.get("completion_tokens", output_tokens)
                    except json.JSONDecodeError:
                        continue
        
        # Enhance with emojis if requested (after streaming is complete)
        if args.enhance_emojis and accumulated_content:
            print("Enhancing response with emojis...", file=sys.stderr)
            enhanced_content = enhance_with_emojis(accumulated_content)
            print(enhanced_content)
        elif not args.enhance_emojis:
            # Add newline if we were streaming normally
            print()
        
        # Summary output for streaming
        if args.summary:
            end_time = time.time()
            total_time = end_time - start_time
            
            # Add newline before summary
            print(file=sys.stderr)
            
            # For streaming, we might not get exact token counts from the API
            # We can estimate based on the response
            is_estimated = False
            if input_tokens is None or output_tokens is None:
                # Rough estimation: ~4 characters per token
                estimated_output_tokens = len(accumulated_content) // 4
                estimated_input_tokens = len(query) // 4
                input_tokens = input_tokens or estimated_input_tokens
                output_tokens = output_tokens or estimated_output_tokens
                is_estimated = True
            
            # Calculate cost
            cost_info = calculate_cost(
                args.model,
                input_tokens,
                output_tokens
            )
            
            # Calculate time to first token
            ttft = (first_token_time - start_time) if first_token_time else None
            
            # Print summary table
            print_summary_table(
                model=args.model,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                total_time=total_time,
                first_token_time=ttft,
                cost_info=cost_info,
                is_estimated=is_estimated,
                use_color=not args.no_color
            )
        
        # Save chat transaction if not disabled (for streaming)
        if not args.no_preserve and not args.embedding and accumulated_content:
            # Prepare metadata
            metadata = {
                'stream_mode': True,
                'images': args.images
            }
            
            # Add token counts if available
            if input_tokens is not None:
                metadata['input_tokens'] = input_tokens
            if output_tokens is not None:
                metadata['output_tokens'] = output_tokens
            if input_tokens is not None and output_tokens is not None:
                metadata['total_tokens'] = input_tokens + output_tokens
            
            # Add cost info if we have token counts
            if 'input_tokens' in metadata and 'output_tokens' in metadata:
                metadata['cost_info'] = cost_info
            
            # Add timing
            if 'total_time' in locals():
                metadata['total_time'] = total_time
            if 'ttft' in locals():
                metadata['first_token_time'] = ttft
            
            save_chat_transaction("OpenAI", args.model, query, accumulated_content, metadata)

except (ImportError, Exception) as e:
    print(f"Error: {e}")
    exit(1)
