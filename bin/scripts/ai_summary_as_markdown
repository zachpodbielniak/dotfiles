#!/usr/bin/python3
#
# AI Summary as Markdown
# ======================
# This script generates structured markdown summaries from any text input using AI.
# It provides specialized prompt templates for different content types and leverages
# aipy for AI processing, with support for provider routing, model selection, and emoji enhancement.
#
# Author: Zach (with contributions from Claude)
# License: N/A
# 
# Usage:
#   cat input.txt | ai_summary_as_markdown [options]
#
# Options:
#   --type TYPE              Content type to determine specialized summary format
#   --principle              Focus on higher-level principles instead of literal content
#   --prompt TEXT            Append custom instructions to the built-in prompt
#   --debug                  Toggle debug logging
#   --provider PROVIDER      AI provider to use (optional - aipy will auto-route based on model)
#   --model MODEL            Specific model to use (aipy will auto-route to correct provider)
#   --no-preserve            Don't save chat transaction to database or files
#   --emojis [MODE]          Encourage emoji usage ('light' or 'heavy'; default: light)
#   --enhance-emojis         Post-process response to add appropriate emojis
#   --dry-run                Show what command would be executed without running it
#   -L, --list-models        List all available models from aipy
#
# Examples:
#   cat meeting.txt | ai_summary_as_markdown --type meeting
#   cat article.txt | ai_summary_as_markdown --principle
#   cat doc.txt | ai_summary_as_markdown --type custom --prompt "Custom instructions here"
#   cat input.txt | ai_summary_as_markdown --model grok-3  # Auto-routes to grokpy
#   cat input.txt | ai_summary_as_markdown --model claude-sonnet-4-20250514  # Auto-routes to claudpy
#   cat input.txt | ai_summary_as_markdown --provider geminpy  # Uses default Gemini model
#   cat input.txt | ai_summary_as_markdown --emojis heavy --enhance-emojis
#   cat input.txt | ai_summary_as_markdown --no-preserve --emojis light
#   cat input.txt | ai_summary_as_markdown --dry-run  # Show what would happen
#   ai_summary_as_markdown -L  # List all available models
#

import sys
import argparse
from sys import stdin, argv
from subprocess import run

# ===========================================
# PROMPT TEMPLATE DEFINITIONS
# ===========================================
# Each prompt is specialized for a particular content type
# Format pattern: specific elements to extract and formatting instructions

# Meeting transcript prompt template
# Focuses on agenda, points discussed, decisions, and action items
prompt_meeting: str = """
Summarize the following meeting transcript by outlining:
- The meeting agenda
- Major discussion points, organized under relevant agenda sections and listed as bullet points
- What decisions were agreed upon
- Key takeaways from the meeting, specifying who owns each action item.
"""

# Book summary prompt template
# Handles both fiction and non-fiction with appropriate structure
prompt_book: str = """
Create a detailed summary of the book content provided below. Detect if it is fiction or non-fiction based on the content. For fiction, include a concise plot overview, central themes, and significant points or developments broken down by chapter. For non-fiction, outline the primary arguments or topics discussed in each chapter and emphasize the main takeaways.

The book content follows after the separator line.
"""

# Research paper prompt template
# Extracts academic structure of hypothesis, methods, findings, limitations
prompt_research: str = """
Summarize this research paper by outlining:
- Research objectives/hypotheses
- Methodology used (study design/data sources)
- Key findings/results organized by section
- Limitations of the research
- Conclusions & implications for future work
"""

# Lecture transcript prompt template
# Organizes educational content with learning objectives and key concepts
prompt_lecture: str = """
Convert this lecture transcript into an outline featuring:
- The main topic and learning objectives
- Key definitions/formulas introduced (italicized)
- Case studies/examples referenced
- Questions raised during Q&A session
- Connections to other topics in the field
"""

# Tutorial/guide prompt template
# Structures instructional content with prerequisites and steps
prompt_tutorial: str = """
Transform this technical guide into a structured summary:
- Core objective/task explained
- Prerequisites/tools required
- Step-by-step process overview (numbered list)
- Common pitfalls & troubleshooting tips
- Suggestions for further learning
Use markdown code blocks for commands/examples where applicable.
"""

# Interview transcript prompt template
# Extracts background, themes, quotes and insights
prompt_interview: str = """
Condense this interview transcript into:
- Introducing the person being interviewed and their background
- Primary topics/themes covered
- Notable quotes from interviewee(s)
- Contextual insights about tone/subtext
- Key takeaways and actionable advice
"""

# Debate analysis prompt template
# Balances arguments from multiple perspectives
prompt_debate: str = """
Analyze this debate by:
- Identifying the central question or topic of disagreement
- Summarizing the strongest arguments from each side in a balanced way
- Highlighting points of agreement and areas of fundamental disagreement
- Noting logical fallacies or rhetorical techniques used
- Providing a balanced conclusion about the state of the debate
"""

# Podcast episode prompt template
# Includes timestamps and references for audio content
prompt_podcast: str = """
Summarize this podcast episode by identifying:
- Episode topic and guest information
- Main discussion segments with approximate timestamps
- Central arguments/theories proposed
- Resources/references mentioned
- Key takeaways and conclusions
"""

# Legal document prompt template
# Focuses on parties, clauses, obligations and risks
prompt_legal: str = """
Analyze this legal document to extract:
- Parties involved & effective dates
- 3-5 most consequential clauses/sections
- Obligations per party (bullet-pointed)
- Termination conditions & penalties
- Potential risks or concerning clauses
"""

# Medical case study prompt template
# Structures patient information, diagnosis, treatment and outcomes
prompt_medical: str = """
Summarize this medical case study by organizing:
- Patient presentation (demographics & chief complaints)
- Diagnostic tests/results
- Treatment pathway and procedures
- Outcome at discharge/follow-up
- Learning points and recommendations for similar cases
"""

# Product review prompt template
# Evaluates strengths, weaknesses, and comparisons
prompt_review: str = """
Synthesize this product review or multiple reviews into:
- Overall assessment and recommendation
- Key strengths and standout features
- Limitations, weaknesses, or concerns
- Comparison with alternatives (if mentioned)
- Ideal use cases or target users
"""

# Project information prompt template
# Structures goals, timeline, resources and success metrics
prompt_project: str = """
Structure this project information as:
- Project overview and objectives
- Key stakeholders and responsibilities
- Timeline with milestones and deliverables
- Resource requirements and constraints
- Risk assessment and mitigation strategies
- Success metrics and evaluation criteria
"""

# Higher-level principles extraction prompt
# Used with --principle flag to extract conceptual insights
prompt_principles: str = """
focus on the higher level principles that can be derived from this, intead of the literal values: give me a list of potential areas of my life that these principles could be applied.
"""

# General-purpose summary prompt template
# Default fallback for unspecified content types
prompt_general: str = """
Summarize the following input in a clear and concise manner, highlighting the most important points and main ideas for quick understanding. 
"""

# Technical documentation prompt template
# Extracts purpose, audience, components and usage information
prompt_techdoc: str = """
Summarize this technical documentation into structured sections:
- Purpose and primary goal
- Target audience
- Core components/functions/endpoints
- Setup steps (prerequisites, installation, configuration)
- Common use cases with examples
- Troubleshooting tips
"""

# Case study prompt template
# Focuses on problem statement, approach, results and applications
prompt_casestudy: str = """
Condense this case study into:
- Background and problem statement
- Methodology and solutions tested
- Key results and metrics (with quantitative impact)
- Takeaways and broader applicability
"""

# Event recap prompt template
# Summarizes event details, themes, sessions and outcomes
prompt_event: str = """
Summarize this event as:
- Event details (title, date, organizer)
- Key themes with speaker highlights
- Notable sessions/panels
- Networking outcomes
- Next steps/actions
"""

# Academic paper prompt template
# Similar to research but with more focus on academic contributions
prompt_academic: str = """
Summarize this academic paper with:
- Research question/hypothesis
- Methodology approach
- Key findings with statistical significance
- Implications to the field
- Limitations and future work directions
"""

# Email thread prompt template
# Organizes participants, discussions, decisions and action items
prompt_email: str = """
Condense this email thread into:
- Participants and their roles
- Main discussion topics
- Decisions made (with owners and deadlines)
- Open questions requiring follow-up
- Timeline of critical exchanges
"""

# Social media analysis prompt template
# Extracts trends, sentiment, influencers and recommendations
prompt_social: str = """
Analyze this social media data into:
- Top trends/hashtags/keywords with frequency
- Sentiment breakdown with percentages
- Key influencers and notable content
- Actionable recommendations based on trends
"""

# Course curriculum prompt template
# Structures learning objectives, modules and assessments
prompt_course: str = """
Structure this course information into:
- Learning objectives
- Module breakdown with topics, readings, and assignments
- Skills developed through the course
- Assessment methods and criteria
"""

# Survey results prompt template
# Summarizes demographics, insights and follow-up areas
prompt_survey: str = """
Summarize these survey results into:
- Demographic information of respondents
- Key insights and response patterns
- Notable quotes from participants
- Areas requiring follow-up investigation
"""

# User manual prompt template
# Extracts setup, safety, functionality and troubleshooting
prompt_manual: str = """
Extract critical information from this user manual into:
- Setup checklist with required tools
- Safety warnings and precautions
- Core functionality instructions
- Troubleshooting table for common issues
"""

# Product specifications prompt template
# Focuses on technical details, features and compatibility
prompt_product: str = """
Extract specifications from this product document:
- Product details and category
- Technical specifications
- Key features compared to competitors
- Compatibility requirements
- Pricing tiers and availability
"""

# Scientific paper prompt template
# Emphasizes methodological innovations and statistical findings
prompt_scientific: str = """
Create an enhanced summary of this scientific paper:
- Research question/hypothesis
- Methodological innovations
- Key findings with statistical data
- Limitations and suggested future research
"""

# Cleanup prompt template
# Preserves main content while removing navigation and non-essential elements
prompt_cleanup: str = """
Clean up the following content by preserving ONLY the main article/document content while removing all non-essential elements. Your task is to:

1. PRESERVE completely:
   - The main article text, paragraphs, and sections
   - All headings and subheadings from the main content
   - Important lists, quotes, and code blocks within the article
   - Images and their captions that are part of the main content
   - Tables and data that support the main narrative

2. REMOVE completely:
   - Navigation menus and links (e.g., "Home", "About", "Contact")
   - Sidebar content and widgets
   - Header and footer boilerplate text
   - Social media sharing buttons and links
   - Advertisement placeholders or promotional content
   - Cookie notices and privacy policy banners
   - "Related articles" or "You might also like" sections
   - Comment sections and user interactions
   - Newsletter signup forms
   - Copyright notices and legal disclaimers
   - Breadcrumb navigation
   - Tag clouds or category lists
   - Author bio boxes (unless integral to understanding the content)
   - Site-wide announcements or banners

3. OUTPUT FORMAT:
   - Return the cleaned content in markdown format
   - Maintain the original structure and hierarchy of the main content
   - Keep the natural flow and readability of the article
   - Do not add any summary or interpretation - just clean extraction
   - Do not add any prefatory text like "Here is the cleaned content" - start directly with the content

The goal is to extract what a reader would see if they used "Reader Mode" in a web browser - just the core article content without any website chrome or navigation elements.
"""

# Initialize empty prompt string that will be filled during execution
prompt: str = ""


# ===========================================
# MAIN EXECUTION LOGIC
# ===========================================

# Global try-except for robust error handling throughout the script
try:
    # Define the valid summary types that can be selected via --type
    # Each entry must have a corresponding prompt_<type> variable defined above
    SUMMARY_TYPES = ["meeting", "book", "research", "lecture", "tutorial",
                    "interview", "debate", "podcast", "legal", "medical",
                    "review", "project", "general", "custom", "techdoc",
                    "casestudy", "event", "academic", "email", "social",
                    "course", "survey", "manual", "product", "scientific",
                    "cleanup"]

    # Set up command-line argument parsing
    parser = argparse.ArgumentParser(description="Generate AI-powered content summaries in markdown format.")

    # Define available command-line arguments
    parser.add_argument("--type", choices=SUMMARY_TYPES,
                       required=False, help="Summary Type", default="")
    parser.add_argument("--principle", action="store_true", required=False, default=False,
                       help="Focus on higher-level principles instead of literal content")
    parser.add_argument("--prompt", required=False, help="Append custom text to the built-in prompt", default="")
    parser.add_argument("--debug", action="store_true", required=False, default=False,
                       help="Print debug logging to stderr")
    parser.add_argument("--provider", choices=["perpy", "claudpy", "grokpy", "ollampy", "geminpy"], required=False,
                       help="AI provider to use for processing (optional - aipy will auto-route based on model)")
    parser.add_argument("--model", required=False, help="Model to use with the selected provider", default="")
    parser.add_argument("--no-preserve", action="store_true", required=False, default=False,
                       help="Don't save chat transaction to database or files")
    parser.add_argument("--emojis", nargs="?", const="light", choices=["light", "heavy"], required=False,
                       help="Encourage emoji usage in responses. 'light' (default) uses emojis sparingly, 'heavy' uses them frequently.")
    parser.add_argument("--enhance-emojis", action="store_true", required=False, default=False,
                       help="Post-process the response to add appropriate emojis using a secondary AI call.")
    parser.add_argument("--dry-run", action="store_true", required=False, default=False,
                       help="Show what command would be executed without actually running it")
    parser.add_argument("-L", "--list-models", action="store_true", required=False, default=False,
                       help="List all available models from aipy")

    # Parse the arguments provided by the user
    args = parser.parse_args()

    # Handle list-models flag early (no need for prompt processing)
    if args.list_models:
        if (True == args.debug):
            print("[DEBUG] Listing models via aipy -L", file=sys.stderr, flush=True)
        result = run("aipy -L", shell=True)
        sys.exit(result.returncode)

    # ===========================================
    # DYNAMIC PROMPT MAPPING
    # ===========================================
    # Build a mapping from summary type names to their corresponding prompt templates
    # This allows easy extension by just adding new prompt_<type> variables

    if (True == args.debug):
        print("[DEBUG] Building prompt map from defined types", file=sys.stderr, flush=True)

    # Initialize with default mapping for empty type
    prompt_map = {
        "": prompt_general  # Default to general if empty
    }

    # Dynamically add each defined type to the prompt map
    for type_name in SUMMARY_TYPES:
        if (True == args.debug):
            print(f"[DEBUG] Processing type: {type_name}", file=sys.stderr, flush=True)

        if type_name == "custom":
            # Custom type uses the user-provided prompt instead of a predefined template
            prompt_map[type_name] = ""  # Empty template - will use args.prompt directly
            if (True == args.debug):
                print(f"[DEBUG] Added custom type with empty prompt", file=sys.stderr, flush=True)

        elif type_name == "general":
            # General type uses the generic summary prompt
            prompt_map[type_name] = prompt_general
            if (True == args.debug):
                print(f"[DEBUG] Added general type with general prompt", file=sys.stderr, flush=True)

        else:
            # For other types, look up the prompt variable by name (e.g., prompt_meeting)
            prompt_var_name = f"prompt_{type_name}"
            if prompt_var_name in locals():
                # If found, add it to the map
                prompt_map[type_name] = locals()[prompt_var_name]
                if (True == args.debug):
                    print(f"[DEBUG] Added {type_name} with {prompt_var_name}", file=sys.stderr, flush=True)
            else:
                # If not found, log a warning (this shouldn't happen if SUMMARY_TYPES is properly maintained)
                if (True == args.debug):
                    print(f"[DEBUG] Warning: No prompt found for {type_name}", file=sys.stderr, flush=True)

    # ===========================================
    # PROMPT SELECTION AND ASSEMBLY
    # ===========================================
    
    # Get the selected summary type, defaulting to general if not specified
    summary_type = args.type if args.type else "general"

    if (True == args.debug):
        print(f"[DEBUG] {summary_type} mode in use", file=sys.stderr, flush=True)

    # Retrieve the appropriate prompt template from the map
    base_prompt = prompt_map.get(summary_type, prompt_general)

    # Handle prompt assembly based on type and options
    if summary_type == "custom":
        # For custom type, use only the user-provided prompt
        prompt = f"{args.prompt}\n"
    else:
        # For predefined types, combine the template with any additional user instructions
        prompt = f"{base_prompt} {args.prompt}\n"

    # Apply principles modification if requested
    if (True == args.principle):
        if (True == args.debug):
            print("[DEBUG] principles mode is in use", file=sys.stderr, flush=True)
        # Prepend the principles prompt to focus on abstract concepts
        prompt = f"{prompt_principles} {prompt}"

    # Log the final assembled prompt for debugging
    if (True == args.debug):
        print(f"[DEBUG] prompt:\n{prompt}", file=sys.stderr, flush=True)

    # ===========================================
    # INPUT PROCESSING
    # ===========================================
    
    # Read raw bytes from stdin (allows handling of binary content)
    data_bytes = stdin.buffer.read()
    
    # Decode the input with UTF-8, replacing invalid characters to prevent crashes
    # This ensures the script can handle various encodings gracefully
    data = data_bytes.decode('utf-8', errors='replace')
 
    # Combine the prompt and input data with a separator
    query: str = f"{prompt}\n--------------------------\n{data}"
    
    # ===========================================
    # AI PROCESSING AND OUTPUT
    # ===========================================

    # Build the command using aipy with automatic provider routing
    provider_cmd = "aipy"

    # Add model argument if specified (aipy will auto-route to correct provider)
    if args.model:
        provider_cmd += f" --model {args.model}"
    elif args.provider:
        # If provider specified but no model, use default model for that provider
        default_models = {
            "perpy": "llama-3.1-sonar-large-128k-online",
            "claudpy": "claude-sonnet-4-20250514", 
            "grokpy": "grok-2-1212",
            "ollampy": "llama3.1:latest",
            "geminpy": "gemini-2.0-flash"
        }
        if args.provider in default_models:
            provider_cmd += f" --model {default_models[args.provider]}"
    # If neither model nor provider specified, let aipy use its own defaults
    
    # Add no-preserve flag if specified
    if args.no_preserve:
        provider_cmd += " --no-preserve"
    
    # Add emoji flags if specified
    if args.emojis:
        provider_cmd += f" --emojis {args.emojis}"
    
    if args.enhance_emojis:
        provider_cmd += " --enhance-emojis"

    if (True == args.debug):
        print(f"[DEBUG] Using aipy command: {provider_cmd}", file=sys.stderr, flush=True)

    # Handle dry-run mode
    if args.dry_run:
        print("ðŸ” DRY RUN MODE - Showing what would be executed:")
        print("=" * 50)
        print(f"ðŸ“‹ Summary Configuration:")
        print(f"   â€¢ Content Type: {summary_type}")
        print(f"   â€¢ Principles Mode: {'Yes' if args.principle else 'No'}")
        print(f"   â€¢ Custom Prompt: {'Yes' if args.prompt else 'No'}")
        if args.provider:
            print(f"   â€¢ Provider: {args.provider}")
        if args.model:
            print(f"   â€¢ Model: {args.model}")
        else:
            print(f"   â€¢ Model: aipy default (uses built-in model selection)")
        print(f"   â€¢ No Preserve: {'Yes' if args.no_preserve else 'No'}")
        if args.emojis:
            print(f"   â€¢ Emoji Mode: {args.emojis}")
        print(f"   â€¢ Enhance Emojis: {'Yes' if args.enhance_emojis else 'No'}")
        print()
        print(f"ðŸš€ Command that would be executed:")
        print(f"   {provider_cmd}")
        print()
        print(f"ðŸ“ Prompt that would be sent:")
        print("-" * 30)
        print(prompt[:200] + "..." if len(prompt) > 200 else prompt)
        print("-" * 30)
        print(f"ðŸ“Š Input data length: {len(data)} characters")
        print()
        print("âœ¨ No actual AI call would be made in dry-run mode")
        sys.exit(0)

    # Run the selected AI provider to process the input
    result = run(
        provider_cmd,
        input=query.encode('utf-8'),
        shell=True,
        capture_output=True
    )
    
    # Handle the results
    if result.returncode == 0:
        # Success - output the generated summary
        print(result.stdout.decode('utf-8', errors='replace'))
    else:
        # Error in AI processing - provide helpful error information
        print(f"* Error in AI processing\n\n{result.stderr.decode('utf-8', errors='replace')}")
        sys.exit(1)
        
except Exception as e:
    # Catch-all for any other exceptions to prevent cryptic crashes
    print(f"* Error in AI summary script\n\n{str(e)}")
    sys.exit(1)
