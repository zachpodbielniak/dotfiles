#!/usr/bin/python3

# dotfiles - Personal configuration files and scripts
# Copyright (C) 2025  Zach Podbielniak
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.


from os import environ
from subprocess import run
from sys import argv, exit, stdin, stderr
import argparse
import json
import sys
import time
import select
from datetime import datetime
from pathlib import Path


ctr_id: str|None = ""
api_key: str|None = ""

if ("CONTAINER_ID" in environ):
    ctr_id = environ.get("CONTAINER_ID")

# Check if distrobox check should be skipped
no_dbox_check = environ.get("NO_DBOX_CHECK", "").lower() in ("1", "true")

# if we are not in the 'dev' distrobox re-exec the script
# inside of the 'dev' distrobox
if not no_dbox_check and ("dev" != ctr_id):
    cmd: list[str] = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *argv
    ]

    run(cmd)
    exit(0)

if ("PERPLEXITY_TOKEN" in environ):
    api_key = environ.get("PERPLEXITY_TOKEN")
else:
    print("PERPLEXITY_TOKEN is not set")
    exit(1)

# Parse arguments
parser = argparse.ArgumentParser(description="Query Perplexity API")
parser.add_argument("--prompt", help="Prompt to prepend to the input")
parser.add_argument("--model", default="sonar-pro", help="Model to use (default: sonar-pro). Available models: sonar, sonar-pro, reasoning-pro, sonar-reasoning-pro, r1-1776")
parser.add_argument("--debug", action="store_true", help="Enable debug mode (shows request details)")
parser.add_argument("--json", action="store_true", help="Return a clean JSON response without streaming")
parser.add_argument("-S", "--no-streaming", action="store_true", help="Disable streaming mode for cleaner output capture")
parser.add_argument("--embedding", action="store_true", help="Generate an embedding vector instead of a text response")
parser.add_argument("-f", "--file", action="append", dest="files",
                    help="Include file content in the context (can be specified multiple times)")
parser.add_argument("-L", "--list-models", action="store_true",
                    help="List available models from the Perplexity API")
parser.add_argument("--show-limits", action="store_true",
                    help="Show token limits for the specified model (or all known models)")
parser.add_argument("--no-color", action="store_true",
                    help="Disable colored output")
parser.add_argument("--summary", action="store_true",
                    help="Show usage summary with token counts, timing, and costs in a formatted table")
parser.add_argument("--dry-run", action="store_true",
                    help="Show what would be sent to the API without actually making the request")
parser.add_argument("--no-preserve", action="store_true",
                    help="Don't save the chat transaction to the second-brain markdown file")
parser.add_argument("--use-context", action="store_true",
                    help="Include today's chat history as context for the conversation")
parser.add_argument("--use-context-from", 
                    help="Include chat history from a specific date (YYYY-MM-DD format) as context")
parser.add_argument("--personality", 
                    help="Set a personality/role for the AI. Available: cfp, swe, teacher, doctor, lawyer, chef, therapist, scientist, historian, artist, coach, journalist, philosopher, librarian, tutor, consultant, translator, critic, comedian, mentor, investor, cybersecurity. You can also use custom descriptions.")
parser.add_argument("--emojis", nargs="?", const="light", choices=["light", "heavy"],
                    help="Encourage emoji usage in responses. 'light' (default) uses emojis sparingly, 'heavy' uses them frequently.")
parser.add_argument("--enhance-emojis", action="store_true", 
                    help="Post-process the response to add appropriate emojis using a secondary AI call.")
args = parser.parse_args()

# Personality definitions
PERSONALITIES = {
    "cfp": "You are a Certified Financial Planner (CFP) with extensive knowledge of personal finance, investment strategies, tax planning, retirement planning, and estate planning. Provide professional, ethical financial advice while always reminding users to consult with their own financial advisor for personalized guidance.",
    
    "swe": "You are a Senior Software Engineer with deep expertise in software architecture, design patterns, code quality, and best practices. Focus on writing clean, maintainable, and efficient code. Consider scalability, security, and performance in your recommendations.",
    
    "teacher": "You are a patient and encouraging teacher who excels at breaking down complex topics into understandable concepts. Use analogies, examples, and step-by-step explanations. Adapt your teaching style to the learner's level and encourage questions.",
    
    "doctor": "You are a knowledgeable medical professional who provides general health information and guidance. Always emphasize that your advice is educational only and users should consult with healthcare providers for medical diagnosis and treatment.",
    
    "lawyer": "You are a legal expert who provides general legal information and explains legal concepts clearly. Always clarify that this is not legal advice and users should consult with an attorney for specific legal matters.",
    
    "chef": "You are a professional chef with expertise in various cuisines, cooking techniques, and food science. Share recipes, cooking tips, and culinary knowledge with enthusiasm. Consider dietary restrictions and preferences when making suggestions.",
    
    "therapist": "You are a supportive mental health professional who provides emotional support and coping strategies. Always encourage users to seek professional help for serious mental health concerns while offering compassionate guidance.",
    
    "scientist": "You are a research scientist with broad knowledge across multiple scientific disciplines. Explain scientific concepts clearly, cite evidence-based information, and maintain scientific accuracy while making complex topics accessible.",
    
    "historian": "You are a historian with deep knowledge of world history, cultures, and historical analysis. Provide context, multiple perspectives, and help users understand how past events connect to the present.",
    
    "artist": "You are a creative artist with expertise in various art forms, techniques, and art history. Encourage creativity, provide constructive feedback, and help users explore their artistic expression.",
    
    "coach": "You are a life coach focused on helping people achieve their goals, overcome obstacles, and maximize their potential. Use motivational techniques, ask powerful questions, and help users create actionable plans.",
    
    "journalist": "You are an investigative journalist skilled in research, fact-checking, and clear communication. Help users understand complex issues, identify reliable sources, and think critically about information.",
    
    "philosopher": "You are a philosopher who explores deep questions about existence, ethics, knowledge, and reality. Engage in thoughtful dialogue, present multiple philosophical perspectives, and encourage critical thinking.",
    
    "librarian": "You are a research librarian with expertise in finding, evaluating, and organizing information. Help users with research strategies, source evaluation, and information literacy.",
    
    "tutor": "You are a subject-specific tutor who helps students understand difficult concepts, complete assignments, and prepare for exams. Focus on building understanding rather than just providing answers.",
    
    "consultant": "You are a business consultant with expertise in strategy, operations, and organizational development. Provide practical business advice, analyze problems systematically, and offer actionable recommendations.",
    
    "translator": "You are a professional translator and linguist with expertise in multiple languages and cultures. Help with translations, explain linguistic nuances, and provide cultural context.",
    
    "critic": "You are a thoughtful critic who analyzes literature, films, art, and media with depth and insight. Provide balanced critiques that consider both strengths and weaknesses while respecting different perspectives.",
    
    "comedian": "You are a witty comedian who uses humor appropriately to lighten conversations while remaining helpful. Balance entertainment with usefulness, and be sensitive to context and audience.",
    
    "mentor": "You are a wise mentor who guides others based on experience and wisdom. Offer perspective, share relevant experiences, and help mentees navigate challenges while encouraging their growth.",
    
    "investor": "You are an experienced investment professional with deep knowledge of financial markets, portfolio management, risk assessment, and investment strategies. Provide insights on stocks, bonds, real estate, alternative investments, and market analysis while emphasizing the importance of diversification and risk management.",
    
    "cybersecurity": "You are a cybersecurity expert with extensive knowledge of information security, threat analysis, network security, and digital privacy. Help users understand security best practices, identify vulnerabilities, and implement protective measures while staying current with emerging threats and security technologies."
}

def get_personality_prompt(personality_key):
    """Get the personality prompt for the given key, or return empty string if not found."""
    if not personality_key:
        return ""
    
    personality_key = personality_key.lower()
    if personality_key in PERSONALITIES:
        return PERSONALITIES[personality_key] + "\n\n"
    else:
        # If not a predefined personality, treat it as a custom personality description
        return f"You are {personality_key}.\n\n"

def get_emoji_prompt(emoji_mode):
    """Get the emoji prompt for the given mode."""
    if not emoji_mode:
        return ""
    
    emoji_prompts = {
        "light": "Feel free to use relevant emojis to enhance your response where appropriate.",
        "heavy": "Use emojis frequently throughout your response to make it more engaging and expressive. Include emojis to highlight key points, convey emotions, and make the content more visually appealing."
    }
    
    return emoji_prompts.get(emoji_mode, emoji_prompts["light"]) + "\n\n"

def get_model_limits(model):
    """Get input and output token limits for different Perplexity models."""
    # Model limits based on research and official documentation (as of 2025)
    model_limits = {
        # Sonar models
        "sonar-pro": {"input": 200_000, "output": 8_000},  # Confirmed: 200k context, 8k max output
        "sonar": {"input": 128_000, "output": 4_000},  # Estimated: lower tier than pro
        
        # Reasoning models  
        "sonar-reasoning-pro": {"input": 200_000, "output": 16_000},  # Estimated: reasoning + pro features
        "reasoning-pro": {"input": 128_000, "output": 8_000},  # Estimated: reasoning without pro tier
        
        # DeepSeek R1 based model
        "r1-1776": {"input": 256_000, "output": 32_000},  # Estimated: premium reasoning model
    }
    
    # Get limits for the model, default to sonar-pro if not found
    return model_limits.get(model, model_limits.get("sonar-pro", {"input": 200_000, "output": 8_000}))

def enhance_with_emojis(text, model="sonar"):
    """Enhance text with emojis using a secondary AI call."""
    if not text or not text.strip():
        return text
    
    enhancement_prompt = f"""Please add appropriate emojis to the following text to make it more engaging and expressive. Follow these guidelines:

1. Add emojis that are relevant and enhance understanding
2. Don't overdo it - use emojis strategically
3. Maintain the original meaning and tone
4. Place emojis at the BEGINNING of section headings, titles, and key points (e.g., "üöÄ Getting Started" not "Getting Started üöÄ")
5. For bullet points and lists, place emojis at the start of each item
6. Use emojis to emphasize important concepts, but place them before the text they're emphasizing
7. Return only the enhanced text with emojis, no additional commentary

Original text:
{text}"""

    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # Get model limits for enhancement
        enhancement_model_limits = get_model_limits(model)
        max_output = min(len(text.split()) * 2 + 1000, enhancement_model_limits["output"])
        
        data = {
            "model": model,
            "max_tokens": max_output,
            "messages": [
                {
                    "role": "user",
                    "content": enhancement_prompt
                }
            ]
        }
        
        response = requests.post("https://api.perplexity.ai/chat/completions", 
                               headers=headers, 
                               json=data, 
                               timeout=30)
        
        if response.status_code == 200:
            response_data = response.json()
            if "choices" in response_data and len(response_data["choices"]) > 0:
                return response_data["choices"][0]["message"]["content"].strip()
        
        # Return original text if enhancement fails
        return text
        
    except Exception as e:
        print(f"Warning: Emoji enhancement failed ({e}), returning original text", file=sys.stderr)
        return text

def load_context_from_file(date_str):
    """Load chat history from a specific date's markdown file."""
    base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
    filename = f"{date_str}.md"
    file_path = base_path / filename
    
    if not file_path.exists():
        if args.debug:
            print(f"Debug: No context file found at {file_path}", file=sys.stderr)
        return None
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if args.debug:
            print(f"Debug: Loaded {len(content)} characters of context from {filename}", file=sys.stderr)
        
        return f"=== Previous Chat History from {date_str} ===\n\n{content}\n\n=== End of Previous Chat History ===\n\n"
    except Exception as e:
        print(f"Warning: Could not load context from {file_path}: {e}", file=sys.stderr)
        return None

def calculate_cost(model, input_tokens, output_tokens):
    """Calculate cost based on Perplexity pricing (as of 2025)."""
    # Pricing per 1M tokens (in dollars) - Updated from official pricing page
    pricing = {
        # Sonar models
        "sonar-pro": {"input": 3.00, "output": 15.00},
        "sonar": {"input": 1.00, "output": 1.00},
        
        # Reasoning models
        "sonar-reasoning-pro": {"input": 2.00, "output": 8.00},
        "reasoning-pro": {"input": 1.00, "output": 5.00},
        
        # DeepSeek R1 based model
        "r1-1776": {"input": 2.00, "output": 8.00},
    }
    
    # Get pricing for the model, default to sonar-pro if not found
    model_pricing = pricing.get(model, pricing.get("sonar-pro"))
    
    # Calculate costs (convert from per million to actual tokens)
    input_cost = (input_tokens / 1_000_000) * model_pricing["input"]
    output_cost = (output_tokens / 1_000_000) * model_pricing["output"]
    total_cost = input_cost + output_cost
    
    return {
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost
    }

def print_summary_table(model, input_tokens, output_tokens, total_time, first_token_time, cost_info, is_estimated=False, use_color=True):
    """Print a formatted summary table with color support."""
    # ANSI color codes (conditionally set based on use_color)
    if use_color:
        CYAN = '\033[96m'
        GREEN = '\033[92m'
        YELLOW = '\033[93m'
        BLUE = '\033[94m'
        MAGENTA = '\033[95m'
        BOLD = '\033[1m'
        END = '\033[0m'
    else:
        CYAN = GREEN = YELLOW = BLUE = MAGENTA = BOLD = END = ''
    
    # Calculate total tokens
    total_tokens = input_tokens + output_tokens
    
    # Table width (inner content width)
    width = 50
    
    # Helper to strip ANSI codes
    import re
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    
    def strip_ansi(text):
        return ansi_escape.sub('', text)
    
    # Helper to create a padded line
    def make_line(left, right, left_width=22):
        """Create a line with left and right parts, properly padded."""
        left_plain = strip_ansi(left)
        right_plain = strip_ansi(right)
        
        # Total inner content must be exactly 48 characters
        # Left part: left_text + padding to reach left_width
        left_part = left + ' ' * (left_width - len(left_plain))
        
        # Right part: right_text + padding to fill remaining space
        remaining_width = 48 - left_width - 1  # -1 for space between left and right
        right_part = right + ' ' * (remaining_width - len(right_plain))
        
        # Combine: left_part + space + right_part = exactly 48 chars
        content = f"{left_part} {right_part}"
        
        return f"{CYAN}‚ïë{END} {content} {CYAN}‚ïë{END}"
    
    # Build the table
    print(f"\n{BOLD}{CYAN}‚ïî{'‚ïê' * width}‚ïó{END}", file=sys.stderr)
    
    # Title
    title = "API Usage Summary"
    title_pad_left = (width - len(title)) // 2
    title_pad_right = width - len(title) - title_pad_left
    print(f"{BOLD}{CYAN}‚ïë{' ' * title_pad_left}{title}{' ' * title_pad_right}‚ïë{END}", file=sys.stderr)
    
    print(f"{BOLD}{CYAN}‚ï†{'‚ïê' * width}‚ï£{END}", file=sys.stderr)
    
    # Model info
    model_display = model if len(model) <= 30 else model[:27] + "..."
    print(make_line(f"{BOLD}Model:{END}", f"{GREEN}{model_display}{END}"), file=sys.stderr)
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Token usage
    print(make_line(f"{BOLD}Token Usage:{END}", ""), file=sys.stderr)
    
    est_marker = " (est)" if is_estimated else ""
    print(make_line("  Input:", f"{YELLOW}{input_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Output:", f"{YELLOW}{output_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Total:", f"{BOLD}{YELLOW}{total_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Performance
    print(make_line(f"{BOLD}Performance:{END}", ""), file=sys.stderr)
    
    if first_token_time:
        print(make_line("  Time to first token:", f"{BLUE}{first_token_time:.2f}s{END}"), file=sys.stderr)
    
    print(make_line("  Total time:", f"{BLUE}{total_time:.2f}s{END}"), file=sys.stderr)
    
    if output_tokens > 0 and total_time > 0:
        tokens_per_sec = output_tokens / total_time
        print(make_line("  Tokens/second:", f"{BLUE}{tokens_per_sec:.1f}{END}"), file=sys.stderr)
    
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Cost
    print(make_line(f"{BOLD}Cost Breakdown:{END}", ""), file=sys.stderr)
    
    print(make_line("  Input cost:", f"{MAGENTA}${cost_info['input_cost']:.6f}{END}"), file=sys.stderr)
    print(make_line("  Output cost:", f"{MAGENTA}${cost_info['output_cost']:.6f}{END}"), file=sys.stderr)
    print(make_line(f"  {BOLD}Total cost:{END}", f"{BOLD}{MAGENTA}${cost_info['total_cost']:.6f}{END}"), file=sys.stderr)
    
    print(f"{BOLD}{CYAN}‚ïö{'‚ïê' * width}‚ïù{END}", file=sys.stderr)

def save_chat_transaction(provider, model, prompt, response, metadata=None):
    """Save chat transaction to markdown file organized by date."""
    # Create directory structure if it doesn't exist
    base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
    base_path.mkdir(parents=True, exist_ok=True)
    
    # Generate filename based on today's date
    today = datetime.now()
    filename = today.strftime("%Y-%m-%d.md")
    file_path = base_path / filename
    
    # Generate timestamp for section header
    timestamp = today.strftime("%Y-%m-%d %H:%M:%S")
    
    # Prepare content
    content = f"\n# {timestamp}\n\n"
    content += f"## Prompt\n\n```\n{prompt}\n```\n\n"
    content += f"## Response\n\n{response}\n\n"
    
    # Add metadata if provided
    if metadata:
        content += f"## Metadata\n\n"
        content += f"- **Provider**: {provider}\n"
        content += f"- **Model**: {model}\n"
        if 'input_tokens' in metadata:
            content += f"- **Input Tokens**: {metadata['input_tokens']:,}\n"
        if 'output_tokens' in metadata:
            content += f"- **Output Tokens**: {metadata['output_tokens']:,}\n"
        if 'total_tokens' in metadata:
            content += f"- **Total Tokens**: {metadata['total_tokens']:,}\n"
        if 'cost_info' in metadata:
            cost_info = metadata['cost_info']
            content += f"- **Input Cost**: ${cost_info['input_cost']:.6f}\n"
            content += f"- **Output Cost**: ${cost_info['output_cost']:.6f}\n"
            content += f"- **Total Cost**: ${cost_info['total_cost']:.6f}\n"
        if 'total_time' in metadata:
            content += f"- **Total Time**: {metadata['total_time']:.2f}s\n"
        if 'first_token_time' in metadata and metadata['first_token_time']:
            content += f"- **Time to First Token**: {metadata['first_token_time']:.2f}s\n"
        if 'stream_mode' in metadata:
            content += f"- **Streaming**: {metadata['stream_mode']}\n"
    
    content += "\n---\n"
    
    # Write to file (append if exists)
    try:
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        print(f"Warning: Could not save chat transaction: {e}", file=sys.stderr)

# Import perplexipy outside of conditional logic
try:
    from perplexipy import PerplexityClient
    client: PerplexityClient = None
except ImportError:
    PerplexityClient = None
    client = None

# Handle list-models option first
if args.list_models:
    # Check if colors should be disabled
    use_colors = not args.no_color and environ.get("NO_COLOR", "").lower() not in ("1", "true")
    
    # ANSI color codes (conditionally set based on use_colors)
    class Colors:
        HEADER = '\033[95m' if use_colors else ''
        BLUE = '\033[94m' if use_colors else ''
        CYAN = '\033[96m' if use_colors else ''
        GREEN = '\033[92m' if use_colors else ''
        YELLOW = '\033[93m' if use_colors else ''
        RED = '\033[91m' if use_colors else ''
        BOLD = '\033[1m' if use_colors else ''
        UNDERLINE = '\033[4m' if use_colors else ''
        END = '\033[0m' if use_colors else ''
    
    # Note: Perplexity doesn't provide a public models listing endpoint
    # Maintaining a curated list of known models
    models = [
        ("sonar-pro", "Sonar Pro", "Premium model with web search (default)"),
        ("sonar", "Sonar", "Standard model with web search"),
        ("reasoning-pro", "Reasoning Pro", "Advanced reasoning model"),
        ("sonar-reasoning-pro", "Sonar Reasoning Pro", "Premium reasoning model"),
        ("r1-1776", "R1-1776", "Revolutionary reasoning model"),
    ]
    
    # Calculate column widths
    max_model = max(len(model[0]) for model in models)
    max_name = max(len(model[1]) for model in models)
    max_desc = max(len(model[2]) for model in models)
    
    # Ensure minimum widths
    max_model = max(max_model, len("Model ID"))
    max_name = max(max_name, len("Name"))
    max_desc = max(max_desc, len("Description"))
    
    # Print header
    print(f"\n{Colors.BOLD}{Colors.CYAN}Available Perplexity Models:{Colors.END}")
    print(f"{Colors.BOLD}{'‚îÄ' * (max_model + max_name + max_desc + 6)}{Colors.END}")
    
    # Print table header
    print(f"{Colors.BOLD}{Colors.HEADER}{'Model ID':<{max_model}} {'Name':<{max_name}} {'Description':<{max_desc}}{Colors.END}")
    print(f"{Colors.BOLD}{'‚îÄ' * max_model} {'‚îÄ' * max_name} {'‚îÄ' * max_desc}{Colors.END}")
    
    # Print table rows
    for i, (model_id, name, description) in enumerate(models):
        # Alternate row colors
        color = Colors.CYAN if i % 2 == 0 else Colors.BLUE
        name_color = Colors.GREEN
        desc_color = Colors.YELLOW
        
        print(f"{color}{model_id:<{max_model}}{Colors.END} "
              f"{name_color}{name:<{max_name}}{Colors.END} "
              f"{desc_color}{description:<{max_desc}}{Colors.END}")
    
    print(f"{Colors.BOLD}{'‚îÄ' * (max_model + max_name + max_desc + 6)}{Colors.END}")
    print(f"{Colors.BOLD}Total models: {Colors.GREEN}{len(models)}{Colors.END}\n")
    
    exit(0)

# Handle show-limits option
if args.show_limits:
    # Check if colors should be disabled
    use_colors = not args.no_color and environ.get("NO_COLOR", "").lower() not in ("1", "true")
    
    # ANSI color codes
    class Colors:
        HEADER = '\033[95m' if use_colors else ''
        BLUE = '\033[94m' if use_colors else ''
        CYAN = '\033[96m' if use_colors else ''
        GREEN = '\033[92m' if use_colors else ''
        YELLOW = '\033[93m' if use_colors else ''
        RED = '\033[91m' if use_colors else ''
        BOLD = '\033[1m' if use_colors else ''
        UNDERLINE = '\033[4m' if use_colors else ''
        END = '\033[0m' if use_colors else ''
    
    print(f"\n{Colors.BOLD}{Colors.CYAN}Perplexity Model Token Limits:{Colors.END}")
    print(f"{Colors.BOLD}{'‚îÄ' * 70}{Colors.END}")
    
    # Get all model limits
    models_to_show = ["sonar-pro", "sonar", "sonar-reasoning-pro", "reasoning-pro", "r1-1776"]
    if args.model and args.model not in models_to_show:
        models_to_show = [args.model] + models_to_show
    
    # Print table header
    print(f"{Colors.BOLD}{Colors.HEADER}{'Model':<25} {'Input Limit':<15} {'Output Limit':<15}{Colors.END}")
    print(f"{Colors.BOLD}{'‚îÄ' * 25} {'‚îÄ' * 15} {'‚îÄ' * 15}{Colors.END}")
    
    # Print model limits
    for i, model in enumerate(models_to_show):
        limits = get_model_limits(model)
        
        # Alternate row colors
        color = Colors.CYAN if i % 2 == 0 else Colors.BLUE
        input_color = Colors.GREEN
        output_color = Colors.YELLOW
        
        # Format token counts with commas
        input_tokens = f"{limits['input']:,} tokens"
        output_tokens = f"{limits['output']:,} tokens"
        
        print(f"{color}{model:<25}{Colors.END} "
              f"{input_color}{input_tokens:<15}{Colors.END} "
              f"{output_color}{output_tokens:<15}{Colors.END}")
    
    print(f"{Colors.BOLD}{'‚îÄ' * 70}{Colors.END}")
    
    # Show specific model if requested
    if args.model:
        limits = get_model_limits(args.model)
        print(f"\n{Colors.BOLD}Current model ({args.model}) limits:{Colors.END}")
        print(f"  Input: {Colors.GREEN}{limits['input']:,}{Colors.END} tokens")
        print(f"  Output: {Colors.YELLOW}{limits['output']:,}{Colors.END} tokens")
    
    print()
    exit(0)

# Print debug info if requested
if args.debug:
    print(f"Debug: Using Perplexity API", file=sys.stderr)
    print(f"Debug: API key (partially hidden): {api_key[:5]}...{api_key[-4:]}", file=sys.stderr)
    print(f"Debug: Model: {args.model}", file=sys.stderr)
    model_limits = get_model_limits(args.model)
    print(f"Debug: Model limits - Input: {model_limits['input']:,} tokens, Output: {model_limits['output']:,} tokens", file=sys.stderr)

# Read file contents if any files were specified
file_contents = []
if args.files:
    for file_path in args.files:
        try:
            with open(file_path, 'r') as f:
                file_content = f.read()
                file_contents.append(f"=== File: {file_path} ===\n{file_content}\n")
        except IOError as e:
            print(f"Warning: Could not read file {file_path}: {e}", file=stderr)

# Read from standard input only if there's data available
query = ""
# Check if stdin has data (not a terminal and has content)
if not stdin.isatty() or stdin in select.select([stdin], [], [], 0)[0]:
    query = stdin.read()
# If no stdin and no prompt/files provided, show error
elif not args.prompt and not args.files:
    print("Error: No input provided. Use --prompt, -f/--file, or pipe input via stdin", file=stderr)
    exit(1)

# Combine file contents with query
combined_parts = []

# Add file contents first if any
if file_contents:
    combined_parts.extend(file_contents)

# Add prompt if provided
if args.prompt:
    combined_parts.append(args.prompt)

# Add stdin content if any
if query:
    combined_parts.append(query)

# Combine all parts
if combined_parts:
    query = "\n".join(combined_parts)
else:
    query = ""

# Check if input might exceed model's context window
if query and not args.embedding:
    estimated_input_tokens = len(query) // 4  # Rough estimate: 4 chars per token
    model_limits = get_model_limits(args.model)
    if estimated_input_tokens > model_limits["input"]:
        print(f"Warning: Input text (~{estimated_input_tokens:,} tokens) may exceed model's context limit ({model_limits['input']:,} tokens).", file=sys.stderr)
        print(f"Consider using a model with larger context window or reducing input size.", file=sys.stderr)

# Load context if requested
context_parts = []

# Load today's context if --use-context is specified
if args.use_context:
    today_str = datetime.now().strftime("%Y-%m-%d")
    context = load_context_from_file(today_str)
    if context:
        context_parts.append(context)

# Load specific date's context if --use-context-from is specified
if args.use_context_from:
    try:
        # Validate date format
        datetime.strptime(args.use_context_from, "%Y-%m-%d")
        context = load_context_from_file(args.use_context_from)
        if context:
            context_parts.append(context)
    except ValueError:
        print(f"Error: Invalid date format '{args.use_context_from}'. Use YYYY-MM-DD format.", file=stderr)
        exit(1)

# Prepend context to query if any context was loaded
if context_parts:
    context_str = "\n".join(context_parts)
    if query:
        query = context_str + "\nCurrent Query:\n" + query
    else:
        query = context_str

# Prepend personality and emoji prompts if specified
prompt_parts = []

# Add personality prompt if specified
if args.personality:
    personality_prompt = get_personality_prompt(args.personality)
    if personality_prompt:
        prompt_parts.append(personality_prompt)

# Add emoji prompt if specified
if args.emojis:
    emoji_prompt = get_emoji_prompt(args.emojis)
    if emoji_prompt:
        prompt_parts.append(emoji_prompt)

# Combine prompts with original query
if prompt_parts:
    combined_prompt = "".join(prompt_parts)
    if query:
        query = combined_prompt + query
    else:
        query = combined_prompt.rstrip()  # Remove trailing newlines if no other content

# Print debug info for query if requested
if args.debug:
    print(f"Debug: Query: {query[:100]}{'...' if len(query) > 100 else ''}", file=sys.stderr)

# Handle dry-run mode
if args.dry_run:
    print("=== DRY-RUN MODE ===", file=sys.stderr)
    print("API provider: Perplexity", file=sys.stderr)
    print(f"API key (partially hidden): {api_key[:5]}...{api_key[-4:]}", file=sys.stderr)
    
    # Mode-specific information
    if args.embedding:
        print("\nMode: Embedding generation", file=sys.stderr)
        print("API endpoint: (handled by perplexipy library)", file=sys.stderr)
        print("HTTP method: POST", file=sys.stderr)
        print(f"Embedding model: sonar-small-online", file=sys.stderr)
        print("Request format: client.get_embedding(text, model='sonar-small-online')", file=sys.stderr)
        
        # Show truncated prompt
        if query and len(query) <= 1000:
            print(f"\nFull prompt:\n{query}", file=sys.stderr)
        elif query:
            print(f"\nFull prompt (truncated):\n{query[:1000]}... [truncated]", file=sys.stderr)
    else:
        print("\nMode: Text generation", file=sys.stderr)
        print("API endpoint: (handled by perplexipy library)", file=sys.stderr)
        print("HTTP method: POST", file=sys.stderr)
        print(f"Model: {args.model}", file=sys.stderr)
        print(f"Streaming: {not (args.json or args.no_streaming)}", file=sys.stderr)
        
        if args.json or args.no_streaming:
            print("Request format: client.query(prompt)", file=sys.stderr)
        else:
            print("Request format: client.queryStreamable(prompt)", file=sys.stderr)
        
        # Show truncated prompt
        if query and len(query) <= 1000:
            print(f"\nFull prompt:\n{query}", file=sys.stderr)
        elif query:
            print(f"\nFull prompt (truncated):\n{query[:1000]}... [truncated]", file=sys.stderr)
    
    print("\nNote: Using perplexipy library for API interactions", file=sys.stderr)
    print("Library handles request formatting and endpoint details internally", file=sys.stderr)
    
    print("=== END DRY-RUN ===", file=sys.stderr)
    exit(0)

# Check if PerplexityClient is available and create client
if PerplexityClient is None:
    print("Error: perplexipy library not found. Please ensure it's installed in the 'dev' distrobox.", file=sys.stderr)
    exit(1)

client = PerplexityClient(key=api_key)
client.model = args.model

# Execute the API call based on the request type
if args.embedding:
    # Use the get_embedding method for embeddings
    # Note: Default to sonar-small-online model for embeddings
    embedding_model = "sonar-small-online"
    if args.debug:
        print(f"Debug: Generating embedding with model {embedding_model}", file=sys.stderr)
    
    # Track timing if summary mode
    start_time = time.time() if args.summary else None
    
    try:
        # Generate embedding
        embedding = client.get_embedding(query, model=embedding_model)
        # Output as JSON for semantic_search to parse
        print(json.dumps({"embedding": embedding}))
        
        # Summary output for embeddings
        if args.summary:
            end_time = time.time()
            total_time = end_time - start_time
            
            # Add newline before summary
            print(file=sys.stderr)
            
            # For embeddings, estimate token count
            estimated_tokens = len(query) // 4
            
            # Calculate cost
            cost_info = calculate_cost(
                "sonar",  # Use sonar for embedding cost estimation
                estimated_tokens,
                0  # Embeddings have no output tokens
            )
            
            # Print summary table
            print_summary_table(
                model="sonar-small-online",
                input_tokens=estimated_tokens,
                output_tokens=0,
                total_time=total_time,
                first_token_time=None,
                cost_info=cost_info,
                is_estimated=True,
                use_color=not args.no_color
            )
    except Exception as e:
        if args.debug:
            print(f"Debug: Error generating embedding: {e}", file=sys.stderr)
        # Fallback to empty embedding
        print(json.dumps({"embedding": []}))
        
# Execute normal query with or without streaming
elif args.json or args.no_streaming:
    # Use non-streaming version for clean output
    if args.debug:
        print(f"Debug: Executing non-streaming query", file=sys.stderr)
    
    # Track timing if summary mode
    start_time = time.time() if args.summary else None
    
    result = client.query(query)
    
    # Enhance with emojis if requested
    if args.enhance_emojis and result:
        print("Enhancing response with emojis...", file=sys.stderr)
        result = enhance_with_emojis(result)
    
    print(result)
    
    # Summary output for non-streaming
    if args.summary:
        end_time = time.time()
        total_time = end_time - start_time
        
        # Add newline before summary
        print(file=sys.stderr)
        
        # Estimate tokens (Perplexipy doesn't provide token counts)
        estimated_input_tokens = len(query) // 4
        estimated_output_tokens = len(result) // 4
        
        # Calculate cost
        cost_info = calculate_cost(
            args.model,
            estimated_input_tokens,
            estimated_output_tokens
        )
        
        # Print summary table
        print_summary_table(
            model=args.model,
            input_tokens=estimated_input_tokens,
            output_tokens=estimated_output_tokens,
            total_time=total_time,
            first_token_time=None,
            cost_info=cost_info,
            is_estimated=True,
            use_color=not args.no_color
        )
    
    # Save chat transaction if not disabled
    if not args.no_preserve and not args.embedding:
        # Prepare metadata
        metadata = {
            'stream_mode': False,
            'input_tokens': estimated_input_tokens,
            'output_tokens': estimated_output_tokens,
            'total_tokens': estimated_input_tokens + estimated_output_tokens,
            'cost_info': cost_info,
            'total_time': total_time
        }
        
        save_chat_transaction("Perplexity", args.model, query, result, metadata)
else:
    # Use streaming version for interactive output
    if args.debug:
        print(f"Debug: Executing streaming query", file=sys.stderr)
    
    # Track timing if summary mode
    start_time = time.time() if args.summary else None
    first_token_time = None
    accumulated_content = ""
    
    results = client.queryStreamable(query)
    for result in results:
        # Track first token time
        if args.summary and first_token_time is None and result:
            first_token_time = time.time()
        
        # flush since it may not have a '\n' to give it
        # a streaming output appearance
        if not args.enhance_emojis:
            print(result, end="", flush=True)
        accumulated_content += result
    
    # Enhance with emojis if requested (after streaming is complete)
    if args.enhance_emojis and accumulated_content:
        print("Enhancing response with emojis...", file=sys.stderr)
        enhanced_content = enhance_with_emojis(accumulated_content)
        print(enhanced_content)
    elif not args.enhance_emojis:
        # Add newline if we were streaming normally
        print()
    
    # Summary output for streaming
    if args.summary:
        end_time = time.time()
        total_time = end_time - start_time
        
        # Add newline before summary
        print(file=sys.stderr)
        
        # Estimate tokens (Perplexipy doesn't provide token counts)
        estimated_input_tokens = len(query) // 4
        estimated_output_tokens = len(accumulated_content) // 4
        
        # Calculate cost
        cost_info = calculate_cost(
            args.model,
            estimated_input_tokens,
            estimated_output_tokens
        )
        
        # Calculate time to first token
        ttft = (first_token_time - start_time) if first_token_time else None
        
        # Print summary table
        print_summary_table(
            model=args.model,
            input_tokens=estimated_input_tokens,
            output_tokens=estimated_output_tokens,
            total_time=total_time,
            first_token_time=ttft,
            cost_info=cost_info,
            is_estimated=True,
            use_color=not args.no_color
        )
    
    # Save chat transaction if not disabled (for streaming)
    if not args.no_preserve and not args.embedding and accumulated_content:
        # Prepare metadata
        metadata = {
            'stream_mode': True,
            'input_tokens': estimated_input_tokens,
            'output_tokens': estimated_output_tokens,
            'total_tokens': estimated_input_tokens + estimated_output_tokens,
            'cost_info': cost_info,
            'total_time': total_time,
            'first_token_time': ttft
        }
        
        save_chat_transaction("Perplexity", args.model, query, accumulated_content, metadata)

