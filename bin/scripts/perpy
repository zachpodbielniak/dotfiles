#!/usr/bin/python3

from os import environ
from subprocess import run
from sys import argv, exit, stdin, stderr
import argparse
import json
import sys


ctr_id: str|None = ""
api_key: str|None = ""

if ("CONTAINER_ID" in environ):
    ctr_id = environ.get("CONTAINER_ID")

# if we are not in the 'dev' distrobox re-exec the script
# inside of the 'dev' distrobox
if ("dev" != ctr_id):
    cmd: list[str] = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *argv
    ]

    run(cmd)
    exit(0)

if ("PERPLEXITY_TOKEN" in environ):
    api_key = environ.get("PERPLEXITY_TOKEN")
else:
    print("PERPLEXITY_TOKEN is not set")
    exit(1)

# Parse arguments
parser = argparse.ArgumentParser(description="Query Perplexity API")
parser.add_argument("--prompt", help="Prompt to prepend to the input")
parser.add_argument("--model", default="sonar-pro", help="Model to use (default: sonar-pro). Available models: sonar, sonar-pro, llama-3.1-sonar-small-128k-online, llama-3.1-sonar-large-128k-online, llama-3.1-sonar-huge-128k-online, llama-3.1-sonar-small-128k-chat, llama-3.1-sonar-large-128k-chat, llama-3.1-8b-instruct, llama-3.1-70b-instruct, reasoning-pro, sonar-reasoning-pro, r1-1776")
parser.add_argument("--debug", action="store_true", help="Enable debug mode (shows request details)")
parser.add_argument("--json", action="store_true", help="Return a clean JSON response without streaming")
parser.add_argument("--embedding", action="store_true", help="Generate an embedding vector instead of a text response")
args = parser.parse_args()

# Since outside of the distrobox we may not have these modules
# quietly ignore the fact that they may not exist
try:
    from perplexipy import PerplexityClient
    client: PerplexityClient
    client = PerplexityClient(key=api_key)
    client.model = args.model

    # Print debug info if requested
    if args.debug:
        print(f"Debug: Using Perplexity API", file=sys.stderr)
        print(f"Debug: API key (partially hidden): {api_key[:5]}...{api_key[-4:]}", file=sys.stderr)
        print(f"Debug: Model: {args.model}", file=sys.stderr)
except ImportError:
    pass

# Read from standard input
query = stdin.read()

# Prepend prompt if provided
if args.prompt:
    if query:
        query = f"{args.prompt}\n\n{query}"
    else:
        query = args.prompt

# Print debug info for query if requested
if args.debug:
    print(f"Debug: Query: {query[:100]}{'...' if len(query) > 100 else ''}", file=sys.stderr)

# Execute the API call based on the request type
if args.embedding:
    # Use the get_embedding method for embeddings
    # Note: Default to sonar-small-online model for embeddings
    embedding_model = "sonar-small-online"
    if args.debug:
        print(f"Debug: Generating embedding with model {embedding_model}", file=sys.stderr)
    
    try:
        # Generate embedding
        embedding = client.get_embedding(query, model=embedding_model)
        # Output as JSON for semantic_search to parse
        print(json.dumps({"embedding": embedding}))
    except Exception as e:
        if args.debug:
            print(f"Debug: Error generating embedding: {e}", file=sys.stderr)
        # Fallback to empty embedding
        print(json.dumps({"embedding": []}))
        
# Execute normal query with or without streaming
elif args.json:
    # Use non-streaming version for clean JSON output
    if args.debug:
        print(f"Debug: Executing non-streaming query", file=sys.stderr)
    result = client.query(query)
    print(result)
else:
    # Use streaming version for interactive output
    if args.debug:
        print(f"Debug: Executing streaming query", file=sys.stderr)
    results = client.queryStreamable(query)
    for result in results:
        # flush since it may not have a '\n' to give it
        # a streaming output appearance
        print(result, end="", flush=True)

