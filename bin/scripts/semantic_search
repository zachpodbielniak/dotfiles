#!/usr/bin/python3

# semantic_search - A specialized script for semantic search operations over your second brain
# This script provides a higher-level interface that:
# - Automatically generates embeddings from your query
# - Searches your vector database for similar content
# - Displays matching content from your second brain with relevant context
# Like other AI tools, it runs in the 'dev' distrobox container where dependencies are installed.

import os
import sys
import argparse
import json
import tempfile
import time
from subprocess import run, PIPE
from pathlib import Path

# ----- Distrobox Container Check -----
# Check if we're running inside the dev container by examining the CONTAINER_ID env var
ctr_id = os.environ.get("CONTAINER_ID", "")

# If we're not in the 'dev' distrobox, re-execute this exact script with all arguments
if ctr_id != "dev":
    cmd = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *sys.argv
    ]
    run(cmd)
    sys.exit(0)

# ----- Dependencies -----
try:
    import numpy as np
    import psycopg
    import psycopg_binary
    from psycopg.rows import dict_row
except ImportError as e:
    print(f"Error: Required module not found: {e}")
    print("Please ensure numpy, psycopg, and psycopg_binary are installed in the dev container")
    print("Install with: pip install numpy psycopg psycopg_binary")
    sys.exit(1)

# ----- Constants -----
# Default location of the second brain notes
DEFAULT_NOTES_DIR = os.path.expanduser("~/Documents/notes")
# Default table name in the vector database
DEFAULT_TABLE_NAME = "second_brain_vectors"
# Default AI provider for embeddings
DEFAULT_PROVIDER = "ollampy"
# Provider-specific defaults
PROVIDER_CONFIGS = {
    "openpy": {
        "dimension": 1536,
        "embedding_prompt": "IMPORTANT: Respond ONLY with a valid JSON object containing an 'embedding' key with an array of 1536 float numbers. No other text. Generate an embedding vector for the following text that captures semantic meaning for vector search. Format: {\"embedding\": [0.123, -0.456, ...]} with 1536 values.",
        "model": "text-embedding-3-small"
    },
    "claudpy": {
        "dimension": 1536,
        "embedding_prompt": "IMPORTANT: Respond ONLY with a valid JSON object containing an 'embedding' key with an array of 1536 float numbers. No other text. Generate an embedding vector for the following text that captures semantic meaning for vector search. Format: {\"embedding\": [0.123, -0.456, ...]} with 1536 values.",
        "model": "claude-3-haiku-20240307"
    },
    "grokpy": {
        "dimension": 1536,
        "embedding_prompt": "IMPORTANT: Respond ONLY with a valid JSON object containing an 'embedding' key with an array of 1536 float numbers. No other text. Generate an embedding vector for the following text that captures semantic meaning for vector search. Format: {\"embedding\": [0.123, -0.456, ...]} with 1536 values.",
        "model": "grok-3-mini-beta"
    },
    "geminpy": {
        "dimension": 768,
        "embedding_prompt": "IMPORTANT: Respond ONLY with a valid JSON object containing an 'embedding' key with an array of 768 float numbers. No other text. Generate an embedding vector for the following text that captures semantic meaning for vector search. Format: {\"embedding\": [0.123, -0.456, ...]} with 768 values.",
        "model": "gemini-1.5-pro"
    },
    "ollampy": {
        "dimension": 768,
        "embedding_prompt": "IMPORTANT: Respond ONLY with a valid JSON object containing an 'embedding' key with an array of 768 float numbers. No other text. Generate an embedding vector for the following text that captures semantic meaning for vector search. Format: {\"embedding\": [0.123, -0.456, ...]} with 768 values.",
        "model": "nomic-embed-text:v1.5"
    },
    "perpy": {
        "dimension": 1536,
        "embedding_prompt": "IMPORTANT: Respond ONLY with a valid JSON object containing an 'embedding' key with an array of 1536 float numbers. No other text. Generate an embedding vector for the following text that captures semantic meaning for vector search. Format: {\"embedding\": [0.123, -0.456, ...]} with 1536 values.",
        "model": "sonar-small-online"
    }
}

# ----- Command-line Argument Parsing -----
def parse_args():
    """Parse command-line arguments for the semantic search functionality."""
    parser = argparse.ArgumentParser(
        description="Semantic search across your second brain using vector embeddings."
    )
    
    # Main query argument
    parser.add_argument(
        "query", 
        help="The natural language query to search for", 
        nargs="?",  # Make it optional to allow for index operations
    )
    
    # Search options
    parser.add_argument(
        "--index", 
        help="Index your second brain content before searching", 
        action="store_true"
    )
    parser.add_argument(
        "--limit", 
        type=int, 
        default=5,
        help="Maximum number of results to return (default: 5)"
    )
    parser.add_argument(
        "--path", 
        default=DEFAULT_NOTES_DIR,
        help=f"Path to your second brain notes (default: {DEFAULT_NOTES_DIR})"
    )
    parser.add_argument(
        "--table", 
        default=DEFAULT_TABLE_NAME,
        help=f"Vector database table name (default: {DEFAULT_TABLE_NAME})"
    )
    parser.add_argument(
        "--provider", 
        choices=["openpy", "claudpy", "grokpy", "geminpy", "ollampy", "perpy"],
        default=DEFAULT_PROVIDER,
        help=f"AI provider script for embeddings (default: {DEFAULT_PROVIDER})"
    )
    parser.add_argument(
        "--model", 
        help="Embedding model to use (provider-specific default if not specified)"
    )
    parser.add_argument(
        "--mock", 
        action="store_true",
        help="Use mock embeddings for testing instead of real AI providers"
    )
    parser.add_argument(
        "--fallback-prompt", 
        action="store_true",
        help="Use prompt-based approach for embeddings instead of direct API calls"
    )
    parser.add_argument(
        "--threshold", 
        type=float, 
        default=0.75,
        help="Similarity threshold between 0 and 1 (default: 0.75, lower = more results)"
    )
    parser.add_argument(
        "--include-dirs", 
        nargs="+", 
        default=["01_projects", "02_areas", "03_resources"],
        help="Directories to include when indexing (default: 01_projects 02_areas 03_resources)"
    )
    parser.add_argument(
        "--exclude-dirs", 
        nargs="+", 
        default=["04_archives"],
        help="Directories to exclude when indexing (default: 04_archives)"
    )
    parser.add_argument(
        "--file-types", 
        nargs="+", 
        default=[".norg", ".md", ".txt"],
        help="File types to index (default: .norg .md .txt)"
    )
    
    return parser.parse_args()

# ----- Database Functions -----
def get_connection():
    """Connect to the PostgreSQL database with pgvector extension."""
    try:
        conn = psycopg.connect(
            host="127.0.0.1",
            port=5432,
            dbname="postgres",
            user="postgres",
            password="postgres",
            row_factory=dict_row
        )
        return conn
    except Exception as e:
        print(f"Error connecting to database: {e}")
        sys.exit(1)

def ensure_extension_and_table(conn, table_name, provider):
    """Ensure the pgvector extension is installed and the table exists."""
    # Get the vector dimension for the provider
    dimension = PROVIDER_CONFIGS[provider]["dimension"]
    
    with conn.cursor() as cur:
        # Create the vector extension if it doesn't exist
        cur.execute("CREATE EXTENSION IF NOT EXISTS vector")
        
        # Check if the table exists
        cur.execute(
            "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = %s)",
            (table_name,)
        )
        table_exists = cur.fetchone()["exists"]
        
        # Create the table if it doesn't exist
        if not table_exists:
            cur.execute(f"""
            CREATE TABLE {table_name} (
                id TEXT PRIMARY KEY,
                embedding VECTOR({dimension}),
                path TEXT NOT NULL,
                content_snippet TEXT,
                provider TEXT NOT NULL,
                model TEXT NOT NULL,
                metadata JSONB
            )
            """)
            print(f"Created table '{table_name}' for vector embeddings with dimension {dimension}")
        else:
            # Check if the table has the correct structure for storing provider info
            # This handles migration from older versions that didn't track provider/model
            try:
                cur.execute(f"SELECT provider FROM {table_name} LIMIT 0")
            except Exception:
                # Need to add provider and model columns
                print(f"Upgrading table '{table_name}' to store provider information...")
                cur.execute(f"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS provider TEXT")
                cur.execute(f"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS model TEXT")
                cur.execute(f"UPDATE {table_name} SET provider = %s, model = %s WHERE provider IS NULL", 
                           (DEFAULT_PROVIDER, PROVIDER_CONFIGS[DEFAULT_PROVIDER]["model"]))
                try:
                    cur.execute(f"ALTER TABLE {table_name} ALTER COLUMN provider SET NOT NULL")
                    cur.execute(f"ALTER TABLE {table_name} ALTER COLUMN model SET NOT NULL")
                except Exception as e:
                    print(f"Warning: Could not set NOT NULL constraint: {e}")
                    print("Table migration completed with warnings.")
                else:
                    print("Table upgrade complete.")
                
        conn.commit()

# ----- Embedding Functions -----
def generate_embedding(text, provider=DEFAULT_PROVIDER, model=None, use_fallback_prompt=False):
    """Generate an embedding vector for the given text using the specified AI provider.
    
    This function uses your existing AI provider scripts (grokpy, claudpy, etc.) with either:
    1. Direct embedding flags (--embedding and --json flags for direct API calls)
    2. A fallback prompt-based approach if the provider doesn't support direct embedding
    """
    # Get provider config
    if provider not in PROVIDER_CONFIGS:
        print(f"Error: Provider '{provider}' not supported")
        print(f"Supported providers: {', '.join(PROVIDER_CONFIGS.keys())}")
        sys.exit(1)
        
    config = PROVIDER_CONFIGS[provider]
    
    # Use default model for provider if not specified
    if not model:
        model = config["model"]
        
    # For testing/development with a mock embedding
    if os.environ.get("SEMANTIC_SEARCH_MOCK_EMBEDDING") == "1":
        print(f"[MOCK] Using mock embedding for {provider}")
        dimension = config["dimension"]
        import random
        mock_embedding = [random.uniform(-1, 1) for _ in range(dimension)]
        return mock_embedding
    
    # Provider script is directly specified
    script = provider
    
    # Ensure the script exists in our configuration
    if provider not in PROVIDER_CONFIGS:
        print(f"Error: Provider '{provider}' not supported")
        return None
    
    # Choose between direct API call or prompt-based approach
    if use_fallback_prompt:
        return _generate_embedding_via_prompt(text, provider, model, script, config)
    else:
        return _generate_embedding_via_api(text, provider, model, script, config)

def _generate_embedding_via_api(text, provider, model, script, config):
    """Generate embedding using direct API calls with --embedding and --json flags."""
    try:
        # Construct the command to call the appropriate script with embedding flags
        cmd = [script, "--embedding", "--json"]
        
        # Add model parameter for scripts that support it
        if model:
            cmd.extend(["--model", model])
        
        # Run the command with the text as input
        print(f"Running: {' '.join(cmd)}")
        result = run(
            cmd,
            input=text.encode('utf-8'),
            stdout=PIPE,
            stderr=PIPE
        )
        
        # Check for errors
        if result.returncode != 0:
            stderr = result.stderr.decode('utf-8').strip()
            print(f"Error running {script}: {stderr}")
            return None
        
        # Parse the output JSON
        output = result.stdout.decode('utf-8').strip()
        
        # Handle truncated JSON output containing "embedding" key
        if '"embedding"' in output:
            try:
                # Try to extract the numeric values from a potentially truncated array
                import re
                
                # Find the beginning of the embedding array
                array_start_match = re.search(r'"embedding"\s*:\s*\[\s*', output)
                if array_start_match:
                    # Extract from the start of the array to the end of the available text
                    array_text = output[array_start_match.end():]
                    
                    # Parse the numbers (both integers and floats)
                    number_pattern = r'-?\d+(?:\.\d+)?'
                    numbers = re.findall(number_pattern, array_text)
                    
                    if numbers:
                        # Convert strings to floats
                        embedding = [float(n) for n in numbers]
                        
                        # For ollampy specifically, check if we have exactly 769 values (common issue)
                        # In this case, take the first 768 values as this is likely just an extra token
                        if provider == "ollampy" and len(embedding) == 769:
                            print(f"Detected ollampy with 769 values, automatically trimming to {config['dimension']}")
                            embedding = embedding[:config["dimension"]]
                        # Generate any missing values if needed
                        elif len(embedding) < config["dimension"]:
                            print(f"Warning: Found only {len(embedding)} values, generating {config['dimension'] - len(embedding)} random values")
                            import random
                            embedding.extend([random.uniform(-0.1, 0.1) for _ in range(config["dimension"] - len(embedding))])
                        # Trim if too many values
                        elif len(embedding) > config["dimension"]:
                            print(f"Warning: Found {len(embedding)} values, trimming to {config['dimension']}")
                            embedding = embedding[:config["dimension"]]
                        
                        return embedding
            except Exception as e:
                print(f"Error trying to parse numbers from truncated JSON: {e}")
                
        # Try standard JSON parsing methods
        try:
            # Try direct JSON parsing
            embedding_data = json.loads(output)
            if "embedding" in embedding_data and isinstance(embedding_data["embedding"], list):
                embedding = embedding_data["embedding"]
                
                # For ollampy specifically, check if we have exactly 769 values (common issue)
                if provider == "ollampy" and len(embedding) == 769:
                    print(f"Detected ollampy with 769 values, automatically trimming to {config['dimension']}")
                    embedding = embedding[:config["dimension"]]
                # Verify dimension for other cases
                elif len(embedding) != config["dimension"]:
                    print(f"Warning: Expected dimension {config['dimension']} but got {len(embedding)}")
                    # If too many values, trim to expected dimension
                    if len(embedding) > config["dimension"]:
                        embedding = embedding[:config["dimension"]]
                    # If too few values, pad with small random values
                    elif len(embedding) < config["dimension"]:
                        import random
                        embedding.extend([random.uniform(-0.1, 0.1) for _ in range(config["dimension"] - len(embedding))])
                
                return embedding
        except json.JSONDecodeError:
            # If not valid JSON, try to find JSON in the output
            import re
            json_pattern = r'\{\s*"embedding"\s*:\s*\[.*?\]\s*\}'
            match = re.search(json_pattern, output)
            
            if match:
                try:
                    embedding_json = match.group(0)
                    embedding_data = json.loads(embedding_json)
                    embedding = embedding_data["embedding"]
                    
                    # For ollampy specifically, check if we have exactly 769 values (common issue)
                    if provider == "ollampy" and len(embedding) == 769:
                        print(f"Detected ollampy with 769 values, automatically trimming to {config['dimension']}")
                        embedding = embedding[:config["dimension"]]
                    # Verify dimension for other cases
                    elif len(embedding) != config["dimension"]:
                        print(f"Warning: Expected dimension {config['dimension']} but got {len(embedding)}")
                        # If too many values, trim to expected dimension
                        if len(embedding) > config["dimension"]:
                            embedding = embedding[:config["dimension"]]
                        # If too few values, pad with small random values
                        elif len(embedding) < config["dimension"]:
                            import random
                            embedding.extend([random.uniform(-0.1, 0.1) for _ in range(config["dimension"] - len(embedding))])
                    
                    return embedding
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"Error parsing embedding JSON: {e}")
        
        print(f"Error: Could not parse embedding from output. First 200 chars: {output[:200]}...")
        return None
        
    except Exception as e:
        print(f"Error generating embedding with {provider}: {e}")
        if hasattr(e, 'stderr') and e.stderr:
            print(f"Error details: {e.stderr.decode('utf-8')}")
        return None

def _generate_embedding_via_prompt(text, provider, model, script, config):
    """Fallback method that uses a prompt to get embeddings."""
    # Create a temporary file with the embedding prompt and text
    with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp:
        prompt = config["embedding_prompt"]
        content = f"{prompt}\n\nText to embed:\n{text}"
        temp.write(content)
        temp_path = temp.name
    
    try:
        # Construct the command to call the appropriate script
        cmd = [script]
        
        # Add model parameter for scripts that support it
        if model:
            cmd.extend(["--model", model])
        
        # Run the command with the prompt + text as input via the temp file
        print(f"Running (fallback prompt mode): {' '.join(cmd)}")
        with open(temp_path, 'r') as temp_file:
            result = run(
                cmd,
                stdin=temp_file,
                stdout=PIPE,
                stderr=PIPE
            )
        
        # Check for errors
        if result.returncode != 0:
            stderr = result.stderr.decode('utf-8').strip()
            print(f"Error running {script}: {stderr}")
            return None
        
        # Parse the output to extract the JSON object with the embedding
        output = result.stdout.decode('utf-8').strip()
        
        # Try multiple approaches to find and parse valid JSON in the output
        
        # First try: direct JSON parsing of the entire output
        try:
            data = json.loads(output)
            if "embedding" in data and isinstance(data["embedding"], list):
                embedding = data["embedding"]
                
                # For ollampy specifically, check if we have exactly 769 values (common issue)
                if provider == "ollampy" and len(embedding) == 769:
                    print(f"Detected ollampy with 769 values, automatically trimming to {config['dimension']}")
                    embedding = embedding[:config["dimension"]]
                # Handle other dimension mismatches
                elif len(embedding) != config["dimension"]:
                    print(f"Warning: Expected dimension {config['dimension']} but got {len(embedding)}")
                    # If too many values, trim to expected dimension
                    if len(embedding) > config["dimension"]:
                        embedding = embedding[:config["dimension"]]
                    # If too few values, pad with small random values
                    elif len(embedding) < config["dimension"]:
                        import random
                        embedding.extend([random.uniform(-0.1, 0.1) for _ in range(config["dimension"] - len(embedding))])
                        
                return embedding
        except json.JSONDecodeError:
            # If the whole output isn't valid JSON, continue to other methods
            pass
            
        # Second try: Find a JSON object using regex
        import re
        json_pattern = r'\{\s*"embedding"\s*:\s*\[.*?\]\s*\}'
        match = re.search(json_pattern, output)
        
        if match:
            try:
                embedding_json = match.group(0)
                embedding_data = json.loads(embedding_json)
                embedding = embedding_data["embedding"]
                
                # For ollampy specifically, check if we have exactly 769 values (common issue)
                if provider == "ollampy" and len(embedding) == 769:
                    print(f"Detected ollampy with 769 values, automatically trimming to {config['dimension']}")
                    embedding = embedding[:config["dimension"]]
                # Handle other dimension mismatches
                elif len(embedding) != config["dimension"]:
                    print(f"Warning: Expected dimension {config['dimension']} but got {len(embedding)}")
                    # If too many values, trim to expected dimension
                    if len(embedding) > config["dimension"]:
                        embedding = embedding[:config["dimension"]]
                    # If too few values, pad with small random values
                    elif len(embedding) < config["dimension"]:
                        import random
                        embedding.extend([random.uniform(-0.1, 0.1) for _ in range(config["dimension"] - len(embedding))])
                
                return embedding
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Error parsing embedding JSON: {e}")
        
        # Third try: Look for any array in the output
        array_pattern = r'\[\s*-?\d+(\.\d+)?\s*,\s*-?\d+(\.\d+)?\s*,.*?\]'
        array_match = re.search(array_pattern, output)
        
        if array_match:
            try:
                array_json = array_match.group(0)
                embedding = json.loads(array_json)
                
                # Verify it's a list of numbers with appropriate dimension
                if isinstance(embedding, list) and all(isinstance(x, (int, float)) for x in embedding):
                    # For ollampy specifically, check if we have exactly 769 values (common issue)
                    if provider == "ollampy" and len(embedding) == 769:
                        print(f"Detected ollampy with 769 values, automatically trimming to {config['dimension']}")
                        embedding = embedding[:config["dimension"]]
                    # Handle other dimension mismatches
                    elif len(embedding) != config["dimension"]:
                        print(f"Warning: Expected dimension {config['dimension']} but got {len(embedding)}")
                        # If too many values, trim to expected dimension
                        if len(embedding) > config["dimension"]:
                            embedding = embedding[:config["dimension"]]
                        # If too few values, pad with small random values
                        elif len(embedding) < config["dimension"]:
                            import random
                            embedding.extend([random.uniform(-0.1, 0.1) for _ in range(config["dimension"] - len(embedding))])
                    
                    return embedding
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Error parsing array JSON: {e}")
        
        # If all attempts fail, provide helpful error message
        print("Error: Could not find valid embedding JSON in the output")
        print(f"Output preview: {output[:200]}...")
        print("Try using the --mock flag for testing")
        
        return None
        
    except Exception as e:
        print(f"Error generating embedding with {provider}: {e}")
        if hasattr(e, 'stderr') and e.stderr:
            print(f"Error details: {e.stderr.decode('utf-8')}")
        return None
    finally:
        # Clean up the temporary file
        if os.path.exists(temp_path):
            os.unlink(temp_path)

def get_file_content(file_path):
    """Read and return the content of a file, handling different encodings."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except UnicodeDecodeError:
        try:
            with open(file_path, 'r', encoding='latin-1') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
            return None
    except (FileNotFoundError, OSError) as e:
        # Handle files with spaces or special characters in the path
        try:
            # Try using the raw string representation of the path
            path_str = str(file_path)
            with open(path_str, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e2:
            print(f"Error reading file {file_path}: {e2}")
            return None

def create_content_snippet(content, max_length=300):
    """Create a snippet of the content for preview purposes."""
    if not content:
        return ""
    
    # For now, just take the first part of the content
    # This could be improved to extract more meaningful snippets
    snippet = content[:max_length]
    if len(content) > max_length:
        snippet += "..."
    
    return snippet

def index_files(args, conn, table_name):
    """Index files from the second brain into the vector database."""
    base_path = Path(args.path)
    
    # Validate the path exists
    if not base_path.exists() or not base_path.is_dir():
        print(f"Error: Path {base_path} does not exist or is not a directory")
        return
    
    print(f"Indexing files in {base_path}...")
    
    # Get provider configuration
    provider = args.provider
    model = args.model or PROVIDER_CONFIGS[provider]["model"]
    print(f"Using {provider} provider with model {model}")
    
    # Find all files matching the criteria
    files_to_index = []
    
    for include_dir in args.include_dirs:
        # Handle directory paths that might have spaces
        if isinstance(include_dir, str) and ' ' in include_dir:
            print(f"Warning: Directory path '{include_dir}' contains spaces. Trying both formats.")
            # Try the path as provided and with underscores instead of spaces
            dir_paths = [
                base_path / include_dir,
                base_path / include_dir.replace(' ', '_')
            ]
        else:
            dir_paths = [base_path / include_dir]
        
        for dir_path in dir_paths:
            if not dir_path.exists():
                continue
                
            for file_type in args.file_types:
                # Use recursive glob to find all matching files
                matches = list(dir_path.glob(f"**/*{file_type}"))
                files_to_index.extend(matches)
    
    # Remove files from excluded directories
    for exclude_dir in args.exclude_dirs:
        exclude_path = str(base_path / exclude_dir)
        # Also check for underscore version
        exclude_path_alt = str(base_path / exclude_dir.replace(' ', '_'))
        
        files_to_index = [f for f in files_to_index if not (str(f).startswith(exclude_path) or 
                                                              str(f).startswith(exclude_path_alt))]
    
    total_files = len(files_to_index)
    print(f"Found {total_files} files to index")
    
    # Process each file
    for i, file_path in enumerate(files_to_index):
        rel_path = file_path.relative_to(base_path)
        file_id = str(rel_path).replace("/", "_").replace(".", "_")
        
        # Check if this file is already indexed with current provider/model
        with conn.cursor() as cur:
            try:
                cur.execute(
                    f"SELECT id FROM {table_name} WHERE id = %s AND provider = %s AND model = %s",
                    (file_id, provider, model)
                )
                exists = cur.fetchone() is not None
            except Exception:
                # Table might not have provider/model columns yet
                try:
                    cur.execute(f"SELECT id FROM {table_name} WHERE id = %s", (file_id,))
                    exists = cur.fetchone() is not None
                except Exception:
                    exists = False
            
            if exists:
                print(f"[{i+1}/{total_files}] Already indexed: {rel_path}")
                continue
        
        # Read the file content
        content = get_file_content(file_path)
        if not content:
            print(f"[{i+1}/{total_files}] Skipping (empty or unreadable): {rel_path}")
            continue
            
        # Generate a snippet for preview
        snippet = create_content_snippet(content)
        
        # Generate embedding (using fallback prompt mode if requested)
        embedding = generate_embedding(content, provider, model, args.fallback_prompt)
        if not embedding:
            print(f"[{i+1}/{total_files}] Failed to generate embedding: {rel_path}")
            continue
        
        # Create metadata
        try:
            last_modified = os.path.getmtime(file_path)
        except (FileNotFoundError, OSError):
            # Handle files with spaces or special characters in the path
            print(f"Warning: Could not get modification time for {file_path}, using current time")
            last_modified = time.time()
            
        metadata = {
            "filename": file_path.name,
            "file_type": file_path.suffix,
            "size": len(content),
            "last_modified": last_modified
        }
        
        # Insert or update in database
        with conn.cursor() as cur:
            # Check if we have this file with a different provider/model
            try:
                cur.execute(f"SELECT id FROM {table_name} WHERE id = %s", (file_id,))
                exists_with_diff_provider = cur.fetchone() is not None
            except Exception:
                exists_with_diff_provider = False
                
            if exists_with_diff_provider:
                # Update existing record
                # Use embedding directly without converting to numpy array
                cur.execute(
                    f"UPDATE {table_name} SET embedding = %s, content_snippet = %s, "
                    f"provider = %s, model = %s, metadata = %s WHERE id = %s",
                    (embedding, snippet, provider, model, json.dumps(metadata), file_id)
                )
                print(f"[{i+1}/{total_files}] Updated: {rel_path}")
            else:
                # Insert new record
                # Convert embedding to a list to avoid numpy array serialization issues
                cur.execute(
                    f"INSERT INTO {table_name} "
                    f"(id, embedding, path, content_snippet, provider, model, metadata) "
                    f"VALUES (%s, %s, %s, %s, %s, %s, %s)",
                    (file_id, embedding, str(rel_path), snippet, 
                     provider, model, json.dumps(metadata))
                )
                print(f"[{i+1}/{total_files}] Indexed: {rel_path}")
            conn.commit()
    
    print(f"Indexing complete. {total_files} files processed.")

def search_similar(args, conn, table_name, query):
    """Search for content similar to the query."""
    # Get provider configuration
    provider = args.provider
    model = args.model or PROVIDER_CONFIGS[provider]["model"]
    
    print(f"Generating embedding for query using {provider} provider with model {model}")
    query_embedding = generate_embedding(query, provider, model, args.fallback_prompt)
    
    if not query_embedding:
        print("Failed to generate embedding for the query")
        return
    
    print(f"Searching for similar content in {table_name}...")
    
    with conn.cursor() as cur:
        # First check if we have any content indexed with this provider/model
        cur.execute(
            f"SELECT COUNT(*) AS count FROM {table_name} WHERE provider = %s AND model = %s",
            (provider, model)
        )
        count = cur.fetchone()["count"]
        
        if count == 0:
            print(f"No content indexed with provider={provider}, model={model}.")
            # Check if we have any content at all
            cur.execute(f"SELECT DISTINCT provider, model FROM {table_name}")
            available = cur.fetchall()
            if available:
                print("Available provider/model combinations:")
                for combo in available:
                    print(f"  - {combo['provider']} / {combo['model']}")
                print("Tip: Use --provider and --model to match your search with indexed content.")
            else:
                print("No content indexed. Use --index to add content first.")
            return
            
        # Perform the similarity search
        # Note: We need to cast the Python list to a PostgreSQL vector type
        query_sql = f"""
        SELECT id, path, content_snippet, embedding <=> %s::vector AS distance, metadata, provider, model
        FROM {table_name}
        WHERE provider = %s AND model = %s AND embedding <=> %s::vector <= %s
        ORDER BY distance
        LIMIT %s
        """
        
        # Convert the embedding list to a string representation for PostgreSQL vector casting
        # Format: '[val1,val2,val3,...]'
        vector_str = '[' + ','.join(str(x) for x in query_embedding) + ']'
        
        cur.execute(
            query_sql,
            (
                vector_str,
                provider,
                model,
                vector_str,
                args.threshold,
                args.limit
            )
        )
        
        results = cur.fetchall()
    
    if not results:
        print("No similar content found. Try adjusting the threshold or indexing more content.")
        return
    
    print(f"\nFound {len(results)} matches:")
    print("-" * 80)
    
    for i, result in enumerate(results):
        similarity = 1 - result["distance"]  # Convert distance to similarity
        
        print(f"{i+1}. {result['path']} (Similarity: {similarity:.2f})")
        print(f"   Snippet: {result['content_snippet']}")
        print(f"   Provider: {result['provider']}, Model: {result['model']}")
        
        # Try to extract and display relevant metadata
        if result["metadata"]:
            # Handle both string JSON and dictionary metadata formats
            try:
                # If metadata is already a dict (from newer psycopg versions)
                if isinstance(result["metadata"], dict):
                    metadata = result["metadata"]
                # If metadata is a JSON string (from older psycopg versions)
                else:
                    metadata = json.loads(result["metadata"])
                    
                if "last_modified" in metadata:
                    from datetime import datetime
                    last_modified = datetime.fromtimestamp(metadata["last_modified"]).strftime("%Y-%m-%d %H:%M:%S")
                    print(f"   Last modified: {last_modified}")
                
                if "size" in metadata:
                    print(f"   Size: {metadata['size']} bytes")
            except (json.JSONDecodeError, TypeError) as e:
                print(f"   Note: Could not parse metadata: {e}")
                print(f"   Raw metadata: {str(result['metadata'])[:50]}...")
        else:
            print("   No metadata available")
                
        print("-" * 80)
    
    # Display the full path to the most relevant result
    if results:
        most_relevant = results[0]
        full_path = os.path.join(args.path, most_relevant["path"])
        print(f"Most relevant file: {full_path}")

def main():
    """Main function to handle semantic search operations."""
    args = parse_args()
    
    # Verify provider is valid
    if args.provider not in PROVIDER_CONFIGS:
        print(f"Error: Provider '{args.provider}' not supported")
        print(f"Supported providers: {', '.join(PROVIDER_CONFIGS.keys())}")
        sys.exit(1)
    
    # Set mock embedding environment variable if --mock flag is used
    if args.mock:
        os.environ["SEMANTIC_SEARCH_MOCK_EMBEDDING"] = "1"
        print("Using mock embeddings for testing")
    
    # Connect to the database
    conn = get_connection()
    
    # Ensure the pgvector extension and table exist
    ensure_extension_and_table(conn, args.table, args.provider)
    
    # Handle indexing if requested
    if args.index:
        index_files(args, conn, args.table)
        
        # If no query provided, exit after indexing
        if not args.query:
            conn.close()
            return
    
    # Perform the search if a query is provided
    if args.query:
        search_similar(args, conn, args.table, args.query)
    elif not args.index:
        # No query and no index flag, show help
        print("Please provide a search query or use --index to index your second brain")
        print("Run with --help for more information")
    
    # Close the database connection
    conn.close()

if __name__ == "__main__":
    main()
