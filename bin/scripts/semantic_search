#!/usr/bin/python3

# dotfiles - Personal configuration files and scripts
# Copyright (C) 2025  Zach Podbielniak
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.


"""
semantic_search - Semantic search over your second brain using document chunking and hybrid search.

This script provides:
- Document chunking with configurable size and overlap
- Ollama-based embeddings (nomic-embed-text:v1.5)
- Hybrid search combining vector similarity and BM25 keyword search
- Reciprocal Rank Fusion for combining search results
"""

import os
import sys
import argparse
import json
import time
import re
import hashlib
import yaml
import shlex
from subprocess import run, PIPE, Popen
from pathlib import Path
from typing import Optional

# ----- Distrobox Container Check -----
ctr_id: str = os.environ.get("CONTAINER_ID", "")
no_dbox_check: bool = os.environ.get("NO_DBOX_CHECK", "").lower() in ("1", "true")

if not no_dbox_check and ctr_id != "dev":
    cmd: list[str] = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *sys.argv
    ]
    run(cmd)
    sys.exit(0)

# ----- Dependencies -----
try:
    import numpy as np
    import psycopg
    import psycopg_binary
    from psycopg.rows import dict_row
    import requests
except ImportError as e:
    print(f"Error: Required module not found: {e}")
    print("Please ensure numpy, psycopg, psycopg_binary, and requests are installed")
    print("Install with: pip install numpy psycopg psycopg_binary requests")
    sys.exit(1)

# ----- Constants -----
DEFAULT_NOTES_DIR: str = os.path.expanduser("~/Documents/notes")
DEFAULT_TABLE_NAME: str = "second_brain_chunks"
EMBEDDING_MODEL: str = "nomic-embed-text:v1.5"
EMBEDDING_DIMENSION: int = 768
OLLAMA_ENDPOINT: str = os.environ.get("OLLAMPY_ENDPOINT", "http://localhost:11434")

# Chunking defaults
DEFAULT_CHUNK_SIZE: int = 512  # tokens
DEFAULT_CHUNK_OVERLAP: int = 128  # tokens
CHARS_PER_TOKEN: float = 4.0  # approximate

# Search defaults
DEFAULT_LIMIT: int = 10
DEFAULT_THRESHOLD: float = 0.45  # Filter low-quality vector matches
RRF_K: int = 60  # Reciprocal Rank Fusion constant
KEYWORD_BOOST: float = 1.5  # Boost keyword matches in RRF (exact terms are strong signal)


# ----- Argument Parsing -----
def parse_args() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Semantic search across your second brain using hybrid vector + keyword search.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  semantic_search "how to configure neovim"
  semantic_search --index
  semantic_search --reindex
  semantic_search --limit 20 --threshold 0.3 "bash scripting tips"
  semantic_search --yaml "python async patterns"
  semantic_search --simple --full-path "health goals"
"""
    )

    # Main query argument
    parser.add_argument(
        "query",
        help="The natural language query to search for",
        nargs="?",
    )

    # Indexing options
    indexing_group = parser.add_argument_group("Indexing")
    indexing_group.add_argument(
        "--index",
        action="store_true",
        help="Build/update the vector index"
    )
    indexing_group.add_argument(
        "--reindex",
        action="store_true",
        help="Force full reindex (drop existing data)"
    )
    indexing_group.add_argument(
        "--chunk-size",
        type=int,
        default=DEFAULT_CHUNK_SIZE,
        metavar="N",
        help=f"Chunk size in tokens (default: {DEFAULT_CHUNK_SIZE})"
    )
    indexing_group.add_argument(
        "--chunk-overlap",
        type=int,
        default=DEFAULT_CHUNK_OVERLAP,
        metavar="N",
        help=f"Overlap between chunks in tokens (default: {DEFAULT_CHUNK_OVERLAP})"
    )

    # Search options
    search_group = parser.add_argument_group("Searching")
    search_group.add_argument(
        "--limit",
        type=int,
        default=DEFAULT_LIMIT,
        metavar="N",
        help=f"Number of results (default: {DEFAULT_LIMIT})"
    )
    search_group.add_argument(
        "--threshold",
        type=float,
        default=DEFAULT_THRESHOLD,
        metavar="FLOAT",
        help=f"Minimum similarity threshold 0-1 (default: {DEFAULT_THRESHOLD})"
    )
    search_group.add_argument(
        "--keyword-only",
        action="store_true",
        help="Use only BM25 keyword search"
    )
    search_group.add_argument(
        "--vector-only",
        action="store_true",
        help="Use only vector similarity search"
    )

    # Output options
    output_group = parser.add_argument_group("Output")
    output_group.add_argument(
        "--simple",
        action="store_true",
        help="Output only file paths"
    )
    output_group.add_argument(
        "--full-path",
        action="store_true",
        help="With --simple, output absolute paths"
    )
    output_group.add_argument(
        "--yaml",
        action="store_true",
        help="YAML formatted output"
    )
    output_group.add_argument(
        "--json",
        action="store_true",
        help="JSON formatted output"
    )
    output_group.add_argument(
        "--show-chunks",
        action="store_true",
        help="Show matched chunk content in output"
    )
    output_group.add_argument(
        "--silent",
        action="store_true",
        help="Hide progress messages"
    )
    output_group.add_argument(
        "--open",
        action="store_true",
        help="Open the most relevant result in editor"
    )
    output_group.add_argument(
        "--editor",
        help="Editor to use (default: $EDITOR)"
    )

    # Path options
    path_group = parser.add_argument_group("Paths")
    path_group.add_argument(
        "--path",
        default=DEFAULT_NOTES_DIR,
        help=f"Notes directory (default: {DEFAULT_NOTES_DIR})"
    )
    path_group.add_argument(
        "--include-dirs",
        nargs="+",
        default=["00_inbox", "01_projects", "02_areas", "03_resources", "04_archives"],
        help="Directories to include"
    )
    path_group.add_argument(
        "--exclude-dirs",
        nargs="+",
        default=[],
        help="Directories to exclude"
    )
    path_group.add_argument(
        "--file-types",
        nargs="+",
        default=[".norg", ".md", ".txt"],
        help="File extensions to index"
    )

    # Other options
    parser.add_argument(
        "--table",
        default=DEFAULT_TABLE_NAME,
        help=f"Database table name (default: {DEFAULT_TABLE_NAME})"
    )
    parser.add_argument(
        "--license",
        action="store_true",
        help="Show license information"
    )

    return parser.parse_args()


# ----- Database Functions -----
def get_connection() -> psycopg.Connection:
    """Connect to PostgreSQL database."""
    try:
        conn = psycopg.connect(
            host="127.0.0.1",
            port=5432,
            dbname="postgres",
            user="postgres",
            password="postgres",
            row_factory=dict_row
        )
        return conn
    except Exception as e:
        print(f"Error connecting to database: {e}", file=sys.stderr)
        sys.exit(1)


def ensure_schema(conn: psycopg.Connection, table_name: str) -> None:
    """Ensure pgvector extension and table schema exist."""
    with conn.cursor() as cur:
        # Create vector extension
        cur.execute("CREATE EXTENSION IF NOT EXISTS vector")

        # Create chunks table with new schema
        cur.execute(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id SERIAL PRIMARY KEY,
            file_path TEXT NOT NULL,
            chunk_index INTEGER NOT NULL,
            content TEXT NOT NULL,
            embedding VECTOR({EMBEDDING_DIMENSION}),
            content_tsv TSVECTOR,
            file_hash TEXT,
            metadata JSONB,
            created_at TIMESTAMP DEFAULT NOW(),
            UNIQUE(file_path, chunk_index)
        )
        """)

        # Create indexes for performance
        cur.execute(f"""
        CREATE INDEX IF NOT EXISTS {table_name}_embedding_idx
        ON {table_name} USING ivfflat (embedding vector_cosine_ops)
        WITH (lists = 100)
        """)
        cur.execute(f"""
        CREATE INDEX IF NOT EXISTS {table_name}_content_tsv_idx
        ON {table_name} USING gin (content_tsv)
        """)
        cur.execute(f"""
        CREATE INDEX IF NOT EXISTS {table_name}_file_path_idx
        ON {table_name} (file_path)
        """)

        conn.commit()


def drop_table(conn: psycopg.Connection, table_name: str) -> None:
    """Drop the chunks table for reindexing."""
    with conn.cursor() as cur:
        cur.execute(f"DROP TABLE IF EXISTS {table_name}")
        conn.commit()


# ----- Chunking Functions -----
def estimate_tokens(text: str) -> int:
    """Estimate token count from text (rough approximation)."""
    return int(len(text) / CHARS_PER_TOKEN)


def tokens_to_chars(tokens: int) -> int:
    """Convert token count to approximate character count."""
    return int(tokens * CHARS_PER_TOKEN)


def find_section_boundaries(text: str) -> list[int]:
    """
    Find natural section boundaries in markdown/norg text.
    Returns list of character positions where sections start.
    """
    boundaries: list[int] = [0]

    # Match markdown headers (# ## ### etc) and norg headers (* ** *** etc)
    header_pattern = re.compile(r'^(?:#{1,6}\s|[*]{1,6}\s)', re.MULTILINE)

    for match in header_pattern.finditer(text):
        boundaries.append(match.start())

    # Also consider double newlines as potential boundaries
    double_newline_pattern = re.compile(r'\n\n+')
    for match in double_newline_pattern.finditer(text):
        boundaries.append(match.end())

    return sorted(set(boundaries))


def chunk_document(
    content: str,
    chunk_size_tokens: int = DEFAULT_CHUNK_SIZE,
    overlap_tokens: int = DEFAULT_CHUNK_OVERLAP
) -> list[dict]:
    """
    Split document into overlapping chunks respecting section boundaries.

    Args:
        content: The document text to chunk
        chunk_size_tokens: Target chunk size in tokens
        overlap_tokens: Overlap between chunks in tokens

    Returns:
        List of chunk dictionaries with content and metadata
    """
    if not content or not content.strip():
        return []

    chunk_size_chars: int = tokens_to_chars(chunk_size_tokens)
    overlap_chars: int = tokens_to_chars(overlap_tokens)

    # Find natural boundaries
    boundaries: list[int] = find_section_boundaries(content)

    chunks: list[dict] = []
    start: int = 0
    chunk_index: int = 0

    while start < len(content):
        # Calculate end position
        end: int = start + chunk_size_chars

        if end >= len(content):
            # Last chunk - take everything remaining
            end = len(content)
        else:
            # Try to find a natural boundary near the end
            best_boundary: Optional[int] = None
            min_dist: int = chunk_size_chars // 4  # Allow up to 25% deviation

            for boundary in boundaries:
                if start < boundary <= end + min_dist:
                    dist: int = abs(boundary - end)
                    if dist < min_dist:
                        min_dist = dist
                        best_boundary = boundary

            if best_boundary:
                end = best_boundary

        # Extract chunk content
        chunk_content: str = content[start:end].strip()

        if chunk_content:
            chunks.append({
                "chunk_index": chunk_index,
                "content": chunk_content,
                "start_char": start,
                "end_char": end,
                "estimated_tokens": estimate_tokens(chunk_content)
            })
            chunk_index += 1

        # Move start position with overlap
        if end >= len(content):
            break

        start = end - overlap_chars
        if start <= chunks[-1]["start_char"] if chunks else 0:
            # Prevent infinite loop
            start = end

    return chunks


# ----- Embedding Functions -----
def generate_embedding(text: str, max_retries: int = 3) -> Optional[list[float]]:
    """
    Generate embedding using Ollama's nomic-embed-text model.

    Args:
        text: Text to embed
        max_retries: Number of retries on failure

    Returns:
        List of floats representing the embedding, or None on failure
    """
    url: str = f"{OLLAMA_ENDPOINT}/api/embeddings"

    # Clean text
    text = text.replace('\x00', '')  # Remove NUL bytes

    data: dict = {
        "model": EMBEDDING_MODEL,
        "prompt": text
    }

    for attempt in range(max_retries):
        try:
            response = requests.post(url, json=data, timeout=30)

            if response.status_code == 200:
                result: dict = response.json()
                if "embedding" in result:
                    embedding: list[float] = result["embedding"]
                    # Verify dimension
                    if len(embedding) == EMBEDDING_DIMENSION:
                        return embedding
                    else:
                        print(f"Warning: Got {len(embedding)} dimensions, expected {EMBEDDING_DIMENSION}", file=sys.stderr)
                        return embedding[:EMBEDDING_DIMENSION] if len(embedding) > EMBEDDING_DIMENSION else None

            if attempt < max_retries - 1:
                time.sleep(1 * (attempt + 1))  # Exponential backoff

        except requests.RequestException as e:
            if attempt < max_retries - 1:
                time.sleep(1 * (attempt + 1))
            else:
                print(f"Error generating embedding: {e}", file=sys.stderr)

    return None


# ----- File Processing Functions -----
def get_file_hash(file_path: Path) -> str:
    """Generate hash of file path and modification time for change detection."""
    try:
        mtime: float = os.path.getmtime(file_path)
        hash_input: str = f"{file_path}:{mtime}"
        return hashlib.md5(hash_input.encode()).hexdigest()
    except OSError:
        return hashlib.md5(str(file_path).encode()).hexdigest()


def read_file_content(file_path: Path) -> Optional[str]:
    """Read file content handling different encodings."""
    encodings: list[str] = ['utf-8', 'latin-1', 'cp1252']

    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                content: str = f.read()
                # Remove NUL bytes
                return content.replace('\x00', '')
        except (UnicodeDecodeError, OSError):
            continue

    return None


def find_files_to_index(
    base_path: Path,
    include_dirs: list[str],
    exclude_dirs: list[str],
    file_types: list[str]
) -> list[Path]:
    """Find all files matching the indexing criteria."""
    files: list[Path] = []

    for include_dir in include_dirs:
        dir_path: Path = base_path / include_dir
        if not dir_path.exists():
            continue

        for file_type in file_types:
            for file_path in dir_path.glob(f"**/*{file_type}"):
                # Check exclusions
                excluded: bool = False
                for exclude_dir in exclude_dirs:
                    if exclude_dir in str(file_path):
                        excluded = True
                        break

                # Skip index files
                if file_path.name in ["00_index.norg", "00_index.md"]:
                    continue

                if not excluded:
                    files.append(file_path)

    return files


def check_file_needs_update(
    conn: psycopg.Connection,
    table_name: str,
    file_path: str,
    file_hash: str
) -> bool:
    """Check if file needs to be reindexed based on hash."""
    with conn.cursor() as cur:
        cur.execute(
            f"SELECT file_hash FROM {table_name} WHERE file_path = %s LIMIT 1",
            (file_path,)
        )
        result = cur.fetchone()

        if result is None:
            return True  # New file
        return result["file_hash"] != file_hash  # Changed file


def delete_file_chunks(conn: psycopg.Connection, table_name: str, file_path: str) -> None:
    """Delete all chunks for a specific file."""
    with conn.cursor() as cur:
        cur.execute(f"DELETE FROM {table_name} WHERE file_path = %s", (file_path,))
        conn.commit()


def insert_chunk(
    conn: psycopg.Connection,
    table_name: str,
    file_path: str,
    chunk: dict,
    embedding: list[float],
    file_hash: str,
    metadata: dict
) -> None:
    """Insert a chunk into the database."""
    content: str = chunk["content"]

    with conn.cursor() as cur:
        # Convert embedding to vector string format
        embedding_str: str = '[' + ','.join(str(x) for x in embedding) + ']'

        cur.execute(f"""
        INSERT INTO {table_name}
        (file_path, chunk_index, content, embedding, content_tsv, file_hash, metadata)
        VALUES (%s, %s, %s, %s::vector, to_tsvector('english', %s), %s, %s)
        ON CONFLICT (file_path, chunk_index) DO UPDATE SET
            content = EXCLUDED.content,
            embedding = EXCLUDED.embedding,
            content_tsv = EXCLUDED.content_tsv,
            file_hash = EXCLUDED.file_hash,
            metadata = EXCLUDED.metadata,
            created_at = NOW()
        """, (
            file_path,
            chunk["chunk_index"],
            content,
            embedding_str,
            content,
            file_hash,
            json.dumps(metadata)
        ))
        conn.commit()


def cleanup_missing_files(
    conn: psycopg.Connection,
    table_name: str,
    base_path: Path,
    silent_print
) -> int:
    """Remove chunks for files that no longer exist."""
    deleted_count: int = 0

    with conn.cursor() as cur:
        cur.execute(f"SELECT DISTINCT file_path FROM {table_name}")
        indexed_files = cur.fetchall()

        for row in indexed_files:
            file_path: Path = base_path / row["file_path"]
            if not file_path.exists():
                silent_print(f"Removing deleted file: {row['file_path']}")
                cur.execute(f"DELETE FROM {table_name} WHERE file_path = %s", (row["file_path"],))
                deleted_count += 1

        if deleted_count > 0:
            conn.commit()

    return deleted_count


# ----- Indexing Functions -----
def index_files(
    args: argparse.Namespace,
    conn: psycopg.Connection,
    table_name: str
) -> None:
    """Index files from the second brain into the database."""
    base_path: Path = Path(args.path)

    if not base_path.exists() or not base_path.is_dir():
        print(f"Error: Path {base_path} does not exist or is not a directory", file=sys.stderr)
        sys.exit(1)

    # Create print function for silent mode
    def silent_print(*message, **kwargs):
        if not args.silent:
            print(*message, **kwargs)

    silent_print(f"Indexing files in {base_path}...")
    silent_print(f"Using Ollama embedding model: {EMBEDDING_MODEL}")
    silent_print(f"Chunk size: {args.chunk_size} tokens, overlap: {args.chunk_overlap} tokens")

    # Handle reindex - drop existing data
    if args.reindex:
        silent_print("Dropping existing index for full reindex...")
        drop_table(conn, table_name)
        ensure_schema(conn, table_name)

    # Clean up files that no longer exist
    cleanup_missing_files(conn, table_name, base_path, silent_print)

    # Find files to index
    files: list[Path] = find_files_to_index(
        base_path,
        args.include_dirs,
        args.exclude_dirs,
        args.file_types
    )

    silent_print(f"Found {len(files)} files to check")

    indexed_count: int = 0
    chunk_count: int = 0

    for i, file_path in enumerate(files):
        rel_path: str = str(file_path.relative_to(base_path))
        file_hash: str = get_file_hash(file_path)

        # Check if file needs update
        if not args.reindex and not check_file_needs_update(conn, table_name, rel_path, file_hash):
            continue

        silent_print(f"[{i+1}/{len(files)}] Indexing: {rel_path}")

        # Read file content
        content: Optional[str] = read_file_content(file_path)
        if not content:
            silent_print(f"  Skipping (empty or unreadable)")
            continue

        # Delete existing chunks for this file
        delete_file_chunks(conn, table_name, rel_path)

        # Chunk the document
        chunks: list[dict] = chunk_document(content, args.chunk_size, args.chunk_overlap)
        silent_print(f"  Created {len(chunks)} chunks")

        # Generate embeddings and insert chunks
        for chunk in chunks:
            embedding: Optional[list[float]] = generate_embedding(chunk["content"])
            if embedding is None:
                silent_print(f"  Warning: Failed to generate embedding for chunk {chunk['chunk_index']}")
                continue

            metadata: dict = {
                "filename": file_path.name,
                "file_type": file_path.suffix,
                "start_char": chunk["start_char"],
                "end_char": chunk["end_char"],
                "estimated_tokens": chunk["estimated_tokens"]
            }

            insert_chunk(conn, table_name, rel_path, chunk, embedding, file_hash, metadata)
            chunk_count += 1

        indexed_count += 1

    silent_print(f"\nIndexing complete:")
    silent_print(f"  Files indexed: {indexed_count}")
    silent_print(f"  Total chunks: {chunk_count}")


# ----- Search Functions -----
def vector_search(
    conn: psycopg.Connection,
    table_name: str,
    query_embedding: list[float],
    limit: int,
    threshold: float
) -> list[dict]:
    """Perform vector similarity search."""
    embedding_str: str = '[' + ','.join(str(x) for x in query_embedding) + ']'

    with conn.cursor() as cur:
        cur.execute(f"""
        SELECT
            id, file_path, chunk_index, content, metadata,
            1 - (embedding <=> %s::vector) as similarity
        FROM {table_name}
        WHERE 1 - (embedding <=> %s::vector) >= %s
        ORDER BY embedding <=> %s::vector
        LIMIT %s
        """, (embedding_str, embedding_str, threshold, embedding_str, limit * 2))

        return cur.fetchall()


def bm25_search(
    conn: psycopg.Connection,
    table_name: str,
    query: str,
    limit: int
) -> list[dict]:
    """Perform BM25 full-text search."""
    # Escape special characters for tsquery
    query_cleaned: str = re.sub(r'[^\w\s]', ' ', query)
    query_terms: str = ' & '.join(query_cleaned.split())

    if not query_terms:
        return []

    with conn.cursor() as cur:
        cur.execute(f"""
        SELECT
            id, file_path, chunk_index, content, metadata,
            ts_rank_cd(content_tsv, plainto_tsquery('english', %s)) as rank
        FROM {table_name}
        WHERE content_tsv @@ plainto_tsquery('english', %s)
        ORDER BY rank DESC
        LIMIT %s
        """, (query, query, limit * 2))

        return cur.fetchall()


def reciprocal_rank_fusion(
    vector_results: list[dict],
    bm25_results: list[dict],
    k: int = RRF_K,
    keyword_boost: float = KEYWORD_BOOST
) -> list[dict]:
    """
    Combine vector and BM25 results using Reciprocal Rank Fusion.

    RRF score = sum(1 / (k + rank)) for each result list
    Keyword matches get a boost since exact term matches are strong signals.
    """
    scores: dict[int, float] = {}  # id -> RRF score
    results_map: dict[int, dict] = {}  # id -> result data

    # Score vector results
    for rank, result in enumerate(vector_results):
        result_id: int = result["id"]
        scores[result_id] = scores.get(result_id, 0) + 1 / (k + rank + 1)
        if result_id not in results_map:
            results_map[result_id] = dict(result)
            results_map[result_id]["vector_similarity"] = result.get("similarity", 0)

    # Score BM25 results with boost (exact term matches are strong signal)
    for rank, result in enumerate(bm25_results):
        result_id: int = result["id"]
        scores[result_id] = scores.get(result_id, 0) + (keyword_boost / (k + rank + 1))
        if result_id not in results_map:
            results_map[result_id] = dict(result)
        results_map[result_id]["bm25_rank"] = result.get("rank", 0)

    # Sort by RRF score
    sorted_ids: list[int] = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

    # Build final results
    results: list[dict] = []
    for result_id in sorted_ids:
        result: dict = results_map[result_id]
        result["rrf_score"] = scores[result_id]
        results.append(result)

    return results


def deduplicate_by_file(results: list[dict], limit: int) -> list[dict]:
    """
    Deduplicate results to show one result per file (best chunk).
    Also aggregates chunks from same file for context.
    """
    file_results: dict[str, dict] = {}
    file_chunks: dict[str, list[dict]] = {}

    for result in results:
        file_path: str = result["file_path"]

        if file_path not in file_results:
            file_results[file_path] = result
            file_chunks[file_path] = [result]
        else:
            file_chunks[file_path].append(result)
            # Update if this chunk has higher score
            current_score: float = file_results[file_path].get("rrf_score", 0) or file_results[file_path].get("similarity", 0)
            new_score: float = result.get("rrf_score", 0) or result.get("similarity", 0)
            if new_score > current_score:
                file_results[file_path] = result

    # Add chunk count to results
    final_results: list[dict] = []
    for file_path, result in file_results.items():
        result["matched_chunks"] = len(file_chunks[file_path])
        result["all_chunks"] = file_chunks[file_path]
        final_results.append(result)

    # Sort by best score and limit
    final_results.sort(
        key=lambda x: x.get("rrf_score", 0) or x.get("similarity", 0),
        reverse=True
    )

    return final_results[:limit]


def search(
    args: argparse.Namespace,
    conn: psycopg.Connection,
    table_name: str,
    query: str
) -> Optional[list[dict]]:
    """Perform hybrid search combining vector and keyword search."""
    def silent_print(*message, **kwargs):
        if not args.silent and not args.simple and not args.yaml and not args.json:
            print(*message, **kwargs)

    # Generate query embedding
    silent_print("Generating query embedding...")
    query_embedding: Optional[list[float]] = generate_embedding(query)

    if query_embedding is None and not args.keyword_only:
        silent_print("Warning: Failed to generate embedding, falling back to keyword-only search")
        args.keyword_only = True

    vector_results: list[dict] = []
    bm25_results: list[dict] = []

    # Vector search
    if not args.keyword_only and query_embedding:
        silent_print("Performing vector search...")
        vector_results = vector_search(conn, table_name, query_embedding, args.limit, args.threshold)
        silent_print(f"  Found {len(vector_results)} vector matches")

    # BM25 keyword search
    if not args.vector_only:
        silent_print("Performing keyword search...")
        bm25_results = bm25_search(conn, table_name, query, args.limit)
        silent_print(f"  Found {len(bm25_results)} keyword matches")

    # Combine results
    if args.keyword_only:
        results = bm25_results
        # Add similarity field for consistency
        for r in results:
            r["similarity"] = r.get("rank", 0)
    elif args.vector_only:
        results = vector_results
    else:
        silent_print("Combining results with Reciprocal Rank Fusion...")
        results = reciprocal_rank_fusion(vector_results, bm25_results)

    if not results:
        if args.yaml:
            print(yaml.safe_dump({"results": []}, sort_keys=False))
        elif args.json:
            print(json.dumps({"results": []}, indent=2))
        elif not args.simple:
            silent_print("\nNo matches found. Try:")
            silent_print("  - Lowering --threshold (current: {})".format(args.threshold))
            silent_print("  - Using --keyword-only for exact term matching")
            silent_print("  - Running --index to update the index")
        return None

    # Deduplicate by file
    results = deduplicate_by_file(results, args.limit)

    return results


def format_output(
    args: argparse.Namespace,
    results: list[dict],
    base_path: str
) -> None:
    """Format and print search results."""
    if args.simple:
        for result in results:
            if args.full_path:
                print(os.path.join(base_path, result["file_path"]))
            else:
                print(result["file_path"])
        return

    if args.yaml or args.json:
        output_results: list[dict] = []
        for result in results:
            out: dict = {
                "file_path": result["file_path"],
                "full_path": os.path.join(base_path, result["file_path"]),
                "matched_chunks": result.get("matched_chunks", 1),
            }
            # Include available scores
            if result.get("vector_similarity"):
                out["vector_similarity"] = round(result["vector_similarity"], 4)
            if result.get("bm25_rank"):
                out["bm25_rank"] = round(result["bm25_rank"], 4)
            if result.get("rrf_score"):
                out["rrf_score"] = round(result["rrf_score"], 4)
            if args.show_chunks:
                out["chunk_content"] = result["content"][:500] + "..." if len(result["content"]) > 500 else result["content"]
            if result.get("metadata"):
                metadata = result["metadata"]
                if isinstance(metadata, str):
                    metadata = json.loads(metadata)
                out["metadata"] = metadata
            output_results.append(out)

        output: dict = {"results": output_results}
        if args.yaml:
            print(yaml.safe_dump(output, sort_keys=False))
        else:
            print(json.dumps(output, indent=2))
        return

    # Standard detailed output
    print(f"\nFound {len(results)} matching files:")
    print("-" * 80)

    for i, result in enumerate(results):
        chunks: int = result.get("matched_chunks", 1)

        # Build score display - show vector similarity if available
        score_parts: list[str] = []
        if result.get("vector_similarity"):
            score_parts.append(f"similarity: {result['vector_similarity']:.2%}")
        if result.get("bm25_rank"):
            score_parts.append(f"keyword rank: {result['bm25_rank']:.2f}")
        if result.get("rrf_score") and not score_parts:
            # Fallback to RRF if no other scores
            score_parts.append(f"score: {result['rrf_score']:.4f}")

        score_str: str = " | ".join(score_parts) if score_parts else "matched"

        print(f"{i+1}. {result['file_path']}")
        print(f"   {score_str} | chunks: {chunks}")

        if args.show_chunks:
            content: str = result["content"]
            snippet: str = content[:300] + "..." if len(content) > 300 else content
            # Clean up snippet for display
            snippet = snippet.replace('\n', ' ').strip()
            print(f"   Preview: {snippet}")

        print("-" * 80)

    # Show most relevant file
    if results:
        most_relevant: dict = results[0]
        full_path: str = os.path.join(base_path, most_relevant["file_path"])
        print(f"\nMost relevant: {full_path}")
        if not args.open:
            print("Tip: Use --open to open this file in your editor")


def open_file_in_editor(file_path: str, editor: Optional[str] = None) -> bool:
    """Open a file in the specified editor."""
    if not editor:
        editor = os.environ.get("EDITOR")

    if not editor:
        for fallback in ["nvim", "vim", "nano", "code"]:
            result = run(["which", fallback], stdout=PIPE, stderr=PIPE)
            if result.returncode == 0:
                editor = fallback
                break

    if not editor:
        print("Error: No editor found. Set $EDITOR or use --editor", file=sys.stderr)
        return False

    try:
        editor_parts: list[str] = shlex.split(editor)
        # Use execvp to replace current process with editor
        # This properly hands off the terminal to the editor
        print(f"Opening {file_path} with {editor}")
        sys.stdout.flush()
        sys.stderr.flush()
        os.execvp(editor_parts[0], [*editor_parts, file_path])
        # execvp doesn't return if successful
        return True
    except Exception as e:
        print(f"Error opening file: {e}", file=sys.stderr)
        return False


# ----- Main -----
def main() -> None:
    """Main entry point."""
    args: argparse.Namespace = parse_args()

    # Handle license flag
    if args.license:
        print("GNU Affero General Public License v3.0")
        print("https://www.gnu.org/licenses/agpl-3.0.html")
        sys.exit(0)

    # Auto-enable silent mode for machine-readable output
    if args.simple or args.yaml or args.json:
        args.silent = True

    # Check for conflicting options
    if args.open and args.simple:
        print("Error: --open and --simple cannot be used together", file=sys.stderr)
        sys.exit(1)

    if args.keyword_only and args.vector_only:
        print("Error: --keyword-only and --vector-only cannot be used together", file=sys.stderr)
        sys.exit(1)

    # Connect to database
    conn: psycopg.Connection = get_connection()
    ensure_schema(conn, args.table)

    # Handle indexing
    if args.index or args.reindex:
        index_files(args, conn, args.table)
        if not args.query:
            conn.close()
            return

    # Perform search if query provided
    if args.query:
        results: Optional[list[dict]] = search(args, conn, args.table, args.query)

        if results:
            format_output(args, results, args.path)

            # Open most relevant file if requested
            if args.open:
                full_path: str = os.path.join(args.path, results[0]["file_path"])
                # Close database before exec (exec replaces process, won't return)
                conn.close()
                open_file_in_editor(full_path, args.editor)
                # If we get here, exec failed
                sys.exit(1)

    elif not args.index and not args.reindex:
        print("Please provide a search query or use --index to index your second brain")
        print("Run with --help for more information")

    conn.close()


if __name__ == "__main__":
    main()
