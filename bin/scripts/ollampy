#!/usr/bin/python3

from os import environ
from subprocess import run
from sys import argv, exit, stdin, stderr
import argparse
import json
import requests
import sys
import os
import base64
import mimetypes
import time
import select


# Default configurations for Ollama
DEFAULT_ENDPOINT: str|None = ""
DEFAULT_MODEL: str|None = ""

if ("OLLAMPY_ENDPOINT" in environ):
    DEFAULT_ENDPOINT = environ.get("OLLAMPY_ENDPOINT")
else:
    DEFAULT_ENDPOINT = "http://localhost:11434"

if ("OLLAMPY_MODEL" in environ): 
    DEFAULT_MODEL = environ.get("OLLAMPY_MODEL")
else:
    DEFAULT_MODEL = "gemma3:12b"

# Container check
ctr_id: str|None = ""

if ("CONTAINER_ID" in environ):
    ctr_id = environ.get("CONTAINER_ID")

# if we are not in the 'dev' distrobox re-exec the script
# inside of the 'dev' distrobox
if ("dev" != ctr_id):
    cmd: list[str] = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *argv
    ]

    run(cmd)
    exit(0)

# Parse arguments
parser = argparse.ArgumentParser(description="Query Ollama API")
parser.add_argument("--prompt", help="Prompt to prepend to the input")
parser.add_argument("--model", default=DEFAULT_MODEL,
                    help=f"Model to use (default: {DEFAULT_MODEL}). Available models depend on your Ollama installation.")
parser.add_argument("--endpoint", default=DEFAULT_ENDPOINT,
                    help=f"Ollama API endpoint (default: {DEFAULT_ENDPOINT})")
parser.add_argument("--debug", action="store_true", help="Enable debug mode (shows request details)")
parser.add_argument("--json", action="store_true", help="Return a clean JSON response without streaming")
parser.add_argument("-S", "--no-streaming", action="store_true", help="Disable streaming mode for cleaner output capture")
parser.add_argument("--embedding", action="store_true", help="Generate an embedding vector instead of a text response")
parser.add_argument("-f", "--file", action="append", dest="files",
                    help="Include file content in the context (can be specified multiple times)")
parser.add_argument("-L", "--list-models", action="store_true",
                    help="List available models from the Ollama server")
parser.add_argument("--no-color", action="store_true",
                    help="Disable colored output")
parser.add_argument("--image-gen", action="store_true",
                    help="[NOT SUPPORTED] Generate an image (Ollama doesn't support image generation)")
parser.add_argument("--regen", action="store_true",
                    help="Regenerate/enhance an image provided via --image (uses vision model)")
parser.add_argument("--output", help="Output filename for image generation (default: auto-numbered output-NNNN.png)")
parser.add_argument("--image", action="append", dest="images",
                    help="Include image file(s) for vision analysis (can be specified multiple times)")
parser.add_argument("--summary", action="store_true",
                    help="Show usage summary with token counts, timing, and costs in a formatted table")
parser.add_argument("--dry-run", action="store_true",
                    help="Show what would be sent to the API without actually making the request")
args = parser.parse_args()

def get_auto_numbered_filename(base_name="output", extension=".png"):
    """Generate an auto-numbered filename that doesn't exist yet."""
    counter = 0
    while True:
        filename = f"{base_name}-{counter:04d}{extension}"
        if not os.path.exists(filename):
            return filename
        counter += 1

def encode_image_to_base64(image_path):
    """Encode an image file to base64 for Ollama API."""
    try:
        with open(image_path, 'rb') as image_file:
            image_data = image_file.read()
            encoded_string = base64.b64encode(image_data).decode('utf-8')
            return encoded_string
    except Exception as e:
        print(f"Error reading image {image_path}: {e}", file=sys.stderr)
        exit(1)

def calculate_cost(model, input_tokens, output_tokens):
    """Calculate cost for Ollama (typically free for local models)."""
    # Ollama running locally is typically free
    # If using a remote Ollama server, costs would depend on that setup
    return {
        "input_cost": 0.0,
        "output_cost": 0.0,
        "total_cost": 0.0
    }

def print_summary_table(model, input_tokens, output_tokens, total_time, first_token_time, cost_info, is_estimated=False, use_color=True):
    """Print a formatted summary table with color support."""
    # ANSI color codes (conditionally set based on use_color)
    if use_color:
        CYAN = '\033[96m'
        GREEN = '\033[92m'
        YELLOW = '\033[93m'
        BLUE = '\033[94m'
        MAGENTA = '\033[95m'
        BOLD = '\033[1m'
        END = '\033[0m'
    else:
        CYAN = GREEN = YELLOW = BLUE = MAGENTA = BOLD = END = ''
    
    # Calculate total tokens
    total_tokens = input_tokens + output_tokens
    
    # Table width (inner content width)
    width = 50
    
    # Helper to strip ANSI codes
    import re
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    
    def strip_ansi(text):
        return ansi_escape.sub('', text)
    
    # Helper to create a padded line
    def make_line(left, right, left_width=22):
        """Create a line with left and right parts, properly padded."""
        left_plain = strip_ansi(left)
        right_plain = strip_ansi(right)
        
        # Total inner content must be exactly 48 characters
        # Left part: left_text + padding to reach left_width
        left_part = left + ' ' * (left_width - len(left_plain))
        
        # Right part: right_text + padding to fill remaining space
        remaining_width = 48 - left_width - 1  # -1 for space between left and right
        right_part = right + ' ' * (remaining_width - len(right_plain))
        
        # Combine: left_part + space + right_part = exactly 48 chars
        content = f"{left_part} {right_part}"
        
        return f"{CYAN}║{END} {content} {CYAN}║{END}"
    
    # Build the table
    print(f"\n{BOLD}{CYAN}╔{'═' * width}╗{END}", file=sys.stderr)
    
    # Title
    title = "API Usage Summary"
    title_pad_left = (width - len(title)) // 2
    title_pad_right = width - len(title) - title_pad_left
    print(f"{BOLD}{CYAN}║{' ' * title_pad_left}{title}{' ' * title_pad_right}║{END}", file=sys.stderr)
    
    print(f"{BOLD}{CYAN}╠{'═' * width}╣{END}", file=sys.stderr)
    
    # Model info
    model_display = model if len(model) <= 30 else model[:27] + "..."
    print(make_line(f"{BOLD}Model:{END}", f"{GREEN}{model_display}{END}"), file=sys.stderr)
    print(f"{CYAN}╠{'─' * width}╣{END}", file=sys.stderr)
    
    # Token usage
    print(make_line(f"{BOLD}Token Usage:{END}", ""), file=sys.stderr)
    
    est_marker = " (est)" if is_estimated else ""
    print(make_line("  Input:", f"{YELLOW}{input_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Output:", f"{YELLOW}{output_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Total:", f"{BOLD}{YELLOW}{total_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(f"{CYAN}╠{'─' * width}╣{END}", file=sys.stderr)
    
    # Performance
    print(make_line(f"{BOLD}Performance:{END}", ""), file=sys.stderr)
    
    if first_token_time:
        print(make_line("  Time to first token:", f"{BLUE}{first_token_time:.2f}s{END}"), file=sys.stderr)
    
    print(make_line("  Total time:", f"{BLUE}{total_time:.2f}s{END}"), file=sys.stderr)
    
    if output_tokens > 0 and total_time > 0:
        tokens_per_sec = output_tokens / total_time
        print(make_line("  Tokens/second:", f"{BLUE}{tokens_per_sec:.1f}{END}"), file=sys.stderr)
    
    print(f"{CYAN}╠{'─' * width}╣{END}", file=sys.stderr)
    
    # Cost (typically free for local Ollama)
    print(make_line(f"{BOLD}Cost Breakdown:{END}", ""), file=sys.stderr)
    
    if cost_info['total_cost'] == 0.0:
        print(make_line("  Status:", f"{GREEN}Free (Local){END}"), file=sys.stderr)
    else:
        print(make_line("  Input cost:", f"{MAGENTA}${cost_info['input_cost']:.6f}{END}"), file=sys.stderr)
        print(make_line("  Output cost:", f"{MAGENTA}${cost_info['output_cost']:.6f}{END}"), file=sys.stderr)
        print(make_line(f"  {BOLD}Total cost:{END}", f"{BOLD}{MAGENTA}${cost_info['total_cost']:.6f}{END}"), file=sys.stderr)
    
    print(f"{BOLD}{CYAN}╚{'═' * width}╝{END}", file=sys.stderr)

# Since outside of the distrobox we may not have these modules
# quietly ignore the fact that they may not exist
try:
    # Handle list-models option first
    if args.list_models:
        # Check if colors should be disabled
        use_colors = not args.no_color and environ.get("NO_COLOR", "").lower() not in ("1", "true")
        
        # Fetch available models from the Ollama API
        url = f"{args.endpoint}/api/tags"
        
        if args.debug:
            print(f"Debug: Listing models from {url}", file=sys.stderr)
        
        try:
            response = requests.get(url)
            if response.status_code != 200:
                print(f"Error fetching models: {response.status_code}", file=sys.stderr)
                print(response.text, file=sys.stderr)
                exit(1)
            
            models_data = response.json()
            if "models" in models_data:
                # ANSI color codes (conditionally set based on use_colors)
                class Colors:
                    HEADER = '\033[95m' if use_colors else ''
                    BLUE = '\033[94m' if use_colors else ''
                    CYAN = '\033[96m' if use_colors else ''
                    GREEN = '\033[92m' if use_colors else ''
                    YELLOW = '\033[93m' if use_colors else ''
                    RED = '\033[91m' if use_colors else ''
                    BOLD = '\033[1m' if use_colors else ''
                    UNDERLINE = '\033[4m' if use_colors else ''
                    END = '\033[0m' if use_colors else ''
                
                # Prepare data for table
                table_data = []
                for model in models_data["models"]:
                    name = model.get("name", "Unknown")
                    size = model.get("size", 0)
                    modified = model.get("modified_at", "Unknown")
                    
                    # Format size in human-readable format
                    if size > 0:
                        if size >= 1024**3:  # GB
                            size_str = f"{size / (1024**3):.1f}GB"
                        elif size >= 1024**2:  # MB
                            size_str = f"{size / (1024**2):.1f}MB"
                        else:  # Bytes
                            size_str = f"{size}B"
                    else:
                        size_str = "Unknown"
                    
                    # Format modified date
                    if modified != "Unknown":
                        try:
                            from datetime import datetime
                            # Parse ISO format and display human-readable
                            dt = datetime.fromisoformat(modified.replace('Z', '+00:00'))
                            modified_str = dt.strftime("%Y-%m-%d %H:%M")
                        except:
                            modified_str = modified
                    else:
                        modified_str = "Unknown"
                    
                    table_data.append([name, size_str, modified_str])
                
                # Calculate column widths
                if table_data:
                    max_name = max(len(row[0]) for row in table_data)
                    max_size = max(len(row[1]) for row in table_data)
                    max_date = max(len(row[2]) for row in table_data)
                    
                    # Ensure minimum widths
                    max_name = max(max_name, len("Model Name"))
                    max_size = max(max_size, len("Size"))
                    max_date = max(max_date, len("Modified"))
                    
                    # Print header
                    print(f"\n{Colors.BOLD}{Colors.CYAN}Available Ollama Models:{Colors.END}")
                    print(f"{Colors.BOLD}{'─' * (max_name + max_size + max_date + 6)}{Colors.END}")
                    
                    # Print table header
                    print(f"{Colors.BOLD}{Colors.HEADER}{'Model Name':<{max_name}} {'Size':<{max_size}} {'Modified':<{max_date}}{Colors.END}")
                    print(f"{Colors.BOLD}{'─' * max_name} {'─' * max_size} {'─' * max_date}{Colors.END}")
                    
                    # Print table rows
                    for i, (name, size_str, modified_str) in enumerate(table_data):
                        # Alternate row colors
                        color = Colors.CYAN if i % 2 == 0 else Colors.BLUE
                        size_color = Colors.GREEN if size_str != "Unknown" else Colors.YELLOW
                        date_color = Colors.GREEN if modified_str != "Unknown" else Colors.YELLOW
                        
                        print(f"{color}{name:<{max_name}}{Colors.END} "
                              f"{size_color}{size_str:<{max_size}}{Colors.END} "
                              f"{date_color}{modified_str:<{max_date}}{Colors.END}")
                    
                    print(f"{Colors.BOLD}{'─' * (max_name + max_size + max_date + 6)}{Colors.END}")
                    print(f"{Colors.BOLD}Total models: {Colors.GREEN}{len(table_data)}{Colors.END}\n")
                else:
                    print(f"{Colors.YELLOW}No models found{Colors.END}")
            else:
                print("No models found or unexpected response format")
                if args.debug:
                    print(f"Debug: Response: {json.dumps(models_data)}", file=sys.stderr)
            
        except requests.RequestException as e:
            print(f"Error connecting to Ollama server: {e}", file=sys.stderr)
            exit(1)
        
        exit(0)
    
    # Read file contents if any files were specified
    file_contents = []
    if args.files:
        for file_path in args.files:
            try:
                with open(file_path, 'r') as f:
                    file_content = f.read()
                    file_contents.append(f"=== File: {file_path} ===\n{file_content}\n")
            except IOError as e:
                print(f"Warning: Could not read file {file_path}: {e}", file=stderr)
    
    # Read from standard input only if there's data available
    query = ""
    # Check if stdin has data (not a terminal and has content)
    if not stdin.isatty() or stdin in select.select([stdin], [], [], 0)[0]:
        query = stdin.read()
    # If no stdin and no prompt/files provided, and not doing image generation or regen, show error
    elif not args.prompt and not args.files and not args.image_gen and not args.regen and not args.images:
        print("Error: No input provided. Use --prompt, -f/--file, or pipe input via stdin", file=stderr)
        exit(1)

    # Combine file contents with query
    combined_parts = []
    
    # Add file contents first if any
    if file_contents:
        combined_parts.extend(file_contents)
    
    # Add prompt if provided
    if args.prompt:
        combined_parts.append(args.prompt)
    
    # Add stdin content if any
    if query:
        combined_parts.append(query)
    
    # Combine all parts
    if combined_parts:
        query = "\n".join(combined_parts)
    else:
        query = ""

    # Set up API call
    headers = {
        "Content-Type": "application/json"
    }

    # Set the API endpoint based on the request type
    if args.embedding:
        # Use the embeddings endpoint for Ollama
        url = f"{args.endpoint}/api/embeddings"
        
        # For embeddings, we use a different request format
        data = {
            "model": args.model,
            "prompt": query  # Ollama uses "prompt" instead of "input" for embeddings
        }
    elif args.regen:
        # Note: Ollama doesn't support image generation, only vision analysis
        print("Error: Image regeneration is not supported by Ollama.", file=sys.stderr)
        print("Ollama only supports vision models for analyzing images, not generating them.", file=sys.stderr)
        print("Consider using 'ollampy --image image.png' with a vision model like llava instead.", file=sys.stderr)
        exit(1)
        
    elif args.image_gen:
        # Note: Ollama doesn't actually support image generation
        print("Error: Image generation is not supported by Ollama.", file=sys.stderr)
        print("Ollama only supports vision models for analyzing images, not generating them.", file=sys.stderr)
        exit(1)
    else:
        # Use the chat endpoint for regular requests
        url = f"{args.endpoint}/api/chat"
        
        # Construct message content
        message_content = query
        images_list = []
        
        # Add image content if provided
        if args.images:
            for image_path in args.images:
                if not os.path.exists(image_path):
                    print(f"Error: Image file not found: {image_path}", file=sys.stderr)
                    exit(1)
                
                image_data = encode_image_to_base64(image_path)
                images_list.append(image_data)
        
        # Build the message
        message = {
            "role": "user",
            "content": message_content
        }
        
        # Add images if any
        if images_list:
            message["images"] = images_list
        
        data = {
            "model": args.model,
            "messages": [message],
            # Only stream if not in JSON mode or --no-streaming
            "stream": not (args.json or args.no_streaming)
        }

    # Print debug info if requested
    if args.debug:
        print(f"Debug: API URL: {url}", file=sys.stderr)
        print(f"Debug: Headers: {headers}", file=sys.stderr)
        print(f"Debug: Data: {json.dumps(data)}", file=sys.stderr)
        if args.image_gen:
            output_file = args.output if args.output else get_auto_numbered_filename()
            print(f"Debug: Image will be saved to: {output_file}", file=sys.stderr)
            print(f"Debug: Note - Image generation support depends on the selected model", file=sys.stderr)
        if args.images:
            print(f"Debug: Processing {len(args.images)} image(s): {', '.join(args.images)}", file=sys.stderr)
            print(f"Debug: Note - Vision support depends on the selected model", file=sys.stderr)
    
    # Handle dry-run mode
    if args.dry_run:
        print("=== DRY-RUN MODE ===", file=sys.stderr)
        print(f"API endpoint: {url}", file=sys.stderr)
        print("HTTP method: POST", file=sys.stderr)
        print(f"Headers: {headers}", file=sys.stderr)
        
        # Format request payload (truncate long content)
        request_data = data.copy()
        if "messages" in request_data and request_data["messages"]:
            for msg in request_data["messages"]:
                if isinstance(msg.get("content"), str) and len(msg["content"]) > 200:
                    msg["content"] = msg["content"][:200] + "... [truncated]"
                # Handle images in messages
                if "images" in msg and msg["images"]:
                    msg["images"] = [img[:100] + "... [truncated]" if len(img) > 100 else img for img in msg["images"]]
        elif "prompt" in request_data and len(request_data["prompt"]) > 200:
            request_data["prompt"] = request_data["prompt"][:200] + "... [truncated]"
        
        print(f"Request payload: {json.dumps(request_data, indent=2)}", file=sys.stderr)
        
        # Show full prompt if not too long
        if query and len(query) <= 1000:
            print(f"\nFull prompt:\n{query}", file=sys.stderr)
        elif query:
            print(f"\nFull prompt (truncated):\n{query[:1000]}... [truncated]", file=sys.stderr)
        
        # Mode-specific information
        if args.embedding:
            print("\nMode: Embedding generation", file=sys.stderr)
            print(f"Embedding model: {args.model}", file=sys.stderr)
            print("Endpoint: /api/embeddings", file=sys.stderr)
        elif args.images:
            print("\nMode: Vision analysis", file=sys.stderr)
            print(f"Vision model: {args.model}", file=sys.stderr)
            print(f"Images: {', '.join(args.images)}", file=sys.stderr)
            print("Note: Vision support depends on model capabilities", file=sys.stderr)
        else:
            print("\nMode: Text generation", file=sys.stderr)
            print(f"Model: {args.model}", file=sys.stderr)
            print(f"Streaming: {not (args.json or args.no_streaming)}", file=sys.stderr)
        
        print(f"\nOllama server: {args.endpoint}", file=sys.stderr)
        print("Note: This is a local Ollama instance", file=sys.stderr)
        
        print("=== END DRY-RUN ===", file=sys.stderr)
        exit(0)
    
    # Track timing if summary mode
    start_time = time.time() if args.summary else None
    first_token_time = None

    # Send request to Ollama API (streaming only if needed)
    response = requests.post(url, headers=headers, json=data, stream=data.get("stream", False))

    if response.status_code != 200:
        print(f"Error: {response.status_code}")
        print(response.text)
        exit(1)
    
    # Handle the response based on request type
    if args.embedding:
        # Process embeddings response (never streamed)
        response_data = response.json()
        if "embedding" in response_data:
            # Format the embedding as expected by semantic_search
            print(json.dumps({"embedding": response_data["embedding"]}))
            
            # Summary output for embeddings
            if args.summary:
                end_time = time.time()
                total_time = end_time - start_time
                
                # Add newline before summary
                print(file=sys.stderr)
                
                # For embeddings, estimate token count
                estimated_tokens = len(query) // 4
                
                # Calculate cost (free for local Ollama)
                cost_info = calculate_cost(args.model, estimated_tokens, 0)
                
                # Print summary table
                print_summary_table(
                    model=args.model,
                    input_tokens=estimated_tokens,
                    output_tokens=0,
                    total_time=total_time,
                    first_token_time=None,
                    cost_info=cost_info,
                    is_estimated=True,
                    use_color=not args.no_color
                )
        else:
            print(json.dumps(response_data))
    elif args.image_gen:
        # Process image generation response
        response_data = response.json()
        if "images" in response_data and len(response_data["images"]) > 0:
            # Get the base64 image data (Ollama returns images as base64)
            image_data = response_data["images"][0]
            
            # Determine output filename
            if args.output:
                output_filename = args.output
            else:
                output_filename = get_auto_numbered_filename()
            
            # Decode and save the image
            try:
                image_bytes = base64.b64decode(image_data)
                with open(output_filename, 'wb') as f:
                    f.write(image_bytes)
                print(f"Image saved to: {output_filename}")
                
                # Summary output for image generation
                if args.summary:
                    end_time = time.time()
                    total_time = end_time - start_time
                    
                    # Add newline before summary
                    print(file=sys.stderr)
                    
                    # Image generation typically doesn't provide token counts
                    # Estimate input tokens from prompt length
                    estimated_input_tokens = len(query) // 4
                    estimated_output_tokens = 0  # Images don't have text output tokens
                    
                    # Calculate cost for image generation (free for local Ollama)
                    cost_info = calculate_cost(
                        args.model,  # Use the actual model for pricing
                        estimated_input_tokens,
                        estimated_output_tokens
                    )
                    
                    # Print summary table
                    print_summary_table(
                        model=args.model,
                        input_tokens=estimated_input_tokens,
                        output_tokens=estimated_output_tokens,
                        total_time=total_time,
                        first_token_time=None,
                        cost_info=cost_info,
                        is_estimated=True,
                        use_color=not args.no_color
                    )
            except Exception as e:
                print(f"Error saving image: {e}", file=sys.stderr)
                exit(1)
        else:
            # Check if the model supports image generation
            if "error" in response_data:
                error_msg = response_data["error"]
                if "not found" in error_msg.lower() or "unsupported" in error_msg.lower():
                    print(f"Error: Model '{args.model}' does not support image generation.", file=sys.stderr)
                    print("Try using a model that supports image generation (e.g., dall-e, stable-diffusion variants)", file=sys.stderr)
                else:
                    print(f"Error: {error_msg}", file=sys.stderr)
            else:
                print(f"Error: No image data in response: {json.dumps(response_data)}", file=sys.stderr)
            exit(1)
    elif args.json or args.no_streaming:
        # Process standard JSON response (not streamed)
        response_data = response.json()
        if "message" in response_data and "content" in response_data["message"]:
            # Just return the content without any JSON formatting
            message_content = response_data["message"]["content"]
            print(message_content)
            
            # Summary output for non-streaming
            if args.summary:
                end_time = time.time()
                total_time = end_time - start_time
                
                # Add newline before summary
                print(file=sys.stderr)
                
                # Estimate tokens (Ollama doesn't always provide token counts)
                estimated_input_tokens = len(query) // 4
                estimated_output_tokens = len(message_content) // 4
                
                # Calculate cost (free for local Ollama)
                cost_info = calculate_cost(args.model, estimated_input_tokens, estimated_output_tokens)
                
                # Print summary table
                print_summary_table(
                    model=args.model,
                    input_tokens=estimated_input_tokens,
                    output_tokens=estimated_output_tokens,
                    total_time=total_time,
                    first_token_time=None,
                    cost_info=cost_info,
                    is_estimated=True,
                    use_color=not args.no_color
                )
        else:
            # Just print the raw response as fallback
            if args.json:
                print(json.dumps(response_data))
            else:
                print(f"Unexpected response format: {json.dumps(response_data)}", file=sys.stderr)
    else:
        # Process the streaming response
        accumulated_content = ""
        
        for line in response.iter_lines():
            if line:
                try:
                    event = json.loads(line.decode('utf-8'))
                    # Extract content from the message
                    if "message" in event and "content" in event["message"]:
                        content = event["message"]["content"]
                        print(content, end="", flush=True)
                        
                        # Track first token time
                        if args.summary and first_token_time is None and content:
                            first_token_time = time.time()
                        
                        accumulated_content += content
                    # Handle done message
                    if event.get("done", False):
                        break
                except json.JSONDecodeError:
                    continue
        
        # Summary output for streaming
        if args.summary:
            end_time = time.time()
            total_time = end_time - start_time
            
            # Add newline before summary
            print(file=sys.stderr)
            
            # Estimate tokens (Ollama doesn't provide token counts in streaming)
            estimated_input_tokens = len(query) // 4
            estimated_output_tokens = len(accumulated_content) // 4
            
            # Calculate cost (free for local Ollama)
            cost_info = calculate_cost(args.model, estimated_input_tokens, estimated_output_tokens)
            
            # Calculate time to first token
            ttft = (first_token_time - start_time) if first_token_time else None
            
            # Print summary table
            print_summary_table(
                model=args.model,
                input_tokens=estimated_input_tokens,
                output_tokens=estimated_output_tokens,
                total_time=total_time,
                first_token_time=ttft,
                cost_info=cost_info,
                is_estimated=True,
                use_color=not args.no_color
            )

except (ImportError, Exception) as e:
    print(f"Error: {e}")
    exit(1)
