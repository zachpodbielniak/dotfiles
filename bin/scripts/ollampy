#!/usr/bin/python3

# dotfiles - Personal configuration files and scripts
# Copyright (C) 2025  Zach Podbielniak
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.


from os import environ
from subprocess import run
from sys import argv, exit, stdin, stderr
import argparse
import json
import requests
import sys
import os
import base64
import mimetypes
import time
import select
from datetime import datetime
from pathlib import Path
import subprocess
import difflib


# Default configurations for Ollama
DEFAULT_ENDPOINT: str|None = ""
DEFAULT_MODEL: str|None = ""

if ("OLLAMPY_ENDPOINT" in environ):
    DEFAULT_ENDPOINT = environ.get("OLLAMPY_ENDPOINT")
else:
    DEFAULT_ENDPOINT = "http://localhost:11434"

if ("OLLAMPY_MODEL" in environ): 
    DEFAULT_MODEL = environ.get("OLLAMPY_MODEL")
else:
    DEFAULT_MODEL = "gemma3:12b"

# Container check
ctr_id: str|None = ""

if ("CONTAINER_ID" in environ):
    ctr_id = environ.get("CONTAINER_ID")

# Check if distrobox check should be skipped
no_dbox_check = environ.get("NO_DBOX_CHECK", "").lower() in ("1", "true")

# if we are not in the 'dev' distrobox re-exec the script
# inside of the 'dev' distrobox
if not no_dbox_check and ("dev" != ctr_id):
    cmd: list[str] = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *argv
    ]

    run(cmd)
    exit(0)

# Parse arguments
parser = argparse.ArgumentParser(description="Query Ollama API")
parser.add_argument("--prompt", help="Prompt to prepend to the input")
parser.add_argument("--model", default=DEFAULT_MODEL,
                    help=f"Model to use (default: {DEFAULT_MODEL}). Available models depend on your Ollama installation.")
parser.add_argument("--endpoint", default=DEFAULT_ENDPOINT,
                    help=f"Ollama API endpoint (default: {DEFAULT_ENDPOINT})")
parser.add_argument("--debug", action="store_true", help="Enable debug mode (shows request details)")
parser.add_argument("--json", action="store_true", help="Return a clean JSON response without streaming")
parser.add_argument("-S", "--no-streaming", action="store_true", help="Disable streaming mode for cleaner output capture")
parser.add_argument("--embedding", action="store_true", help="Generate an embedding vector instead of a text response")
parser.add_argument("-f", "--file", action="append", dest="files",
                    help="Include file content in the context (can be specified multiple times)")
parser.add_argument("-L", "--list-models", action="store_true",
                    help="List available models from the Ollama server")
parser.add_argument("--no-color", action="store_true",
                    help="Disable colored output")
parser.add_argument("--image-gen", action="store_true",
                    help="[NOT SUPPORTED] Generate an image (Ollama doesn't support image generation)")
parser.add_argument("--regen", action="store_true",
                    help="Regenerate/enhance an image provided via --image (uses vision model)")
parser.add_argument("--output", help="Output filename for image generation (default: auto-numbered output-NNNN.png)")
parser.add_argument("--image", action="append", dest="images",
                    help="Include image file(s) for vision analysis (can be specified multiple times)")
parser.add_argument("--summary", action="store_true",
                    help="Show usage summary with token counts, timing, and costs in a formatted table")
parser.add_argument("--dry-run", action="store_true",
                    help="Show what would be sent to the API without actually making the request")
parser.add_argument("--no-preserve", action="store_true",
                    help="Don't save the chat transaction to the second-brain markdown file")
parser.add_argument("--use-context", action="store_true",
                    help="Include today's chat history as context for the conversation")
parser.add_argument("--use-context-from", 
                    help="Include chat history from a specific date (YYYY-MM-DD format) as context")
parser.add_argument("--profile-performance", action="store_true",
                    help="Profile performance of various Ollama models and compare accuracy with other AI providers")
parser.add_argument("--profile-prompt", default="Explain quantum computing in one paragraph.",
                    help="Custom prompt for performance profiling (default: 'Explain quantum computing in one paragraph.')")
parser.add_argument("--profile-models",
                    help="Comma-separated list of specific models to profile (e.g., 'llama2,mistral,codellama')")
parser.add_argument("--profile-show-output", action="store_true",
                    help="Show the output from each model in the performance profile table")
parser.add_argument("--personality", 
                    help="Set a personality/role for the AI. Available: cfp, swe, teacher, doctor, lawyer, chef, therapist, scientist, historian, artist, coach, journalist, philosopher, librarian, tutor, consultant, translator, critic, comedian, mentor, investor, cybersecurity. You can also use custom descriptions.")
parser.add_argument("--emojis", nargs="?", const="light", choices=["light", "heavy"],
                    help="Encourage emoji usage in responses. 'light' (default) uses emojis sparingly, 'heavy' uses them frequently.")
parser.add_argument("--enhance-emojis", action="store_true", 
                    help="Post-process the response to add appropriate emojis using a secondary AI call.")
args = parser.parse_args()

# Personality definitions
PERSONALITIES = {
    "cfp": "You are a Certified Financial Planner (CFP) with extensive knowledge of personal finance, investment strategies, tax planning, retirement planning, and estate planning. Provide professional, ethical financial advice while always reminding users to consult with their own financial advisor for personalized guidance.",
    
    "swe": "You are a Senior Software Engineer with deep expertise in software architecture, design patterns, code quality, and best practices. Focus on writing clean, maintainable, and efficient code. Consider scalability, security, and performance in your recommendations.",
    
    "teacher": "You are a patient and encouraging teacher who excels at breaking down complex topics into understandable concepts. Use analogies, examples, and step-by-step explanations. Adapt your teaching style to the learner's level and encourage questions.",
    
    "doctor": "You are a knowledgeable medical professional who provides general health information and guidance. Always emphasize that your advice is educational only and users should consult with healthcare providers for medical diagnosis and treatment.",
    
    "lawyer": "You are a legal expert who provides general legal information and explains legal concepts clearly. Always clarify that this is not legal advice and users should consult with an attorney for specific legal matters.",
    
    "chef": "You are a professional chef with expertise in various cuisines, cooking techniques, and food science. Share recipes, cooking tips, and culinary knowledge with enthusiasm. Consider dietary restrictions and preferences when making suggestions.",
    
    "therapist": "You are a supportive mental health professional who provides emotional support and coping strategies. Always encourage users to seek professional help for serious mental health concerns while offering compassionate guidance.",
    
    "scientist": "You are a research scientist with broad knowledge across multiple scientific disciplines. Explain scientific concepts clearly, cite evidence-based information, and maintain scientific accuracy while making complex topics accessible.",
    
    "historian": "You are a historian with deep knowledge of world history, cultures, and historical analysis. Provide context, multiple perspectives, and help users understand how past events connect to the present.",
    
    "artist": "You are a creative artist with expertise in various art forms, techniques, and art history. Encourage creativity, provide constructive feedback, and help users explore their artistic expression.",
    
    "coach": "You are a life coach focused on helping people achieve their goals, overcome obstacles, and maximize their potential. Use motivational techniques, ask powerful questions, and help users create actionable plans.",
    
    "journalist": "You are an investigative journalist skilled in research, fact-checking, and clear communication. Help users understand complex issues, identify reliable sources, and think critically about information.",
    
    "philosopher": "You are a philosopher who explores deep questions about existence, ethics, knowledge, and reality. Engage in thoughtful dialogue, present multiple philosophical perspectives, and encourage critical thinking.",
    
    "librarian": "You are a research librarian with expertise in finding, evaluating, and organizing information. Help users with research strategies, source evaluation, and information literacy.",
    
    "tutor": "You are a subject-specific tutor who helps students understand difficult concepts, complete assignments, and prepare for exams. Focus on building understanding rather than just providing answers.",
    
    "consultant": "You are a business consultant with expertise in strategy, operations, and organizational development. Provide practical business advice, analyze problems systematically, and offer actionable recommendations.",
    
    "translator": "You are a professional translator and linguist with expertise in multiple languages and cultures. Help with translations, explain linguistic nuances, and provide cultural context.",
    
    "critic": "You are a thoughtful critic who analyzes literature, films, art, and media with depth and insight. Provide balanced critiques that consider both strengths and weaknesses while respecting different perspectives.",
    
    "comedian": "You are a witty comedian who uses humor appropriately to lighten conversations while remaining helpful. Balance entertainment with usefulness, and be sensitive to context and audience.",
    
    "mentor": "You are a wise mentor who guides others based on experience and wisdom. Offer perspective, share relevant experiences, and help mentees navigate challenges while encouraging their growth.",
    
    "investor": "You are an experienced investment professional with deep knowledge of financial markets, portfolio management, risk assessment, and investment strategies. Provide insights on stocks, bonds, real estate, alternative investments, and market analysis while emphasizing the importance of diversification and risk management.",
    
    "cybersecurity": "You are a cybersecurity expert with extensive knowledge of information security, threat analysis, network security, and digital privacy. Help users understand security best practices, identify vulnerabilities, and implement protective measures while staying current with emerging threats and security technologies."
}

def get_personality_prompt(personality_key):
    """Get the personality prompt for the given key, or return empty string if not found."""
    if not personality_key:
        return ""
    
    personality_key = personality_key.lower()
    if personality_key in PERSONALITIES:
        return PERSONALITIES[personality_key] + "\n\n"
    else:
        # If not a predefined personality, treat it as a custom personality description
        return f"You are {personality_key}.\n\n"

def get_emoji_prompt(emoji_mode):
    """Get the emoji prompt for the given mode."""
    if not emoji_mode:
        return ""
    
    emoji_prompts = {
        "light": "Feel free to use relevant emojis to enhance your response where appropriate.",
        "heavy": "Use emojis frequently throughout your response to make it more engaging and expressive. Include emojis to highlight key points, convey emotions, and make the content more visually appealing."
    }
    
    return emoji_prompts.get(emoji_mode, emoji_prompts["light"]) + "\n\n"

def enhance_with_emojis(text, model="gemma3:12b"):
    """Enhance text with emojis using a secondary AI call to the local Ollama instance."""
    if not text or not text.strip():
        return text
    
    enhancement_prompt = f"""Please add appropriate emojis to the following text to make it more engaging and expressive. Follow these guidelines:

1. Add emojis that are relevant and enhance understanding
2. Don't overdo it - use emojis strategically
3. Maintain the original meaning and tone
4. Place emojis at the BEGINNING of section headings, titles, and key points (e.g., "üöÄ Getting Started" not "Getting Started üöÄ")
5. For bullet points and lists, place emojis at the start of each item
6. Use emojis to emphasize important concepts, but place them before the text they're emphasizing
7. Return only the enhanced text with emojis, no additional commentary

Original text:
{text}"""

    try:
        headers = {
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": enhancement_prompt
                }
            ],
            "stream": False
        }
        
        response = requests.post(f"{args.endpoint}/api/chat", 
                               headers=headers, 
                               json=data, 
                               timeout=30)
        
        if response.status_code == 200:
            response_data = response.json()
            if "message" in response_data and "content" in response_data["message"]:
                return response_data["message"]["content"].strip()
        
        # Return original text if enhancement fails
        return text
        
    except Exception as e:
        print(f"Warning: Emoji enhancement failed ({e}), returning original text", file=sys.stderr)
        return text

def get_auto_numbered_filename(base_name="output", extension=".png"):
    """Generate an auto-numbered filename that doesn't exist yet."""
    counter = 0
    while True:
        filename = f"{base_name}-{counter:04d}{extension}"
        if not os.path.exists(filename):
            return filename
        counter += 1

def encode_image_to_base64(image_path):
    """Encode an image file to base64 for Ollama API."""
    try:
        with open(image_path, 'rb') as image_file:
            image_data = image_file.read()
            encoded_string = base64.b64encode(image_data).decode('utf-8')
            return encoded_string
    except Exception as e:
        print(f"Error reading image {image_path}: {e}", file=sys.stderr)
        exit(1)

def load_context_from_file(date_str):
    """Load chat history from a specific date's markdown file."""
    base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
    filename = f"{date_str}.md"
    file_path = base_path / filename
    
    if not file_path.exists():
        if args.debug:
            print(f"Debug: No context file found at {file_path}", file=sys.stderr)
        return None
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if args.debug:
            print(f"Debug: Loaded {len(content)} characters of context from {filename}", file=sys.stderr)
        
        return f"=== Previous Chat History from {date_str} ===\n\n{content}\n\n=== End of Previous Chat History ===\n\n"
    except Exception as e:
        print(f"Warning: Could not load context from {file_path}: {e}", file=sys.stderr)
        return None

def calculate_cost(model, input_tokens, output_tokens):
    """Calculate cost for Ollama (typically free for local models)."""
    # Ollama running locally is typically free
    # If using a remote Ollama server, costs would depend on that setup
    return {
        "input_cost": 0.0,
        "output_cost": 0.0,
        "total_cost": 0.0
    }

def print_summary_table(model, input_tokens, output_tokens, total_time, first_token_time, cost_info, is_estimated=False, use_color=True):
    """Print a formatted summary table with color support."""
    # ANSI color codes (conditionally set based on use_color)
    if use_color:
        CYAN = '\033[96m'
        GREEN = '\033[92m'
        YELLOW = '\033[93m'
        BLUE = '\033[94m'
        MAGENTA = '\033[95m'
        BOLD = '\033[1m'
        END = '\033[0m'
    else:
        CYAN = GREEN = YELLOW = BLUE = MAGENTA = BOLD = END = ''
    
    # Calculate total tokens
    total_tokens = input_tokens + output_tokens
    
    # Table width (inner content width)
    width = 50
    
    # Helper to strip ANSI codes
    import re
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    
    def strip_ansi(text):
        return ansi_escape.sub('', text)
    
    # Helper to create a padded line
    def make_line(left, right, left_width=22):
        """Create a line with left and right parts, properly padded."""
        left_plain = strip_ansi(left)
        right_plain = strip_ansi(right)
        
        # Total inner content must be exactly 48 characters
        # Left part: left_text + padding to reach left_width
        left_part = left + ' ' * (left_width - len(left_plain))
        
        # Right part: right_text + padding to fill remaining space
        remaining_width = 48 - left_width - 1  # -1 for space between left and right
        right_part = right + ' ' * (remaining_width - len(right_plain))
        
        # Combine: left_part + space + right_part = exactly 48 chars
        content = f"{left_part} {right_part}"
        
        return f"{CYAN}‚ïë{END} {content} {CYAN}‚ïë{END}"
    
    # Build the table
    print(f"\n{BOLD}{CYAN}‚ïî{'‚ïê' * width}‚ïó{END}", file=sys.stderr)
    
    # Title
    title = "API Usage Summary"
    title_pad_left = (width - len(title)) // 2
    title_pad_right = width - len(title) - title_pad_left
    print(f"{BOLD}{CYAN}‚ïë{' ' * title_pad_left}{title}{' ' * title_pad_right}‚ïë{END}", file=sys.stderr)
    
    print(f"{BOLD}{CYAN}‚ï†{'‚ïê' * width}‚ï£{END}", file=sys.stderr)
    
    # Model info
    model_display = model if len(model) <= 30 else model[:27] + "..."
    print(make_line(f"{BOLD}Model:{END}", f"{GREEN}{model_display}{END}"), file=sys.stderr)
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Token usage
    print(make_line(f"{BOLD}Token Usage:{END}", ""), file=sys.stderr)
    
    est_marker = " (est)" if is_estimated else ""
    print(make_line("  Input:", f"{YELLOW}{input_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Output:", f"{YELLOW}{output_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(make_line("  Total:", f"{BOLD}{YELLOW}{total_tokens:,}{est_marker}{END} tokens"), file=sys.stderr)
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Performance
    print(make_line(f"{BOLD}Performance:{END}", ""), file=sys.stderr)
    
    if first_token_time:
        print(make_line("  Time to first token:", f"{BLUE}{first_token_time:.2f}s{END}"), file=sys.stderr)
    
    print(make_line("  Total time:", f"{BLUE}{total_time:.2f}s{END}"), file=sys.stderr)
    
    if output_tokens > 0 and total_time > 0:
        tokens_per_sec = output_tokens / total_time
        print(make_line("  Tokens/second:", f"{BLUE}{tokens_per_sec:.1f}{END}"), file=sys.stderr)
    
    print(f"{CYAN}‚ï†{'‚îÄ' * width}‚ï£{END}", file=sys.stderr)
    
    # Cost (typically free for local Ollama)
    print(make_line(f"{BOLD}Cost Breakdown:{END}", ""), file=sys.stderr)
    
    if cost_info['total_cost'] == 0.0:
        print(make_line("  Status:", f"{GREEN}Free (Local){END}"), file=sys.stderr)
    else:
        print(make_line("  Input cost:", f"{MAGENTA}${cost_info['input_cost']:.6f}{END}"), file=sys.stderr)
        print(make_line("  Output cost:", f"{MAGENTA}${cost_info['output_cost']:.6f}{END}"), file=sys.stderr)
        print(make_line(f"  {BOLD}Total cost:{END}", f"{BOLD}{MAGENTA}${cost_info['total_cost']:.6f}{END}"), file=sys.stderr)
    
    print(f"{BOLD}{CYAN}‚ïö{'‚ïê' * width}‚ïù{END}", file=sys.stderr)

def save_chat_transaction(provider, model, prompt, response, metadata=None):
    """Save chat transaction to markdown file organized by date."""
    # Create directory structure if it doesn't exist
    base_path = Path("/var/home/zach/Documents/notes/03_resources/ai_chats/providers")
    base_path.mkdir(parents=True, exist_ok=True)
    
    # Generate filename based on today's date
    today = datetime.now()
    filename = today.strftime("%Y-%m-%d.md")
    file_path = base_path / filename
    
    # Generate timestamp for section header
    timestamp = today.strftime("%Y-%m-%d %H:%M:%S")
    
    # Prepare content
    content = f"\n# {timestamp}\n\n"
    content += f"## Prompt\n\n```\n{prompt}\n```\n\n"
    content += f"## Response\n\n{response}\n\n"
    
    # Add metadata if provided
    if metadata:
        content += f"## Metadata\n\n"
        content += f"- **Provider**: {provider}\n"
        content += f"- **Model**: {model}\n"
        if 'input_tokens' in metadata:
            content += f"- **Input Tokens**: {metadata['input_tokens']:,}\n"
        if 'output_tokens' in metadata:
            content += f"- **Output Tokens**: {metadata['output_tokens']:,}\n"
        if 'total_tokens' in metadata:
            content += f"- **Total Tokens**: {metadata['total_tokens']:,}\n"
        if 'cost_info' in metadata:
            cost_info = metadata['cost_info']
            content += f"- **Input Cost**: ${cost_info['input_cost']:.6f}\n"
            content += f"- **Output Cost**: ${cost_info['output_cost']:.6f}\n"
            content += f"- **Total Cost**: ${cost_info['total_cost']:.6f}\n"
        if 'total_time' in metadata:
            content += f"- **Total Time**: {metadata['total_time']:.2f}s\n"
        if 'first_token_time' in metadata and metadata['first_token_time']:
            content += f"- **Time to First Token**: {metadata['first_token_time']:.2f}s\n"
        if 'stream_mode' in metadata:
            content += f"- **Streaming**: {metadata['stream_mode']}\n"
        if 'images' in metadata and metadata['images']:
            content += f"- **Images**: {len(metadata['images'])} included\n"
        if 'endpoint' in metadata:
            content += f"- **Endpoint**: {metadata['endpoint']}\n"
    
    content += "\n---\n"
    
    # Write to file (append if exists)
    try:
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        print(f"Warning: Could not save chat transaction: {e}", file=sys.stderr)

def get_ollama_models():
    """Get list of available Ollama models."""
    try:
        url = f"{args.endpoint}/api/tags"
        response = requests.get(url)
        if response.status_code == 200:
            models_data = response.json()
            if "models" in models_data:
                return [model["name"] for model in models_data["models"]]
    except:
        pass
    return []

def test_model_performance(model, prompt):
    """Test a single model's performance."""
    try:
        # Prepare request
        url = f"{args.endpoint}/api/generate"
        data = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        
        # Time the request
        start_time = time.time()
        response = requests.post(url, json=data, timeout=60)
        end_time = time.time()
        
        if response.status_code == 200:
            response_data = response.json()
            total_time = end_time - start_time
            
            # Extract metrics
            result = {
                "model": model,
                "response": response_data.get("response", ""),
                "total_time": total_time,
                "eval_count": response_data.get("eval_count", 0),
                "prompt_eval_count": response_data.get("prompt_eval_count", 0),
                "eval_duration": response_data.get("eval_duration", 0) / 1e9,  # Convert to seconds
                "prompt_eval_duration": response_data.get("prompt_eval_duration", 0) / 1e9,
                "total_duration": response_data.get("total_duration", 0) / 1e9,
                "tokens_per_second": 0
            }
            
            # Calculate tokens per second
            if result["eval_duration"] > 0:
                result["tokens_per_second"] = result["eval_count"] / result["eval_duration"]
            
            return result
        else:
            return {"model": model, "error": f"HTTP {response.status_code}"}
            
    except Exception as e:
        return {"model": model, "error": str(e)}

def test_other_providers(prompt):
    """Test other AI providers for comparison."""
    providers = [
        {"name": "Claude", "cmd": ["claudpy", "--no-preserve"], "model": "claude-sonnet-4-20250514"},
        {"name": "Grok", "cmd": ["grokpy", "--no-preserve"], "model": "grok-3-beta"},
        {"name": "Gemini", "cmd": ["geminpy", "--no-preserve"], "model": "gemini-2.5-pro-preview-06-05"},
        {"name": "OpenAI", "cmd": ["openpy", "--no-preserve"], "model": "gpt-4-turbo"},
        {"name": "Perplexity", "cmd": ["perpy", "--no-preserve"], "model": "sonar-pro"}
    ]
    
    results = []
    for provider in providers:
        try:
            # Run the provider command
            start_time = time.time()
            result = subprocess.run(
                provider["cmd"],
                input=prompt,
                capture_output=True,
                text=True,
                timeout=30
            )
            end_time = time.time()
            
            if result.returncode == 0:
                results.append({
                    "provider": provider["name"],
                    "model": provider["model"],
                    "response": result.stdout.strip(),
                    "total_time": end_time - start_time,
                    "error": None
                })
            else:
                results.append({
                    "provider": provider["name"],
                    "model": provider["model"],
                    "response": "",
                    "total_time": end_time - start_time,
                    "error": result.stderr.strip()
                })
        except subprocess.TimeoutExpired:
            results.append({
                "provider": provider["name"],
                "model": provider["model"],
                "response": "",
                "total_time": 30.0,
                "error": "Timeout"
            })
        except Exception as e:
            results.append({
                "provider": provider["name"],
                "model": provider["model"],
                "response": "",
                "total_time": 0,
                "error": str(e)
            })
    
    return results

def calculate_similarity(text1, text2):
    """Calculate similarity between two texts using difflib."""
    return difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def print_performance_table(ollama_results, other_results):
    """Print a formatted performance comparison table."""
    # ANSI colors
    use_colors = not args.no_color
    CYAN = '\033[96m' if use_colors else ''
    GREEN = '\033[92m' if use_colors else ''
    YELLOW = '\033[93m' if use_colors else ''
    RED = '\033[91m' if use_colors else ''
    BLUE = '\033[94m' if use_colors else ''
    MAGENTA = '\033[95m' if use_colors else ''
    GRAY = '\033[90m' if use_colors else ''
    BOLD = '\033[1m' if use_colors else ''
    END = '\033[0m' if use_colors else ''
    
    # Get reference response (first successful provider response)
    reference_response = ""
    for result in other_results:
        if not result.get("error") and result["response"]:
            reference_response = result["response"]
            break
    
    # If no reference from other providers, use first successful Ollama response
    if not reference_response:
        for result in ollama_results:
            if not result.get("error") and result.get("response"):
                reference_response = result["response"]
                break
    
    # Print header
    print(f"\n{BOLD}{CYAN}‚ïî{'‚ïê' * 120}‚ïó{END}")
    # Center the title: 120 - 2 (borders) = 118 inner width, title is 27 chars
    # (118 - 27) / 2 = 45.5, so 45 before and 46 after
    print(f"{BOLD}{CYAN}‚ïë{' ' * 45}Performance Profile Results{' ' * 46}‚ïë{END}")
    print(f"{BOLD}{CYAN}‚ï†{'‚ïê' * 120}‚ï£{END}")
    
    # Print prompt - width calculation: 120 - 2 (borders) - 1 (start space) - 7 (Prompt:) - 1 (space after) - 1 (end space) = 108
    print(f"{BOLD}{CYAN}‚ïë{END} {BOLD}Prompt:{END} {args.profile_prompt[:108]:<108} {BOLD}{CYAN}‚ïë{END}")
    if len(args.profile_prompt) > 108:
        remaining = args.profile_prompt[108:]
        while remaining:
            # For continuation lines: 120 - 2 (borders) - 1 (start space) - 8 (indent) - 1 (end space) = 108
            print(f"{BOLD}{CYAN}‚ïë{END} {' ' * 8}{remaining[:108]:<108} {BOLD}{CYAN}‚ïë{END}")
            remaining = remaining[108:]
    
    print(f"{BOLD}{CYAN}‚ï†{'‚ïê' * 120}‚ï£{END}")
    
    # Print Ollama models section
    # Width calculation: 120 - 2 (borders) - 1 (start space) - 13 (text) - 1 (end space) = 103
    print(f"{BOLD}{CYAN}‚ïë{END} {BOLD}{GREEN}Ollama Models{END}{' ' * 103} {BOLD}{CYAN}‚ïë{END}")
    print(f"{BOLD}{CYAN}‚ï†{'‚îÄ' * 120}‚ï£{END}")
    
    # Header row - total width must be 120
    # Calculate Status width: 120 - 2 (borders) - 1 (start space) - 25 - 1 - 10 - 1 - 12 - 1 - 15 - 1 - 12 - 1 = 39
    print(f"{BOLD}{CYAN}‚ïë{END} {BOLD}{'Model':<25} {'Time (s)':<10} {'Tokens/s':<12} {'Output Tokens':<15} {'Similarity':<12} {'Status':<39}{END} {BOLD}{CYAN}‚ïë{END}")
    print(f"{BOLD}{CYAN}‚ï†{'‚îÄ' * 120}‚ï£{END}")
    
    # Ollama results
    for i, result in enumerate(ollama_results):
        model = result["model"][:25]
        
        if result.get("error"):
            status_text = f"Error: {result['error'][:20]}"
            status_colored = f"{RED}{status_text}{END}"
            time_str = "-"
            tps_str = "-"
            tokens_str = "-"
            sim_str = "-"
            sim_colored = "-"
        else:
            status_text = "Success"
            status_colored = f"{GREEN}{status_text}{END}"
            time_str = f"{result['total_time']:.2f}"
            tps_str = f"{result['tokens_per_second']:.1f}" if result.get('tokens_per_second') else "-"
            tokens_str = str(result.get('eval_count', '-'))
            
            # Calculate similarity
            if reference_response and result.get("response"):
                similarity = calculate_similarity(reference_response, result["response"])
                sim_str = f"{similarity:.1%}"
                if similarity >= 0.8:
                    sim_colored = f"{GREEN}{sim_str}{END}"
                elif similarity >= 0.6:
                    sim_colored = f"{YELLOW}{sim_str}{END}"
                else:
                    sim_colored = f"{RED}{sim_str}{END}"
            else:
                sim_str = "-"
                sim_colored = "-"
        
        # Build row with exact spacing to match header
        # Header format: ‚ïë Model(25) Time(10) Tokens/s(12) Output Tokens(15) Similarity(12) Status(30) ‚ïë
        # Total content width should be: 25+10+12+15+12+30 + 5 spaces = 109 chars
        row_parts = []
        row_parts.append(f"{BOLD}{CYAN}‚ïë{END} ")
        row_parts.append(f"{model:<25} ")
        row_parts.append(f"{time_str:<10} ")
        row_parts.append(f"{tps_str:<12} ")
        row_parts.append(f"{tokens_str:<15} ")
        
        # Handle colored fields with padding
        if sim_colored == "-":
            row_parts.append(f"{sim_str:<12} ")
        else:
            # Add the colored text and pad with spaces
            padding_needed = 12 - len(sim_str)
            row_parts.append(sim_colored + (" " * padding_needed) + " ")
            
        if status_colored == status_text:  # No color
            row_parts.append(f"{status_text:<39}")
        else:
            # Add the colored text and pad with spaces
            padding_needed = 39 - len(status_text)
            row_parts.append(status_colored + (" " * padding_needed))
            
        row_parts.append(f"{END} {BOLD}{CYAN}‚ïë{END}")
        
        print("".join(row_parts))
        
        # Show output if requested
        if args.profile_show_output and result.get("response"):
            output_lines = result["response"].strip().split('\n')
            for line in output_lines:
                # Wrap long lines (account for table structure: ‚ïë + 3 spaces + text + space + ‚ïë)
                # Total width is 120, so content width = 120 - 2 (‚ïë) - 3 (indent) - 1 (space) - 1 (‚ïë) = 113
                while line:
                    chunk = line[:113]
                    line = line[113:]
                    # Pad the chunk to exactly 113 characters
                    padded_chunk = chunk.ljust(113)
                    print(f"{BOLD}{CYAN}‚ïë{END}   {GRAY}{padded_chunk}{END} {BOLD}{CYAN}‚ïë{END}")
            # Add separator after output (but not after the last item in section)
            if i < len(ollama_results) - 1:
                print(f"{BOLD}{CYAN}‚ïü{'‚îÄ' * 120}‚ï¢{END}")
    
    # Other providers section
    print(f"{BOLD}{CYAN}‚ï†{'‚îÄ' * 120}‚ï£{END}")
    # Width calculation: 120 - 2 (borders) - 1 (start space) - 36 (text) - 1 (end space) = 80
    print(f"{BOLD}{CYAN}‚ïë{END} {BOLD}{BLUE}Other AI Providers (for comparison){END}{' ' * 80} {BOLD}{CYAN}‚ïë{END}")
    print(f"{BOLD}{CYAN}‚ï†{'‚îÄ' * 120}‚ï£{END}")
    
    # Header row - total width must be 120
    # Calculate Status width: 120 - 2 (borders) - 1 (start space) - 15 - 1 - 35 - 1 - 10 - 1 - 12 - 1 = 42
    print(f"{BOLD}{CYAN}‚ïë{END} {BOLD}{'Provider':<15} {'Model':<35} {'Time (s)':<10} {'Similarity':<12} {'Status':<42}{END} {BOLD}{CYAN}‚ïë{END}")
    print(f"{BOLD}{CYAN}‚ï†{'‚îÄ' * 120}‚ï£{END}")
    
    # Other provider results
    for i, result in enumerate(other_results):
        provider = result["provider"][:15]
        model = result["model"][:35]
        
        if result.get("error"):
            status_text = f"Error: {result['error'][:25]}"
            status_colored = f"{RED}{status_text}{END}"
            time_str = "-"
            sim_str = "-"
            sim_colored = "-"
        else:
            status_text = "Success"
            status_colored = f"{GREEN}{status_text}{END}"
            time_str = f"{result['total_time']:.2f}"
            
            # Calculate similarity
            if reference_response and result["response"]:
                similarity = calculate_similarity(reference_response, result["response"])
                sim_str = f"{similarity:.1%}"
                if similarity >= 0.8:
                    sim_colored = f"{GREEN}{sim_str}{END}"
                elif similarity >= 0.6:
                    sim_colored = f"{YELLOW}{sim_str}{END}"
                else:
                    sim_colored = f"{RED}{sim_str}{END}"
            else:
                sim_str = "-"
                sim_colored = "-"
        
        # Build row with exact spacing to match header
        # Header format: ‚ïë Provider(15) Model(35) Time(10) Similarity(12) Status(36) ‚ïë
        row_parts = []
        row_parts.append(f"{BOLD}{CYAN}‚ïë{END} ")
        row_parts.append(f"{provider:<15} ")
        row_parts.append(f"{model:<35} ")
        row_parts.append(f"{time_str:<10} ")
        
        # Handle colored similarity field
        if sim_colored == "-":
            row_parts.append(f"{sim_str:<12} ")
        else:
            # Add the colored text and pad with spaces
            padding_needed = 12 - len(sim_str)
            row_parts.append(sim_colored + (" " * padding_needed) + " ")
            
        # Handle colored status field
        if status_colored == status_text:  # No color
            row_parts.append(f"{status_text:<42}")
        else:
            # Add the colored text and pad with spaces
            padding_needed = 42 - len(status_text)
            row_parts.append(status_colored + (" " * padding_needed))
            
        row_parts.append(f"{END} {BOLD}{CYAN}‚ïë{END}")
        
        print("".join(row_parts))
        
        # Show output if requested
        if args.profile_show_output and result.get("response"):
            output_lines = result["response"].strip().split('\n')
            for line in output_lines:
                # Wrap long lines (account for table structure: ‚ïë + 3 spaces + text + space + ‚ïë)
                # Total width is 120, so content width = 120 - 2 (‚ïë) - 3 (indent) - 1 (space) - 1 (‚ïë) = 113
                while line:
                    chunk = line[:113]
                    line = line[113:]
                    # Pad the chunk to exactly 113 characters
                    padded_chunk = chunk.ljust(113)
                    print(f"{BOLD}{CYAN}‚ïë{END}   {GRAY}{padded_chunk}{END} {BOLD}{CYAN}‚ïë{END}")
            # Add separator after output (but not after the last item)
            if i < len(other_results) - 1:
                print(f"{BOLD}{CYAN}‚ïü{'‚îÄ' * 120}‚ï¢{END}")
    
    print(f"{BOLD}{CYAN}‚ïö{'‚ïê' * 120}‚ïù{END}")
    
    # Print legend
    print(f"\n{BOLD}Legend:{END}")
    print(f"  Similarity: {GREEN}‚â•80% (High){END}, {YELLOW}60-79% (Medium){END}, {RED}<60% (Low){END}")
    print(f"  Similarity is calculated against the first successful response as reference")
    print(f"  Local Ollama models typically have no cost, while cloud providers charge per token\n")

# Since outside of the distrobox we may not have these modules
# quietly ignore the fact that they may not exist
try:
    # Handle profile-performance option first
    if args.profile_performance:
        print("Starting performance profiling...", file=sys.stderr)
        
        # Determine which models to test
        if args.profile_models:
            # Use specified models
            specified_models = [m.strip() for m in args.profile_models.split(',') if m.strip()]
            
            # Get all available models to validate
            available_models = get_ollama_models()
            if not available_models:
                print("Error: No Ollama models found. Make sure Ollama is running.", file=sys.stderr)
                exit(1)
            
            # Filter to only available models
            ollama_models = []
            unavailable = []
            for model in specified_models:
                if model in available_models:
                    ollama_models.append(model)
                else:
                    unavailable.append(model)
            
            if unavailable:
                print(f"Warning: The following models are not available: {', '.join(unavailable)}", file=sys.stderr)
            
            if not ollama_models:
                print("Error: None of the specified models are available.", file=sys.stderr)
                print(f"Available models: {', '.join(available_models)}", file=sys.stderr)
                exit(1)
                
            print(f"Testing {len(ollama_models)} specified models: {', '.join(ollama_models)}", file=sys.stderr)
        else:
            # Get all available Ollama models
            ollama_models = get_ollama_models()
            if not ollama_models:
                print("Error: No Ollama models found. Make sure Ollama is running.", file=sys.stderr)
                exit(1)
            
            print(f"Found {len(ollama_models)} Ollama models to test", file=sys.stderr)
        
        # Test Ollama models
        ollama_results = []
        for i, model in enumerate(ollama_models):
            print(f"Testing Ollama model {i+1}/{len(ollama_models)}: {model}...", file=sys.stderr)
            result = test_model_performance(model, args.profile_prompt)
            ollama_results.append(result)
        
        # Test other providers
        print("\nTesting other AI providers for comparison...", file=sys.stderr)
        other_results = test_other_providers(args.profile_prompt)
        
        # Print results table
        print_performance_table(ollama_results, other_results)
        
        exit(0)
    
    # Handle list-models option
    if args.list_models:
        # Check if colors should be disabled
        use_colors = not args.no_color and environ.get("NO_COLOR", "").lower() not in ("1", "true")
        
        # Fetch available models from the Ollama API
        url = f"{args.endpoint}/api/tags"
        
        if args.debug:
            print(f"Debug: Listing models from {url}", file=sys.stderr)
        
        try:
            response = requests.get(url)
            if response.status_code != 200:
                print(f"Error fetching models: {response.status_code}", file=sys.stderr)
                print(response.text, file=sys.stderr)
                exit(1)
            
            models_data = response.json()
            if "models" in models_data:
                # ANSI color codes (conditionally set based on use_colors)
                class Colors:
                    HEADER = '\033[95m' if use_colors else ''
                    BLUE = '\033[94m' if use_colors else ''
                    CYAN = '\033[96m' if use_colors else ''
                    GREEN = '\033[92m' if use_colors else ''
                    YELLOW = '\033[93m' if use_colors else ''
                    RED = '\033[91m' if use_colors else ''
                    BOLD = '\033[1m' if use_colors else ''
                    UNDERLINE = '\033[4m' if use_colors else ''
                    END = '\033[0m' if use_colors else ''
                
                # Prepare data for table
                table_data = []
                for model in models_data["models"]:
                    name = model.get("name", "Unknown")
                    size = model.get("size", 0)
                    modified = model.get("modified_at", "Unknown")
                    
                    # Format size in human-readable format
                    if size > 0:
                        if size >= 1024**3:  # GB
                            size_str = f"{size / (1024**3):.1f}GB"
                        elif size >= 1024**2:  # MB
                            size_str = f"{size / (1024**2):.1f}MB"
                        else:  # Bytes
                            size_str = f"{size}B"
                    else:
                        size_str = "Unknown"
                    
                    # Format modified date
                    if modified != "Unknown":
                        try:
                            from datetime import datetime
                            # Parse ISO format and display human-readable
                            dt = datetime.fromisoformat(modified.replace('Z', '+00:00'))
                            modified_str = dt.strftime("%Y-%m-%d %H:%M")
                        except:
                            modified_str = modified
                    else:
                        modified_str = "Unknown"
                    
                    table_data.append([name, size_str, modified_str])
                
                # Calculate column widths
                if table_data:
                    max_name = max(len(row[0]) for row in table_data)
                    max_size = max(len(row[1]) for row in table_data)
                    max_date = max(len(row[2]) for row in table_data)
                    
                    # Ensure minimum widths
                    max_name = max(max_name, len("Model Name"))
                    max_size = max(max_size, len("Size"))
                    max_date = max(max_date, len("Modified"))
                    
                    # Print header
                    print(f"\n{Colors.BOLD}{Colors.CYAN}Available Ollama Models:{Colors.END}")
                    print(f"{Colors.BOLD}{'‚îÄ' * (max_name + max_size + max_date + 6)}{Colors.END}")
                    
                    # Print table header
                    print(f"{Colors.BOLD}{Colors.HEADER}{'Model Name':<{max_name}} {'Size':<{max_size}} {'Modified':<{max_date}}{Colors.END}")
                    print(f"{Colors.BOLD}{'‚îÄ' * max_name} {'‚îÄ' * max_size} {'‚îÄ' * max_date}{Colors.END}")
                    
                    # Print table rows
                    for i, (name, size_str, modified_str) in enumerate(table_data):
                        # Alternate row colors
                        color = Colors.CYAN if i % 2 == 0 else Colors.BLUE
                        size_color = Colors.GREEN if size_str != "Unknown" else Colors.YELLOW
                        date_color = Colors.GREEN if modified_str != "Unknown" else Colors.YELLOW
                        
                        print(f"{color}{name:<{max_name}}{Colors.END} "
                              f"{size_color}{size_str:<{max_size}}{Colors.END} "
                              f"{date_color}{modified_str:<{max_date}}{Colors.END}")
                    
                    print(f"{Colors.BOLD}{'‚îÄ' * (max_name + max_size + max_date + 6)}{Colors.END}")
                    print(f"{Colors.BOLD}Total models: {Colors.GREEN}{len(table_data)}{Colors.END}\n")
                else:
                    print(f"{Colors.YELLOW}No models found{Colors.END}")
            else:
                print("No models found or unexpected response format")
                if args.debug:
                    print(f"Debug: Response: {json.dumps(models_data)}", file=sys.stderr)
            
        except requests.RequestException as e:
            print(f"Error connecting to Ollama server: {e}", file=sys.stderr)
            exit(1)
        
        exit(0)
    
    # Read file contents if any files were specified
    file_contents = []
    if args.files:
        for file_path in args.files:
            try:
                with open(file_path, 'r') as f:
                    file_content = f.read()
                    file_contents.append(f"=== File: {file_path} ===\n{file_content}\n")
            except IOError as e:
                print(f"Warning: Could not read file {file_path}: {e}", file=stderr)
    
    # Read from standard input only if there's data available
    query = ""
    # Check if stdin has data (not a terminal and has content)
    if not stdin.isatty() or stdin in select.select([stdin], [], [], 0)[0]:
        query = stdin.read()
    # If no stdin and no prompt/files provided, and not doing image generation or regen, show error
    elif not args.prompt and not args.files and not args.image_gen and not args.regen and not args.images:
        print("Error: No input provided. Use --prompt, -f/--file, or pipe input via stdin", file=stderr)
        exit(1)

    # Combine file contents with query
    combined_parts = []
    
    # Add file contents first if any
    if file_contents:
        combined_parts.extend(file_contents)
    
    # Add prompt if provided
    if args.prompt:
        combined_parts.append(args.prompt)
    
    # Add stdin content if any
    if query:
        combined_parts.append(query)
    
    # Combine all parts
    if combined_parts:
        query = "\n".join(combined_parts)
    else:
        query = ""

    # Load context if requested
    context_parts = []
    
    # Load today's context if --use-context is specified
    if args.use_context:
        today_str = datetime.now().strftime("%Y-%m-%d")
        context = load_context_from_file(today_str)
        if context:
            context_parts.append(context)
    
    # Load specific date's context if --use-context-from is specified
    if args.use_context_from:
        try:
            # Validate date format
            datetime.strptime(args.use_context_from, "%Y-%m-%d")
            context = load_context_from_file(args.use_context_from)
            if context:
                context_parts.append(context)
        except ValueError:
            print(f"Error: Invalid date format '{args.use_context_from}'. Use YYYY-MM-DD format.", file=stderr)
            exit(1)
    
    # Prepend context to query if any context was loaded
    if context_parts:
        context_str = "\n".join(context_parts)
        if query:
            query = context_str + "\nCurrent Query:\n" + query
        else:
            query = context_str
    
    # Prepend personality and emoji prompts if specified
    prompt_parts = []
    
    # Add personality prompt if specified
    if args.personality:
        personality_prompt = get_personality_prompt(args.personality)
        if personality_prompt:
            prompt_parts.append(personality_prompt)
    
    # Add emoji prompt if specified
    if args.emojis:
        emoji_prompt = get_emoji_prompt(args.emojis)
        if emoji_prompt:
            prompt_parts.append(emoji_prompt)
    
    # Combine prompts with original query
    if prompt_parts:
        combined_prompt = "".join(prompt_parts)
        if query:
            query = combined_prompt + query
        else:
            query = combined_prompt.rstrip()  # Remove trailing newlines if no other content

    # Set up API call
    headers = {
        "Content-Type": "application/json"
    }

    # Set the API endpoint based on the request type
    if args.embedding:
        # Use the embeddings endpoint for Ollama
        url = f"{args.endpoint}/api/embeddings"
        
        # For embeddings, we use a different request format
        data = {
            "model": args.model,
            "prompt": query  # Ollama uses "prompt" instead of "input" for embeddings
        }
    elif args.regen:
        # Note: Ollama doesn't support image generation, only vision analysis
        print("Error: Image regeneration is not supported by Ollama.", file=sys.stderr)
        print("Ollama only supports vision models for analyzing images, not generating them.", file=sys.stderr)
        print("Consider using 'ollampy --image image.png' with a vision model like llava instead.", file=sys.stderr)
        exit(1)
        
    elif args.image_gen:
        # Note: Ollama doesn't actually support image generation
        print("Error: Image generation is not supported by Ollama.", file=sys.stderr)
        print("Ollama only supports vision models for analyzing images, not generating them.", file=sys.stderr)
        exit(1)
    else:
        # Use the chat endpoint for regular requests
        url = f"{args.endpoint}/api/chat"
        
        # Construct message content
        message_content = query
        images_list = []
        
        # Add image content if provided
        if args.images:
            for image_path in args.images:
                if not os.path.exists(image_path):
                    print(f"Error: Image file not found: {image_path}", file=sys.stderr)
                    exit(1)
                
                image_data = encode_image_to_base64(image_path)
                images_list.append(image_data)
        
        # Build the message
        message = {
            "role": "user",
            "content": message_content
        }
        
        # Add images if any
        if images_list:
            message["images"] = images_list
        
        data = {
            "model": args.model,
            "messages": [message],
            # Only stream if not in JSON mode or --no-streaming
            "stream": not (args.json or args.no_streaming)
        }

    # Print debug info if requested
    if args.debug:
        print(f"Debug: API URL: {url}", file=sys.stderr)
        print(f"Debug: Headers: {headers}", file=sys.stderr)
        print(f"Debug: Data: {json.dumps(data)}", file=sys.stderr)
        if args.image_gen:
            output_file = args.output if args.output else get_auto_numbered_filename()
            print(f"Debug: Image will be saved to: {output_file}", file=sys.stderr)
            print(f"Debug: Note - Image generation support depends on the selected model", file=sys.stderr)
        if args.images:
            print(f"Debug: Processing {len(args.images)} image(s): {', '.join(args.images)}", file=sys.stderr)
            print(f"Debug: Note - Vision support depends on the selected model", file=sys.stderr)
    
    # Handle dry-run mode
    if args.dry_run:
        print("=== DRY-RUN MODE ===", file=sys.stderr)
        print(f"API endpoint: {url}", file=sys.stderr)
        print("HTTP method: POST", file=sys.stderr)
        print(f"Headers: {headers}", file=sys.stderr)
        
        # Format request payload (truncate long content)
        request_data = data.copy()
        if "messages" in request_data and request_data["messages"]:
            for msg in request_data["messages"]:
                if isinstance(msg.get("content"), str) and len(msg["content"]) > 200:
                    msg["content"] = msg["content"][:200] + "... [truncated]"
                # Handle images in messages
                if "images" in msg and msg["images"]:
                    msg["images"] = [img[:100] + "... [truncated]" if len(img) > 100 else img for img in msg["images"]]
        elif "prompt" in request_data and len(request_data["prompt"]) > 200:
            request_data["prompt"] = request_data["prompt"][:200] + "... [truncated]"
        
        print(f"Request payload: {json.dumps(request_data, indent=2)}", file=sys.stderr)
        
        # Show full prompt if not too long
        if query and len(query) <= 1000:
            print(f"\nFull prompt:\n{query}", file=sys.stderr)
        elif query:
            print(f"\nFull prompt (truncated):\n{query[:1000]}... [truncated]", file=sys.stderr)
        
        # Mode-specific information
        if args.embedding:
            print("\nMode: Embedding generation", file=sys.stderr)
            print(f"Embedding model: {args.model}", file=sys.stderr)
            print("Endpoint: /api/embeddings", file=sys.stderr)
        elif args.images:
            print("\nMode: Vision analysis", file=sys.stderr)
            print(f"Vision model: {args.model}", file=sys.stderr)
            print(f"Images: {', '.join(args.images)}", file=sys.stderr)
            print("Note: Vision support depends on model capabilities", file=sys.stderr)
        else:
            print("\nMode: Text generation", file=sys.stderr)
            print(f"Model: {args.model}", file=sys.stderr)
            print(f"Streaming: {not (args.json or args.no_streaming)}", file=sys.stderr)
        
        print(f"\nOllama server: {args.endpoint}", file=sys.stderr)
        print("Note: This is a local Ollama instance", file=sys.stderr)
        
        print("=== END DRY-RUN ===", file=sys.stderr)
        exit(0)
    
    # Track timing if summary mode
    start_time = time.time() if args.summary else None
    first_token_time = None

    # Send request to Ollama API (streaming only if needed)
    response = requests.post(url, headers=headers, json=data, stream=data.get("stream", False))

    if response.status_code != 200:
        print(f"Error: {response.status_code}")
        print(response.text)
        exit(1)
    
    # Handle the response based on request type
    if args.embedding:
        # Process embeddings response (never streamed)
        response_data = response.json()
        if "embedding" in response_data:
            # Format the embedding as expected by semantic_search
            print(json.dumps({"embedding": response_data["embedding"]}))
            
            # Summary output for embeddings
            if args.summary:
                end_time = time.time()
                total_time = end_time - start_time
                
                # Add newline before summary
                print(file=sys.stderr)
                
                # For embeddings, estimate token count
                estimated_tokens = len(query) // 4
                
                # Calculate cost (free for local Ollama)
                cost_info = calculate_cost(args.model, estimated_tokens, 0)
                
                # Print summary table
                print_summary_table(
                    model=args.model,
                    input_tokens=estimated_tokens,
                    output_tokens=0,
                    total_time=total_time,
                    first_token_time=None,
                    cost_info=cost_info,
                    is_estimated=True,
                    use_color=not args.no_color
                )
        else:
            print(json.dumps(response_data))
    elif args.image_gen:
        # Process image generation response
        response_data = response.json()
        if "images" in response_data and len(response_data["images"]) > 0:
            # Get the base64 image data (Ollama returns images as base64)
            image_data = response_data["images"][0]
            
            # Determine output filename
            if args.output:
                output_filename = args.output
            else:
                output_filename = get_auto_numbered_filename()
            
            # Decode and save the image
            try:
                image_bytes = base64.b64decode(image_data)
                with open(output_filename, 'wb') as f:
                    f.write(image_bytes)
                print(f"Image saved to: {output_filename}")
                
                # Summary output for image generation
                if args.summary:
                    end_time = time.time()
                    total_time = end_time - start_time
                    
                    # Add newline before summary
                    print(file=sys.stderr)
                    
                    # Image generation typically doesn't provide token counts
                    # Estimate input tokens from prompt length
                    estimated_input_tokens = len(query) // 4
                    estimated_output_tokens = 0  # Images don't have text output tokens
                    
                    # Calculate cost for image generation (free for local Ollama)
                    cost_info = calculate_cost(
                        args.model,  # Use the actual model for pricing
                        estimated_input_tokens,
                        estimated_output_tokens
                    )
                    
                    # Print summary table
                    print_summary_table(
                        model=args.model,
                        input_tokens=estimated_input_tokens,
                        output_tokens=estimated_output_tokens,
                        total_time=total_time,
                        first_token_time=None,
                        cost_info=cost_info,
                        is_estimated=True,
                        use_color=not args.no_color
                    )
            except Exception as e:
                print(f"Error saving image: {e}", file=sys.stderr)
                exit(1)
        else:
            # Check if the model supports image generation
            if "error" in response_data:
                error_msg = response_data["error"]
                if "not found" in error_msg.lower() or "unsupported" in error_msg.lower():
                    print(f"Error: Model '{args.model}' does not support image generation.", file=sys.stderr)
                    print("Try using a model that supports image generation (e.g., dall-e, stable-diffusion variants)", file=sys.stderr)
                else:
                    print(f"Error: {error_msg}", file=sys.stderr)
            else:
                print(f"Error: No image data in response: {json.dumps(response_data)}", file=sys.stderr)
            exit(1)
    elif args.json or args.no_streaming:
        # Process standard JSON response (not streamed)
        response_data = response.json()
        if "message" in response_data and "content" in response_data["message"]:
            # Just return the content without any JSON formatting
            message_content = response_data["message"]["content"]
            
            # Enhance with emojis if requested
            if args.enhance_emojis and message_content:
                print("Enhancing response with emojis...", file=sys.stderr)
                message_content = enhance_with_emojis(message_content)
            
            print(message_content)
            
            # Summary output for non-streaming
            if args.summary:
                end_time = time.time()
                total_time = end_time - start_time
                
                # Add newline before summary
                print(file=sys.stderr)
                
                # Estimate tokens (Ollama doesn't always provide token counts)
                estimated_input_tokens = len(query) // 4
                estimated_output_tokens = len(message_content) // 4
                
                # Calculate cost (free for local Ollama)
                cost_info = calculate_cost(args.model, estimated_input_tokens, estimated_output_tokens)
                
                # Print summary table
                print_summary_table(
                    model=args.model,
                    input_tokens=estimated_input_tokens,
                    output_tokens=estimated_output_tokens,
                    total_time=total_time,
                    first_token_time=None,
                    cost_info=cost_info,
                    is_estimated=True,
                    use_color=not args.no_color
                )
            
            # Save chat transaction if not disabled
            if not args.no_preserve and not args.embedding:
                # Prepare metadata
                metadata = {
                    'stream_mode': False,
                    'images': args.images,
                    'input_tokens': estimated_input_tokens,
                    'output_tokens': estimated_output_tokens,
                    'total_tokens': estimated_input_tokens + estimated_output_tokens,
                    'cost_info': cost_info,
                    'total_time': total_time,
                    'endpoint': args.endpoint
                }
                
                save_chat_transaction("Ollama", args.model, query, message_content, metadata)
        else:
            # Just print the raw response as fallback
            if args.json:
                print(json.dumps(response_data))
            else:
                print(f"Unexpected response format: {json.dumps(response_data)}", file=sys.stderr)
    else:
        # Process the streaming response
        accumulated_content = ""
        
        for line in response.iter_lines():
            if line:
                try:
                    event = json.loads(line.decode('utf-8'))
                    # Extract content from the message
                    if "message" in event and "content" in event["message"]:
                        content = event["message"]["content"]
                        if not args.enhance_emojis:
                            print(content, end="", flush=True)
                        
                        # Track first token time
                        if args.summary and first_token_time is None and content:
                            first_token_time = time.time()
                        
                        accumulated_content += content
                    # Handle done message
                    if event.get("done", False):
                        break
                except json.JSONDecodeError:
                    continue
        
        # Enhance with emojis if requested (after streaming is complete)
        if args.enhance_emojis and accumulated_content:
            print("Enhancing response with emojis...", file=sys.stderr)
            enhanced_content = enhance_with_emojis(accumulated_content)
            print(enhanced_content)
        elif not args.enhance_emojis:
            # Add newline if we were streaming normally
            print()
        
        # Summary output for streaming
        if args.summary:
            end_time = time.time()
            total_time = end_time - start_time
            
            # Add newline before summary
            print(file=sys.stderr)
            
            # Estimate tokens (Ollama doesn't provide token counts in streaming)
            estimated_input_tokens = len(query) // 4
            estimated_output_tokens = len(accumulated_content) // 4
            
            # Calculate cost (free for local Ollama)
            cost_info = calculate_cost(args.model, estimated_input_tokens, estimated_output_tokens)
            
            # Calculate time to first token
            ttft = (first_token_time - start_time) if first_token_time else None
            
            # Print summary table
            print_summary_table(
                model=args.model,
                input_tokens=estimated_input_tokens,
                output_tokens=estimated_output_tokens,
                total_time=total_time,
                first_token_time=ttft,
                cost_info=cost_info,
                is_estimated=True,
                use_color=not args.no_color
            )
        
        # Save chat transaction if not disabled (for streaming)
        if not args.no_preserve and not args.embedding and accumulated_content:
            # Prepare metadata
            metadata = {
                'stream_mode': True,
                'images': args.images,
                'input_tokens': estimated_input_tokens,
                'output_tokens': estimated_output_tokens,
                'total_tokens': estimated_input_tokens + estimated_output_tokens,
                'cost_info': cost_info,
                'total_time': total_time,
                'first_token_time': ttft,
                'endpoint': args.endpoint
            }
            
            save_chat_transaction("Ollama", args.model, query, accumulated_content, metadata)

except (ImportError, Exception) as e:
    print(f"Error: {e}")
    exit(1)
