#!/usr/bin/python3

# dotfiles - Personal configuration files and scripts
# Copyright (C) 2025  Zach Podbielniak
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

"""
md_table_ml - Machine learning integration for markdown table data analysis

Usage:
  md_table_ml --classify --target column_name < data.md
  md_table_ml --regress --target price --features col1,col2,col3 < data.md
  md_table_ml --cluster --features col1,col2 --method kmeans < data.md
  md_table_ml --predict --model saved_model.pkl --features col1,col2 < new_data.md
  md_table_ml --feature-importance --target column_name < data.md
  md_table_ml --cross-validate --target column_name --cv 5 < data.md
  
Machine Learning Capabilities:
  - Classification (logistic regression, random forest, SVM, neural network)
  - Regression (linear, polynomial, random forest, gradient boosting)
  - Clustering (k-means, hierarchical, DBSCAN)
  - Feature engineering and selection
  - Model training, evaluation, and persistence
  - Cross-validation and hyperparameter tuning
  - Predictions on new data
  - Feature importance analysis
  - Automated preprocessing and scaling
  - Model comparison and selection
  - Ensemble methods
  - Time series analysis
  
Supported Algorithms:
  Classification:
    - logistic (Logistic Regression)
    - rf (Random Forest)
    - svm (Support Vector Machine)
    - nb (Naive Bayes)
    - knn (K-Nearest Neighbors)
    - mlp (Multi-layer Perceptron)
    - xgb (XGBoost)
  
  Regression:
    - linear (Linear Regression)
    - poly (Polynomial Regression)
    - ridge (Ridge Regression)
    - lasso (Lasso Regression)
    - rf (Random Forest)
    - gbr (Gradient Boosting)
    - svr (Support Vector Regression)
    - xgb (XGBoost Regressor)
  
  Clustering:
    - kmeans (K-Means)
    - hierarchical (Agglomerative Clustering)
    - dbscan (DBSCAN)
    - gaussian (Gaussian Mixture)

Options:
  --classify              Perform classification
  --regress               Perform regression
  --cluster               Perform clustering
  --predict               Make predictions with saved model
  --feature-importance    Analyze feature importance
  --cross-validate        Perform cross-validation
  --target COLUMN         Target column for supervised learning
  --features COLS         Feature columns (comma-separated)
  --algorithm ALG         Algorithm to use (see supported algorithms)
  --test-size FLOAT       Test set size (0.0-1.0, default: 0.2)
  --cv N                  Cross-validation folds (default: 5)
  --random-state N        Random state for reproducibility
  --scale                 Scale/normalize features
  --save-model FILE       Save trained model to file
  --load-model FILE       Load model from file
  --hyperparameter-tune   Perform hyperparameter tuning
  --ensemble              Use ensemble methods
  --output-predictions    Output predictions to stdout
  --output-format FORMAT  Output format: json, csv, md (default: md)
  --plot                  Generate plots (requires matplotlib)
  --plot-format FORMAT    Plot format: png, pdf, svg (default: png)
  --verbose               Verbose output
  --preprocessing STEPS   Preprocessing steps: impute,encode,scale

Examples:
  # Binary classification
  md_table_ml --classify --target survived --algorithm rf < titanic.md
  
  # Regression with feature selection
  md_table_ml --regress --target price --features sqft,bedrooms,bathrooms --algorithm gbr < housing.md
  
  # Clustering analysis
  md_table_ml --cluster --features x,y --algorithm kmeans --scale < points.md
  
  # Cross-validation
  md_table_ml --cross-validate --target class --cv 10 --algorithm svm < data.md
  
  # Hyperparameter tuning
  md_table_ml --classify --target class --hyperparameter-tune --save-model best_model.pkl < data.md
  
  # Make predictions
  md_table_ml --predict --load-model best_model.pkl --features col1,col2 < new_data.md
  
  # Feature importance analysis
  md_table_ml --feature-importance --target price --algorithm rf < data.md
  
  # Ensemble methods
  md_table_ml --classify --target class --ensemble --algorithm rf,svm,mlp < data.md
"""

from os import environ
from subprocess import run
from sys import argv, exit

ctr_id: str|None = ""
if ("CONTAINER_ID" in environ):
    ctr_id = environ.get("CONTAINER_ID")

# Check if distrobox check should be skipped
no_dbox_check = environ.get("NO_DBOX_CHECK", "").lower() in ("1", "true")

# if we are not in the 'dev' distrobox re-exec the script
# inside of the 'dev' distrobox
if not no_dbox_check and ("dev" != ctr_id):
    cmd: list[str] = [
        "distrobox",
        "enter",
        "dev",
        "--",
        *argv
    ]
    run(cmd)
    exit(0)

import argparse
import json
import sys
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
import warnings
warnings.filterwarnings('ignore')

# Try to import required ML libraries
try:
    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
    from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
    from sklearn.impute import SimpleImputer
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, VotingClassifier
    from sklearn.svm import SVC, SVR
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
    from sklearn.mixture import GaussianMixture
    from sklearn.decomposition import PCA
    from sklearn.feature_selection import SelectKBest, f_classif, f_regression, RFE
    from sklearn.pipeline import Pipeline
    from sklearn.compose import ColumnTransformer
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# Try to import XGBoost
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

# Try to import matplotlib for plotting
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False

class MarkdownTableML:
    def __init__(self):
        if not SKLEARN_AVAILABLE:
            raise ImportError("Required ML libraries not available. Install with: pip install scikit-learn pandas numpy")
        
        self.scaler = None
        self.label_encoders = {}
        self.feature_names = []
        self.target_name = None
        self.model = None
        self.preprocessor = None
        
    def _parse_markdown_table(self, content: str) -> pd.DataFrame:
        """Parse markdown table content into pandas DataFrame"""
        lines = content.strip().split('\n')
        
        # Find table lines (start and end with |)
        table_lines = []
        for line in lines:
            stripped = line.strip()
            if stripped.startswith('|') and stripped.endswith('|'):
                table_lines.append(stripped)
        
        if len(table_lines) < 2:
            raise ValueError("No valid markdown table found")
        
        # Parse header
        header_line = table_lines[0]
        headers = [col.strip() for col in header_line.split('|')[1:-1]]
        
        # Skip separator line (assumed to be line 1)
        data_lines = table_lines[2:] if len(table_lines) > 2 else []
        
        # Parse data rows
        rows = []
        for line in data_lines:
            row = [col.strip() for col in line.split('|')[1:-1]]
            # Ensure row has same number of columns as headers
            while len(row) < len(headers):
                row.append('')
            rows.append(row[:len(headers)])
        
        if not rows:
            raise ValueError("No data rows found in table")
        
        df = pd.DataFrame(rows, columns=headers)
        
        # Attempt to convert numeric columns
        for col in df.columns:
            # Replace empty strings with NaN
            df[col] = df[col].replace('', np.nan)
            # Try to convert to numeric
            numeric_series = pd.to_numeric(df[col], errors='coerce')
            # If more than 50% of values are numeric, convert the column
            if numeric_series.notna().sum() / len(df) > 0.5:
                df[col] = numeric_series
        
        return df
    
    def _preprocess_data(self, df: pd.DataFrame, target_column: str = None, 
                        feature_columns: List[str] = None, 
                        preprocessing_steps: List[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
        """Preprocess data for machine learning"""
        
        preprocessing_steps = preprocessing_steps or ['impute', 'encode', 'scale']
        
        # Identify feature and target columns
        if target_column and target_column not in df.columns:
            raise ValueError(f"Target column '{target_column}' not found")
        
        if feature_columns:
            missing_features = [col for col in feature_columns if col not in df.columns]
            if missing_features:
                raise ValueError(f"Feature columns not found: {missing_features}")
            X = df[feature_columns].copy()
        else:
            # Use all columns except target as features
            X = df.drop(columns=[target_column] if target_column else []).copy()
        
        y = df[target_column].copy() if target_column else None
        
        # Store feature names
        self.feature_names = list(X.columns)
        self.target_name = target_column
        
        # Preprocessing pipeline
        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = X.select_dtypes(include=['object']).columns.tolist()
        
        # Handle missing values
        if 'impute' in preprocessing_steps:
            if numeric_features:
                X[numeric_features] = X[numeric_features].fillna(X[numeric_features].mean())
            if categorical_features:
                X[categorical_features] = X[categorical_features].fillna(X[categorical_features].mode().iloc[0] if not X[categorical_features].mode().empty else 'Unknown')
        
        # Encode categorical variables
        if 'encode' in preprocessing_steps and categorical_features:
            for col in categorical_features:
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
                self.label_encoders[col] = le
        
        # Scale numeric features
        if 'scale' in preprocessing_steps and numeric_features:
            scaler = StandardScaler()
            X[numeric_features] = scaler.fit_transform(X[numeric_features])
            self.scaler = scaler
        
        return X, y
    
    def _get_classifier(self, algorithm: str, **kwargs):
        """Get classifier instance"""
        classifiers = {
            'logistic': LogisticRegression(random_state=kwargs.get('random_state', 42)),
            'rf': RandomForestClassifier(random_state=kwargs.get('random_state', 42)),
            'svm': SVC(random_state=kwargs.get('random_state', 42)),
            'nb': GaussianNB(),
            'knn': KNeighborsClassifier(),
            'mlp': MLPClassifier(random_state=kwargs.get('random_state', 42), max_iter=1000),
        }
        
        if XGBOOST_AVAILABLE:
            classifiers['xgb'] = xgb.XGBClassifier(random_state=kwargs.get('random_state', 42))
        
        if algorithm not in classifiers:
            available = list(classifiers.keys())
            raise ValueError(f"Unsupported classifier: {algorithm}. Available: {available}")
        
        return classifiers[algorithm]
    
    def _get_regressor(self, algorithm: str, **kwargs):
        """Get regressor instance"""
        regressors = {
            'linear': LinearRegression(),
            'ridge': Ridge(random_state=kwargs.get('random_state', 42)),
            'lasso': Lasso(random_state=kwargs.get('random_state', 42)),
            'rf': RandomForestRegressor(random_state=kwargs.get('random_state', 42)),
            'gbr': GradientBoostingRegressor(random_state=kwargs.get('random_state', 42)),
            'svr': SVR(),
        }
        
        if XGBOOST_AVAILABLE:
            regressors['xgb'] = xgb.XGBRegressor(random_state=kwargs.get('random_state', 42))
        
        if algorithm not in regressors:
            available = list(regressors.keys())
            raise ValueError(f"Unsupported regressor: {algorithm}. Available: {available}")
        
        return regressors[algorithm]
    
    def _get_clusterer(self, algorithm: str, **kwargs):
        """Get clusterer instance"""
        clusterers = {
            'kmeans': KMeans(random_state=kwargs.get('random_state', 42)),
            'hierarchical': AgglomerativeClustering(),
            'dbscan': DBSCAN(),
            'gaussian': GaussianMixture(random_state=kwargs.get('random_state', 42)),
        }
        
        if algorithm not in clusterers:
            available = list(clusterers.keys())
            raise ValueError(f"Unsupported clusterer: {algorithm}. Available: {available}")
        
        return clusterers[algorithm]
    
    def classify(self, df: pd.DataFrame, target_column: str, feature_columns: List[str] = None,
                algorithm: str = 'rf', test_size: float = 0.2, random_state: int = 42,
                hyperparameter_tune: bool = False, cross_validate: bool = False,
                cv_folds: int = 5) -> Dict[str, Any]:
        """Perform classification"""
        
        # Preprocess data
        X, y = self._preprocess_data(df, target_column, feature_columns, ['impute', 'encode', 'scale'])
        
        if y is None:
            raise ValueError("Target column required for classification")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        # Get classifier
        classifier = self._get_classifier(algorithm, random_state=random_state)
        
        # Hyperparameter tuning
        if hyperparameter_tune:
            param_grids = {
                'rf': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10, None]},
                'svm': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},
                'logistic': {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']},
                'mlp': {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'alpha': [0.001, 0.01, 0.1]}
            }
            
            if algorithm in param_grids:
                grid_search = GridSearchCV(classifier, param_grids[algorithm], cv=cv_folds, scoring='accuracy')
                grid_search.fit(X_train, y_train)
                classifier = grid_search.best_estimator_
        
        # Train model
        classifier.fit(X_train, y_train)
        self.model = classifier
        
        # Predictions
        y_pred = classifier.predict(X_test)
        
        # Evaluate
        accuracy = accuracy_score(y_test, y_pred)
        classification_rep = classification_report(y_test, y_pred, output_dict=True)
        
        results = {
            'algorithm': algorithm,
            'accuracy': accuracy,
            'classification_report': classification_rep,
            'feature_importance': None,
            'cross_validation_scores': None,
            'predictions': y_pred.tolist(),
            'test_labels': y_test.tolist()
        }
        
        # Feature importance
        if hasattr(classifier, 'feature_importances_'):
            importance_dict = dict(zip(self.feature_names, classifier.feature_importances_))
            results['feature_importance'] = importance_dict
        
        # Cross validation
        if cross_validate:
            cv_scores = cross_val_score(classifier, X, y, cv=cv_folds)
            results['cross_validation_scores'] = {
                'scores': cv_scores.tolist(),
                'mean': cv_scores.mean(),
                'std': cv_scores.std()
            }
        
        return results
    
    def regress(self, df: pd.DataFrame, target_column: str, feature_columns: List[str] = None,
               algorithm: str = 'rf', test_size: float = 0.2, random_state: int = 42,
               hyperparameter_tune: bool = False, cross_validate: bool = False,
               cv_folds: int = 5) -> Dict[str, Any]:
        """Perform regression"""
        
        # Preprocess data
        X, y = self._preprocess_data(df, target_column, feature_columns, ['impute', 'encode', 'scale'])
        
        if y is None:
            raise ValueError("Target column required for regression")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        # Get regressor
        regressor = self._get_regressor(algorithm, random_state=random_state)
        
        # Hyperparameter tuning
        if hyperparameter_tune:
            param_grids = {
                'rf': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10, None]},
                'gbr': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10]},
                'ridge': {'alpha': [0.1, 1, 10, 100]},
                'lasso': {'alpha': [0.1, 1, 10, 100]}
            }
            
            if algorithm in param_grids:
                grid_search = GridSearchCV(regressor, param_grids[algorithm], cv=cv_folds, scoring='neg_mean_squared_error')
                grid_search.fit(X_train, y_train)
                regressor = grid_search.best_estimator_
        
        # Train model
        regressor.fit(X_train, y_train)
        self.model = regressor
        
        # Predictions
        y_pred = regressor.predict(X_test)
        
        # Evaluate
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        results = {
            'algorithm': algorithm,
            'mse': mse,
            'mae': mae,
            'r2': r2,
            'rmse': np.sqrt(mse),
            'feature_importance': None,
            'cross_validation_scores': None,
            'predictions': y_pred.tolist(),
            'test_labels': y_test.tolist()
        }
        
        # Feature importance
        if hasattr(regressor, 'feature_importances_'):
            importance_dict = dict(zip(self.feature_names, regressor.feature_importances_))
            results['feature_importance'] = importance_dict
        
        # Cross validation
        if cross_validate:
            cv_scores = cross_val_score(regressor, X, y, cv=cv_folds, scoring='neg_mean_squared_error')
            results['cross_validation_scores'] = {
                'scores': (-cv_scores).tolist(),  # Convert back to positive MSE
                'mean': -cv_scores.mean(),
                'std': cv_scores.std()
            }
        
        return results
    
    def cluster(self, df: pd.DataFrame, feature_columns: List[str] = None,
               algorithm: str = 'kmeans', n_clusters: int = 3,
               random_state: int = 42) -> Dict[str, Any]:
        """Perform clustering"""
        
        # Preprocess data (no target column for clustering)
        X, _ = self._preprocess_data(df, None, feature_columns, ['impute', 'encode', 'scale'])
        
        # Get clusterer
        clusterer = self._get_clusterer(algorithm, random_state=random_state)
        
        # Set number of clusters for algorithms that need it
        if algorithm in ['kmeans', 'hierarchical', 'gaussian']:
            clusterer.set_params(n_clusters=n_clusters)
        
        # Fit model
        cluster_labels = clusterer.fit_predict(X)
        self.model = clusterer
        
        # Calculate cluster centers (if available)
        cluster_centers = None
        if hasattr(clusterer, 'cluster_centers_'):
            cluster_centers = clusterer.cluster_centers_.tolist()
        
        # Calculate silhouette score
        silhouette_score = None
        try:
            from sklearn.metrics import silhouette_score as ss
            if len(set(cluster_labels)) > 1:
                silhouette_score = ss(X, cluster_labels)
        except:
            pass
        
        results = {
            'algorithm': algorithm,
            'n_clusters': len(set(cluster_labels)),
            'cluster_labels': cluster_labels.tolist(),
            'cluster_centers': cluster_centers,
            'silhouette_score': silhouette_score,
            'cluster_sizes': {i: int(np.sum(cluster_labels == i)) for i in set(cluster_labels)}
        }
        
        return results
    
    def feature_importance_analysis(self, df: pd.DataFrame, target_column: str,
                                  feature_columns: List[str] = None,
                                  algorithm: str = 'rf') -> Dict[str, Any]:
        """Analyze feature importance"""
        
        # Preprocess data
        X, y = self._preprocess_data(df, target_column, feature_columns, ['impute', 'encode', 'scale'])
        
        if y is None:
            raise ValueError("Target column required for feature importance analysis")
        
        # Determine if it's classification or regression
        is_classification = y.dtype == 'object' or y.nunique() < 10
        
        if is_classification:
            model = self._get_classifier(algorithm)
        else:
            model = self._get_regressor(algorithm)
        
        # Fit model
        model.fit(X, y)
        
        # Get feature importance
        importance_scores = {}
        
        if hasattr(model, 'feature_importances_'):
            importance_scores = dict(zip(self.feature_names, model.feature_importances_))
        
        # Univariate feature selection
        if is_classification:
            selector = SelectKBest(score_func=f_classif, k='all')
        else:
            selector = SelectKBest(score_func=f_regression, k='all')
        
        selector.fit(X, y)
        univariate_scores = dict(zip(self.feature_names, selector.scores_))
        
        # Recursive feature elimination
        if hasattr(model, 'feature_importances_') or hasattr(model, 'coef_'):
            rfe = RFE(model, n_features_to_select=min(5, len(self.feature_names)))
            rfe.fit(X, y)
            rfe_rankings = dict(zip(self.feature_names, rfe.ranking_))
        else:
            rfe_rankings = {}
        
        results = {
            'feature_importance': importance_scores,
            'univariate_scores': univariate_scores,
            'rfe_rankings': rfe_rankings,
            'top_features': sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)[:10] if importance_scores else []
        }
        
        return results
    
    def predict(self, df: pd.DataFrame, model_path: str = None, 
               feature_columns: List[str] = None) -> List[Any]:
        """Make predictions with trained model"""
        
        if model_path:
            # Load model from file
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
                self.model = model_data['model']
                self.feature_names = model_data['feature_names']
                self.scaler = model_data.get('scaler')
                self.label_encoders = model_data.get('label_encoders', {})
        
        if self.model is None:
            raise ValueError("No model available. Train a model first or load one from file.")
        
        # Preprocess data
        X, _ = self._preprocess_data(df, None, feature_columns or self.feature_names, ['impute', 'encode', 'scale'])
        
        # Make predictions
        predictions = self.model.predict(X)
        
        return predictions.tolist()
    
    def save_model(self, filepath: str):
        """Save trained model to file"""
        if self.model is None:
            raise ValueError("No model to save. Train a model first.")
        
        model_data = {
            'model': self.model,
            'feature_names': self.feature_names,
            'target_name': self.target_name,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
    
    def format_results(self, results: Dict[str, Any], output_format: str = 'md') -> str:
        """Format results for output"""
        
        if output_format == 'json':
            return json.dumps(results, indent=2, default=str)
        
        elif output_format == 'csv':
            # Simple CSV output for key metrics
            lines = []
            for key, value in results.items():
                if isinstance(value, (int, float, str)):
                    lines.append(f"{key},{value}")
            return '\n'.join(lines)
        
        else:  # markdown
            lines = []
            lines.append("# Machine Learning Results\n")
            
            # Algorithm info
            if 'algorithm' in results:
                lines.append(f"**Algorithm**: {results['algorithm']}\n")
            
            # Classification results
            if 'accuracy' in results:
                lines.append(f"**Accuracy**: {results['accuracy']:.4f}\n")
                
                if 'classification_report' in results:
                    lines.append("## Classification Report\n")
                    lines.append("| Class | Precision | Recall | F1-Score | Support |")
                    lines.append("|-------|-----------|--------|----------|---------|")
                    
                    for class_name, metrics in results['classification_report'].items():
                        if class_name not in ['accuracy', 'macro avg', 'weighted avg']:
                            lines.append(f"| {class_name} | {metrics['precision']:.3f} | "
                                       f"{metrics['recall']:.3f} | {metrics['f1-score']:.3f} | "
                                       f"{metrics['support']} |")
                    lines.append("")
            
            # Regression results
            if 'mse' in results:
                lines.append(f"**Mean Squared Error**: {results['mse']:.4f}")
                lines.append(f"**Mean Absolute Error**: {results['mae']:.4f}")
                lines.append(f"**RÂ² Score**: {results['r2']:.4f}")
                lines.append(f"**RMSE**: {results['rmse']:.4f}\n")
            
            # Clustering results
            if 'cluster_labels' in results:
                lines.append(f"**Number of Clusters**: {results['n_clusters']}")
                if results.get('silhouette_score'):
                    lines.append(f"**Silhouette Score**: {results['silhouette_score']:.4f}")
                
                lines.append("\n### Cluster Sizes\n")
                for cluster_id, size in results['cluster_sizes'].items():
                    lines.append(f"- Cluster {cluster_id}: {size} points")
                lines.append("")
            
            # Feature importance
            if 'feature_importance' in results and results['feature_importance']:
                lines.append("## Feature Importance\n")
                sorted_features = sorted(results['feature_importance'].items(), 
                                       key=lambda x: x[1], reverse=True)
                lines.append("| Feature | Importance |")
                lines.append("|---------|------------|")
                for feature, importance in sorted_features:
                    lines.append(f"| {feature} | {importance:.4f} |")
                lines.append("")
            
            # Cross-validation results
            if 'cross_validation_scores' in results and results['cross_validation_scores']:
                cv_results = results['cross_validation_scores']
                lines.append("## Cross-Validation Results\n")
                lines.append(f"**Mean Score**: {cv_results['mean']:.4f}")
                lines.append(f"**Standard Deviation**: {cv_results['std']:.4f}")
                lines.append(f"**Individual Scores**: {cv_results['scores']}\n")
            
            return '\n'.join(lines)

def main():
    parser = argparse.ArgumentParser(
        description='Machine learning integration for markdown table data analysis',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Classification with Random Forest
    md_table_ml --classify --target species --algorithm rf < iris.md
    
    # Regression with feature selection
    md_table_ml --regress --target price --features sqft,bedrooms --algorithm gbr < housing.md
    
    # Clustering analysis
    md_table_ml --cluster --algorithm kmeans --features x,y < points.md
    
    # Cross-validation
    md_table_ml --cross-validate --target class --cv 10 < data.md
    
    # Hyperparameter tuning
    md_table_ml --classify --target class --hyperparameter-tune --save-model model.pkl < data.md
    
    # Make predictions
    md_table_ml --predict --load-model model.pkl < new_data.md
        """
    )
    
    # Main operations
    operation = parser.add_mutually_exclusive_group(required=True)
    operation.add_argument('--classify', action='store_true', help='Perform classification')
    operation.add_argument('--regress', action='store_true', help='Perform regression')
    operation.add_argument('--cluster', action='store_true', help='Perform clustering')
    operation.add_argument('--predict', action='store_true', help='Make predictions')
    operation.add_argument('--feature-importance', action='store_true', help='Analyze feature importance')
    operation.add_argument('--cross-validate', action='store_true', help='Perform cross-validation')
    
    # Data options
    parser.add_argument('--target', metavar='COLUMN', help='Target column for supervised learning')
    parser.add_argument('--features', metavar='COLS', help='Feature columns (comma-separated)')
    
    # Algorithm options
    parser.add_argument('--algorithm', metavar='ALG', default='rf', help='Algorithm to use')
    parser.add_argument('--ensemble', action='store_true', help='Use ensemble methods')
    
    # Training options
    parser.add_argument('--test-size', type=float, default=0.2, help='Test set size (default: 0.2)')
    parser.add_argument('--cv', type=int, default=5, help='Cross-validation folds (default: 5)')
    parser.add_argument('--random-state', type=int, default=42, help='Random state for reproducibility')
    parser.add_argument('--hyperparameter-tune', action='store_true', help='Perform hyperparameter tuning')
    
    # Preprocessing options
    parser.add_argument('--scale', action='store_true', help='Scale/normalize features')
    parser.add_argument('--preprocessing', metavar='STEPS', help='Preprocessing steps: impute,encode,scale')
    
    # Model persistence
    parser.add_argument('--save-model', metavar='FILE', help='Save trained model to file')
    parser.add_argument('--load-model', metavar='FILE', help='Load model from file')
    
    # Clustering options
    parser.add_argument('--n-clusters', type=int, default=3, help='Number of clusters (default: 3)')
    
    # Output options
    parser.add_argument('--output-predictions', action='store_true', help='Output predictions to stdout')
    parser.add_argument('--output-format', choices=['json', 'csv', 'md'], default='md', help='Output format')
    parser.add_argument('--verbose', action='store_true', help='Verbose output')
    
    # Plotting options
    parser.add_argument('--plot', action='store_true', help='Generate plots')
    parser.add_argument('--plot-format', choices=['png', 'pdf', 'svg'], default='png', help='Plot format')
    
    args = parser.parse_args()
    
    # Check dependencies
    if not SKLEARN_AVAILABLE:
        print("Error: Required dependencies not installed.", file=sys.stderr)
        print("Install with: pip install scikit-learn pandas numpy", file=sys.stderr)
        print("Or in distrobox: distrobox enter dev -- pip install scikit-learn pandas numpy", file=sys.stderr)
        sys.exit(1)
    
    # Initialize ML processor
    ml_processor = MarkdownTableML()
    
    try:
        # Read input data
        content = sys.stdin.read()
        if not content.strip():
            print("Error: No input data provided", file=sys.stderr)
            sys.exit(1)
        
        # Parse markdown table
        df = ml_processor._parse_markdown_table(content)
        
        # Parse feature columns
        feature_columns = None
        if args.features:
            feature_columns = [col.strip() for col in args.features.split(',')]
        
        # Parse preprocessing steps
        preprocessing_steps = ['impute', 'encode']
        if args.scale:
            preprocessing_steps.append('scale')
        if args.preprocessing:
            preprocessing_steps = [step.strip() for step in args.preprocessing.split(',')]
        
        # Perform requested operation
        if args.classify:
            if not args.target:
                print("Error: --target required for classification", file=sys.stderr)
                sys.exit(1)
            
            results = ml_processor.classify(
                df=df,
                target_column=args.target,
                feature_columns=feature_columns,
                algorithm=args.algorithm,
                test_size=args.test_size,
                random_state=args.random_state,
                hyperparameter_tune=args.hyperparameter_tune,
                cross_validate=args.cross_validate,
                cv_folds=args.cv
            )
        
        elif args.regress:
            if not args.target:
                print("Error: --target required for regression", file=sys.stderr)
                sys.exit(1)
            
            results = ml_processor.regress(
                df=df,
                target_column=args.target,
                feature_columns=feature_columns,
                algorithm=args.algorithm,
                test_size=args.test_size,
                random_state=args.random_state,
                hyperparameter_tune=args.hyperparameter_tune,
                cross_validate=args.cross_validate,
                cv_folds=args.cv
            )
        
        elif args.cluster:
            results = ml_processor.cluster(
                df=df,
                feature_columns=feature_columns,
                algorithm=args.algorithm,
                n_clusters=args.n_clusters,
                random_state=args.random_state
            )
        
        elif args.predict:
            if not args.load_model:
                print("Error: --load-model required for prediction", file=sys.stderr)
                sys.exit(1)
            
            predictions = ml_processor.predict(
                df=df,
                model_path=args.load_model,
                feature_columns=feature_columns
            )
            
            if args.output_predictions:
                for pred in predictions:
                    print(pred)
            else:
                results = {'predictions': predictions}
        
        elif args.feature_importance:
            if not args.target:
                print("Error: --target required for feature importance analysis", file=sys.stderr)
                sys.exit(1)
            
            results = ml_processor.feature_importance_analysis(
                df=df,
                target_column=args.target,
                feature_columns=feature_columns,
                algorithm=args.algorithm
            )
        
        elif args.cross_validate:
            if not args.target:
                print("Error: --target required for cross-validation", file=sys.stderr)
                sys.exit(1)
            
            # Determine if classification or regression
            target_series = df[args.target]
            is_classification = target_series.dtype == 'object' or target_series.nunique() < 10
            
            if is_classification:
                results = ml_processor.classify(
                    df=df,
                    target_column=args.target,
                    feature_columns=feature_columns,
                    algorithm=args.algorithm,
                    cross_validate=True,
                    cv_folds=args.cv
                )
            else:
                results = ml_processor.regress(
                    df=df,
                    target_column=args.target,
                    feature_columns=feature_columns,
                    algorithm=args.algorithm,
                    cross_validate=True,
                    cv_folds=args.cv
                )
        
        # Save model if requested
        if args.save_model and hasattr(ml_processor, 'model') and ml_processor.model is not None:
            ml_processor.save_model(args.save_model)
            if args.verbose:
                print(f"Model saved to: {args.save_model}", file=sys.stderr)
        
        # Output results
        if 'results' in locals():
            formatted_output = ml_processor.format_results(results, args.output_format)
            print(formatted_output)
        
    except KeyboardInterrupt:
        print("\nOperation cancelled", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()