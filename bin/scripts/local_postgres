#!/bin/bash
set -euo pipefail

# local_postgres - Universal PostgreSQL management script for local dotfiles
# First implementation: Transcription storage
# Designed to be extended with additional schemas and functionality

# Sync configuration - add databases and hosts to sync here
SYNC_DATABASES=(
    "transcriptions"
    "ai_chats"
    "notes"
    "postgres"  # For pgvector/second_brain_vectors table
)

# Tailscale hostnames/IPs to sync with
SYNC_HOSTS=(
    # Add your other devices here, e.g.:
    # "laptop.tailnet-name.ts.net"
    # "desktop.tailnet-name.ts.net"
    # "100.64.0.2"
    "100.72.0.41"
    "100.72.0.43"
    "100.72.0.45"
)

# Configuration file path
CONFIG_FILE="${HOME}/.config/transcription_db/config.yaml"

# Load configuration from YAML if available
if [[ -f "$CONFIG_FILE" ]] && command -v yq &> /dev/null; then
    CONFIG_DB_HOST=$(yq eval '.database.host // "127.0.0.1"' "$CONFIG_FILE")
    CONFIG_DB_PORT=$(yq eval '.database.port // 5432' "$CONFIG_FILE")
    CONFIG_DB_NAME=$(yq eval '.database.name // "transcriptions"' "$CONFIG_FILE")
    CONFIG_DB_USER=$(yq eval '.database.user // "postgres"' "$CONFIG_FILE")
    CONFIG_DB_PASSWORD=$(yq eval '.database.password // ""' "$CONFIG_FILE")
else
    CONFIG_DB_HOST="127.0.0.1"
    CONFIG_DB_PORT="5432"
    CONFIG_DB_NAME="transcriptions"
    CONFIG_DB_USER="postgres"
    CONFIG_DB_PASSWORD=""
fi

# Default configuration (environment variables override config file)
DEFAULT_DB_HOST="${LOCAL_DB_HOST:-${TRANSCRIPTION_DB_HOST:-$CONFIG_DB_HOST}}"
DEFAULT_DB_PORT="${LOCAL_DB_PORT:-${TRANSCRIPTION_DB_PORT:-$CONFIG_DB_PORT}}"
DEFAULT_DB_NAME="${LOCAL_DB_NAME:-${TRANSCRIPTION_DB_NAME:-$CONFIG_DB_NAME}}"
DEFAULT_DB_USER="${LOCAL_DB_USER:-${TRANSCRIPTION_DB_USER:-$CONFIG_DB_USER}}"
DEFAULT_DB_PASSWORD="${LOCAL_DB_PASSWORD:-${TRANSCRIPTION_DB_PASSWORD:-$CONFIG_DB_PASSWORD}}"

# Script variables
SCRIPT_NAME="$(basename "$0")"
ACTION=""
VERBOSE=false
DEBUG=false
TARGET_DATABASE=""
BACKUP_ALL=false

# Database connection string
get_connection_string() {
    local host="${1:-$DEFAULT_DB_HOST}"
    local port="${2:-$DEFAULT_DB_PORT}"
    local dbname="${3:-$DEFAULT_DB_NAME}"
    local user="${4:-$DEFAULT_DB_USER}"
    local password="${5:-$DEFAULT_DB_PASSWORD}"
    
    local conn="postgresql://${user}"
    [[ -n "$password" ]] && conn="${conn}:${password}"
    conn="${conn}@${host}:${port}/${dbname}"
    echo "$conn"
}

# Execute SQL with error handling
execute_sql() {
    local sql="$1"
    local db="${2:-$DEFAULT_DB_NAME}"
    
    if [[ "$VERBOSE" == true ]]; then
        echo "Executing SQL on database '$db':"
        echo "$sql"
    fi
    
    psql -d "$(get_connection_string "" "" "$db")" -c "$sql" 2>&1 || {
        echo "Error executing SQL: $?" >&2
        return 1
    }
}

# Check if database exists
database_exists() {
    local dbname="${1:-$DEFAULT_DB_NAME}"
    psql -d "$(get_connection_string "" "" "postgres")" -tAc "SELECT 1 FROM pg_database WHERE datname='$dbname'" 2>/dev/null | grep -q 1
}

# Create database if it doesn't exist
create_database() {
    local dbname="${DEFAULT_DB_NAME}"
    
    if database_exists "$dbname"; then
        echo "Database '$dbname' already exists"
        return 0
    fi
    
    echo "Creating database '$dbname'..."
    execute_sql "CREATE DATABASE $dbname" "postgres" || return 1
    echo "Database created successfully"
}

# Initialize transcriptions schema
init_transcriptions_schema() {
    echo "Initializing transcriptions schema..."
    
    # Create main transcriptions table
    execute_sql "
    CREATE TABLE IF NOT EXISTS transcriptions (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        filename VARCHAR(512) NOT NULL,
        file_path TEXT,
        file_hash VARCHAR(64) NOT NULL,
        transcribed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        duration_seconds INTEGER,
        model_used VARCHAR(100),
        language VARCHAR(10),
        metadata JSONB DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(file_hash)
    );" || return 1
    
    # Create chunks table
    execute_sql "
    CREATE TABLE IF NOT EXISTS transcription_chunks (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        transcription_id UUID NOT NULL REFERENCES transcriptions(id) ON DELETE CASCADE,
        chunk_number INTEGER NOT NULL,
        content TEXT NOT NULL,
        content_vector tsvector GENERATED ALWAYS AS (to_tsvector('english', content)) STORED,
        character_count INTEGER NOT NULL,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(transcription_id, chunk_number)
    );" || return 1
    
    # Create indexes
    echo "Creating indexes..."
    execute_sql "CREATE INDEX IF NOT EXISTS idx_transcription_chunks_content_vector ON transcription_chunks USING GIN(content_vector);" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_transcription_chunks_transcription_id ON transcription_chunks(transcription_id);" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_transcriptions_file_hash ON transcriptions(file_hash);" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_transcriptions_metadata ON transcriptions USING GIN(metadata);" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_transcriptions_type ON transcriptions((metadata->>'type'));" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_transcriptions_tags ON transcriptions USING GIN((metadata->'tags'));" || return 1
    
    # Create update trigger function
    execute_sql "
    CREATE OR REPLACE FUNCTION update_updated_at_column()
    RETURNS TRIGGER AS \$\$
    BEGIN
        NEW.updated_at = CURRENT_TIMESTAMP;
        RETURN NEW;
    END;
    \$\$ language 'plpgsql';" || return 1
    
    # Create trigger
    execute_sql "
    DO \$\$
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_trigger WHERE tgname = 'update_transcriptions_updated_at') THEN
            CREATE TRIGGER update_transcriptions_updated_at 
            BEFORE UPDATE ON transcriptions
            FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
        END IF;
    END
    \$\$;" || return 1
    
    echo "Transcriptions schema initialized successfully"
}

# Initialize second brain vector schema
init_second_brain_schema() {
    echo "Initializing second brain vector schema..."
    
    # Enable pgvector extension if not already enabled
    execute_sql "CREATE EXTENSION IF NOT EXISTS vector;" "postgres" || {
        echo "Error: Failed to create vector extension" >&2
        return 1
    }
    
    # Check if table exists
    local table_exists=$(psql -d "$(get_connection_string "" "" "postgres")" -tAc "
    SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'second_brain_vectors')" 2>/dev/null || echo "f")
    
    if [[ "$table_exists" == "t" ]]; then
        echo "Table 'second_brain_vectors' already exists"
        
        # Check if it has the expected structure
        local has_provider=$(psql -d "$(get_connection_string "" "" "postgres")" -tAc "
        SELECT COUNT(*) FROM information_schema.columns 
        WHERE table_name = 'second_brain_vectors' AND column_name = 'provider'" 2>/dev/null || echo "0")
        
        if [[ "$has_provider" -eq 0 ]]; then
            echo "Upgrading table to include provider and model columns..."
            execute_sql "ALTER TABLE second_brain_vectors ADD COLUMN IF NOT EXISTS provider TEXT;" "postgres" || return 1
            execute_sql "ALTER TABLE second_brain_vectors ADD COLUMN IF NOT EXISTS model TEXT;" "postgres" || return 1
        fi
    else
        # Create table matching semantic_search expectations (768 dimensions for default ollampy provider)
        execute_sql "
        CREATE TABLE second_brain_vectors (
            id TEXT PRIMARY KEY,
            embedding vector(768),
            path TEXT NOT NULL,
            content_snippet TEXT,
            provider TEXT NOT NULL,
            model TEXT NOT NULL,
            metadata JSONB
        );" "postgres" || {
            echo "Error: Failed to create second_brain_vectors table" >&2
            return 1
        }
        echo "Created table 'second_brain_vectors' for vector embeddings with dimension 768"
    fi
    
    # Create indexes if they don't exist
    echo "Creating indexes for second_brain_vectors..."
    execute_sql "CREATE INDEX IF NOT EXISTS idx_second_brain_vectors_path ON second_brain_vectors(path);" "postgres" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_second_brain_vectors_provider ON second_brain_vectors(provider);" "postgres" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_second_brain_vectors_model ON second_brain_vectors(model);" "postgres" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_second_brain_vectors_metadata ON second_brain_vectors USING GIN(metadata);" "postgres" || return 1
    
    echo "Second brain vector schema initialized successfully"
}

# Initialize ai_chats schema
init_ai_chats_schema() {
    echo "Initializing ai_chats schema..."
    
    # Create ai_chats database if it doesn't exist
    if ! database_exists "ai_chats"; then
        echo "Creating ai_chats database..."
        execute_sql "CREATE DATABASE ai_chats" "postgres" || return 1
    fi
    
    # Create update trigger function in ai_chats database
    execute_sql "
    CREATE OR REPLACE FUNCTION update_updated_at_column()
    RETURNS TRIGGER AS \$\$
    BEGIN
        NEW.updated_at = CURRENT_TIMESTAMP;
        RETURN NEW;
    END;
    \$\$ language 'plpgsql';" "ai_chats" || return 1
    
    # Create ai_chat_contexts table first
    execute_sql "
    CREATE TABLE IF NOT EXISTS ai_chat_contexts (
        context_uuid UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        name VARCHAR(255) DEFAULT '<untitled>',
        summary TEXT,
        summary_provider VARCHAR(50),
        summary_model VARCHAR(200),
        summary_last_updated TIMESTAMP WITH TIME ZONE,
        tags TEXT[],
        ingested BOOLEAN DEFAULT FALSE,
        ingested_path VARCHAR(512),
        ingested_at TIMESTAMP WITH TIME ZONE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
    );" "ai_chats" || return 1
    
    # Create main ai_chats table with context_uuid
    execute_sql "
    CREATE TABLE IF NOT EXISTS ai_chats (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        context_uuid UUID NOT NULL,
        prompt TEXT NOT NULL,
        response TEXT NOT NULL,
        provider VARCHAR(50) NOT NULL,
        model VARCHAR(200) NOT NULL,
        request_timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        response_timestamp TIMESTAMP WITH TIME ZONE,
        duration_ms INTEGER,
        tokens_input INTEGER,
        tokens_output INTEGER,
        cost_input_usd DECIMAL(10,6),
        cost_output_usd DECIMAL(10,6),
        cost_total_usd DECIMAL(10,6),
        prompt_hash VARCHAR(64),
        response_hash VARCHAR(64),
        tags TEXT[],
        metadata JSONB DEFAULT '{}',
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        CONSTRAINT fk_ai_chats_context FOREIGN KEY (context_uuid) REFERENCES ai_chat_contexts(context_uuid) ON DELETE CASCADE
    );" "ai_chats" || return 1
    
    # Create indexes for ai_chat_contexts
    echo "Creating ai_chat_contexts indexes..."
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chat_contexts_name ON ai_chat_contexts(name);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chat_contexts_created_at ON ai_chat_contexts(created_at DESC);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chat_contexts_summary_updated ON ai_chat_contexts(summary_last_updated DESC);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chat_contexts_tags ON ai_chat_contexts USING GIN(tags);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chat_contexts_summary_text ON ai_chat_contexts USING GIN(to_tsvector('english', COALESCE(summary, '')));" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chat_contexts_ingested ON ai_chat_contexts(ingested);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chat_contexts_ingested_at ON ai_chat_contexts(ingested_at DESC);" "ai_chats" || return 1
    
    # Create indexes for ai_chats
    echo "Creating ai_chats indexes..."
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_context_uuid ON ai_chats(context_uuid);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_request_timestamp ON ai_chats(request_timestamp DESC);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_provider ON ai_chats(provider);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_model ON ai_chats(model);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_prompt_hash ON ai_chats(prompt_hash);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_metadata ON ai_chats USING GIN(metadata);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_cost ON ai_chats(cost_total_usd);" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_tags ON ai_chats USING GIN(tags);" "ai_chats" || return 1
    
    # Create text search index for prompts and responses
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_prompt_text ON ai_chats USING GIN(to_tsvector('english', prompt));" "ai_chats" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_ai_chats_response_text ON ai_chats USING GIN(to_tsvector('english', response));" "ai_chats" || return 1
    
    # Create triggers for updated_at on both tables
    execute_sql "
    DO \$\$
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_trigger WHERE tgname = 'update_ai_chat_contexts_updated_at') THEN
            CREATE TRIGGER update_ai_chat_contexts_updated_at 
            BEFORE UPDATE ON ai_chat_contexts
            FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
        END IF;
    END
    \$\$;" "ai_chats" || return 1
    
    execute_sql "
    DO \$\$
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_trigger WHERE tgname = 'update_ai_chats_updated_at') THEN
            CREATE TRIGGER update_ai_chats_updated_at 
            BEFORE UPDATE ON ai_chats
            FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
        END IF;
    END
    \$\$;" "ai_chats" || return 1
    
    echo "AI chats schema initialized successfully"
}

# Initialize notes schema
init_notes_schema() {
    echo "Initializing notes schema..."
    
    # Create notes database if it doesn't exist
    if ! database_exists "notes"; then
        echo "Creating notes database..."
        execute_sql "CREATE DATABASE notes" "postgres" || return 1
    fi
    
    # Create update trigger function in notes database
    execute_sql "
    CREATE OR REPLACE FUNCTION update_updated_at_column()
    RETURNS TRIGGER AS \$\$
    BEGIN
        NEW.updated_at = CURRENT_TIMESTAMP;
        RETURN NEW;
    END;
    \$\$ language 'plpgsql';" "notes" || return 1
    
    # Create main notes table
    execute_sql "
    CREATE TABLE IF NOT EXISTS notes (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        path TEXT NOT NULL UNIQUE,
        para VARCHAR(20) NOT NULL,
        category VARCHAR(100),
        content TEXT NOT NULL,
        content_vector tsvector GENERATED ALWAYS AS (to_tsvector('english', content)) STORED,
        file_size BIGINT,
        last_modified TIMESTAMP WITH TIME ZONE,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        accessed_at TIMESTAMP WITH TIME ZONE,
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        metadata JSONB DEFAULT '{}',
        tags TEXT[],
        CONSTRAINT valid_para CHECK (para IN ('inbox', 'projects', 'areas', 'resources', 'archives'))
    );" "notes" || return 1
    
    # Create indexes
    echo "Creating indexes..."
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_path ON notes(path);" "notes" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_para ON notes(para);" "notes" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_category ON notes(category);" "notes" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_content_vector ON notes USING GIN(content_vector);" "notes" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_metadata ON notes USING GIN(metadata);" "notes" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_tags ON notes USING GIN(tags);" "notes" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_last_modified ON notes(last_modified DESC);" "notes" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_created_at ON notes(created_at DESC);" "notes" || return 1
    execute_sql "CREATE INDEX IF NOT EXISTS idx_notes_accessed_at ON notes(accessed_at DESC);" "notes" || return 1
    
    # Create trigger
    execute_sql "
    DO \$\$
    BEGIN
        IF NOT EXISTS (SELECT 1 FROM pg_trigger WHERE tgname = 'update_notes_updated_at') THEN
            CREATE TRIGGER update_notes_updated_at 
            BEFORE UPDATE ON notes
            FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
        END IF;
    END
    \$\$;" "notes" || return 1
    
    echo "Notes schema initialized successfully"
}

# Initialize all schemas
init_all_schemas() {
    create_database || return 1
    init_transcriptions_schema || return 1
    init_ai_chats_schema || return 1
    init_notes_schema || return 1
    init_second_brain_schema || return 1
    echo "All schemas initialized successfully"
}

# Setup all schemas (alias for backward compatibility)
setup_all() {
    init_all_schemas
}

# Maintenance operations
perform_maintenance() {
    local db="${1:-$DEFAULT_DB_NAME}"
    echo "Performing database maintenance on '$db'..."
    
    case "$db" in
        "transcriptions")
            # Vacuum and analyze transcription tables
            execute_sql "VACUUM ANALYZE transcriptions;" "$db" || echo "Warning: Failed to vacuum transcriptions table"
            execute_sql "VACUUM ANALYZE transcription_chunks;" "$db" || echo "Warning: Failed to vacuum transcription_chunks table"
            
            # Reindex transcription tables
            execute_sql "REINDEX TABLE transcriptions;" "$db" || echo "Warning: Failed to reindex transcriptions table"
            execute_sql "REINDEX TABLE transcription_chunks;" "$db" || echo "Warning: Failed to reindex transcription_chunks table"
            
            # Clean up orphaned chunks
            execute_sql "DELETE FROM transcription_chunks WHERE transcription_id NOT IN (SELECT id FROM transcriptions);" "$db" || echo "Warning: Failed to clean orphaned chunks"
            ;;
        "ai_chats")
            # Vacuum and analyze ai_chats tables
            execute_sql "VACUUM ANALYZE ai_chat_contexts;" "$db" || echo "Warning: Failed to vacuum ai_chat_contexts table"
            execute_sql "VACUUM ANALYZE ai_chats;" "$db" || echo "Warning: Failed to vacuum ai_chats table"
            
            # Reindex ai_chats tables
            execute_sql "REINDEX TABLE ai_chat_contexts;" "$db" || echo "Warning: Failed to reindex ai_chat_contexts table"
            execute_sql "REINDEX TABLE ai_chats;" "$db" || echo "Warning: Failed to reindex ai_chats table"
            
            # Clean up orphaned chats (shouldn't happen due to foreign key, but just in case)
            execute_sql "DELETE FROM ai_chats WHERE context_uuid NOT IN (SELECT context_uuid FROM ai_chat_contexts);" "$db" || echo "Warning: Failed to clean orphaned chats"
            ;;
        "notes")
            # Vacuum and analyze notes table
            execute_sql "VACUUM ANALYZE notes;" "$db" || echo "Warning: Failed to vacuum notes table"
            
            # Reindex notes table
            execute_sql "REINDEX TABLE notes;" "$db" || echo "Warning: Failed to reindex notes table"
            ;;
        "postgres")
            # Vacuum and analyze vector table if it exists
            execute_sql "VACUUM ANALYZE second_brain_vectors;" "$db" || echo "Warning: Failed to vacuum second_brain_vectors table (may not exist)"
            execute_sql "REINDEX TABLE second_brain_vectors;" "$db" || echo "Warning: Failed to reindex second_brain_vectors table (may not exist)"
            ;;
        *)
            # Generic maintenance for unknown databases
            echo "Performing generic maintenance on database '$db'..."
            execute_sql "VACUUM ANALYZE;" "$db" || echo "Warning: Failed to vacuum database"
            execute_sql "REINDEX DATABASE $db;" "postgres" || echo "Warning: Failed to reindex database"
            ;;
    esac
    
    echo "Maintenance completed on '$db'"
}

# Database statistics
show_stats() {
    local db="${1:-$DEFAULT_DB_NAME}"
    echo "Database Statistics for '$db':"
    echo "==================="
    
    # Overall database size
    execute_sql "
    SELECT pg_database.datname as database_name,
           pg_size_pretty(pg_database_size(pg_database.datname)) as size
    FROM pg_database
    WHERE datname = '$db';" "postgres" || return 1
    
    echo -e "\nTable Sizes:"
    execute_sql "
    SELECT schemaname || '.' || tablename AS table,
           pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
    FROM pg_tables
    WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;" "$db" || return 1
    
    case "$db" in
        "transcriptions")
            echo -e "\nTranscription Statistics:"
            execute_sql "
            SELECT 
                COUNT(DISTINCT t.id) as total_transcriptions,
                COUNT(tc.id) as total_chunks,
                COALESCE(SUM(tc.character_count), 0) as total_characters
            FROM transcriptions t
            LEFT JOIN transcription_chunks tc ON t.id = tc.transcription_id;" "$db" || return 1
            ;;
        "ai_chats")
            echo -e "\nAI Chat Statistics:"
            execute_sql "
            SELECT 
                COUNT(*) as total_chats,
                COUNT(DISTINCT context_uuid) as total_contexts,
                COUNT(DISTINCT provider) as unique_providers,
                COUNT(DISTINCT model) as unique_models,
                COALESCE(SUM(tokens_input), 0) as total_input_tokens,
                COALESCE(SUM(tokens_output), 0) as total_output_tokens,
                COALESCE(SUM(cost_total_usd), 0) as total_cost_usd
            FROM ai_chats;" "$db" || return 1
            
            echo -e "\nContext Statistics:"
            execute_sql "
            SELECT 
                COUNT(*) as total_contexts,
                COUNT(CASE WHEN name != '<untitled>' THEN 1 END) as named_contexts,
                COUNT(CASE WHEN name = '<untitled>' THEN 1 END) as untitled_contexts
            FROM ai_chat_contexts;" "$db" || return 1
            
            echo -e "\nTop Providers:"
            execute_sql "
            SELECT provider, COUNT(*) as chat_count 
            FROM ai_chats 
            GROUP BY provider 
            ORDER BY chat_count DESC 
            LIMIT 5;" "$db" || return 1
            
            echo -e "\nTop Models:"
            execute_sql "
            SELECT model, COUNT(*) as chat_count 
            FROM ai_chats 
            GROUP BY model 
            ORDER BY chat_count DESC 
            LIMIT 5;" "$db" || return 1
            
            echo -e "\nLargest Contexts (by chat count):"
            execute_sql "
            SELECT 
                c.context_uuid,
                c.name,
                COUNT(a.id) as chat_count,
                MIN(a.request_timestamp) as first_chat,
                MAX(a.request_timestamp) as last_chat
            FROM ai_chat_contexts c
            LEFT JOIN ai_chats a ON c.context_uuid = a.context_uuid
            GROUP BY c.context_uuid, c.name
            ORDER BY chat_count DESC
            LIMIT 10;" "$db" || return 1
            ;;
        "notes")
            echo -e "\nNotes Statistics:"
            execute_sql "
            SELECT 
                COUNT(*) as total_notes,
                COUNT(DISTINCT para) as para_categories,
                COUNT(DISTINCT category) as unique_categories,
                COUNT(CASE WHEN para = 'inbox' THEN 1 END) as inbox_notes,
                COUNT(CASE WHEN para = 'projects' THEN 1 END) as project_notes,
                COUNT(CASE WHEN para = 'areas' THEN 1 END) as area_notes,
                COUNT(CASE WHEN para = 'resources' THEN 1 END) as resource_notes,
                COUNT(CASE WHEN para = 'archives' THEN 1 END) as archive_notes,
                COALESCE(SUM(file_size), 0) as total_size_bytes,
                pg_size_pretty(COALESCE(SUM(file_size), 0)) as total_size
            FROM notes;" "$db" || return 1
            
            echo -e "\nNotes by Category:"
            execute_sql "
            SELECT para, category, COUNT(*) as note_count 
            FROM notes 
            WHERE category IS NOT NULL
            GROUP BY para, category 
            ORDER BY para, note_count DESC 
            LIMIT 15;" "$db" || return 1
            
            echo -e "\nRecently Modified Notes:"
            execute_sql "
            SELECT 
                path,
                para,
                category,
                last_modified
            FROM notes
            WHERE last_modified IS NOT NULL
            ORDER BY last_modified DESC
            LIMIT 10;" "$db" || return 1
            ;;
        "postgres")
            echo -e "\nVector Database Statistics:"
            execute_sql "
            SELECT 
                COUNT(*) as total_vectors,
                COUNT(DISTINCT provider) as unique_providers,
                COUNT(DISTINCT model) as unique_models
            FROM second_brain_vectors;" "$db" 2>/dev/null || echo "No second_brain_vectors table found"
            ;;
        *)
            echo -e "\nGeneric table count:"
            execute_sql "
            SELECT schemaname, tablename, n_tup_ins as inserts, n_tup_upd as updates, n_tup_del as deletes
            FROM pg_stat_user_tables 
            ORDER BY n_tup_ins DESC;" "$db" || return 1
            ;;
    esac
}

# Show statistics for all databases
show_all_stats() {
    echo "PostgreSQL Database Statistics - All Databases"
    echo "=============================================="
    
    # Check which databases exist
    local available_dbs=()
    
    # Check for transcriptions database
    if database_exists "transcriptions"; then
        available_dbs+=("transcriptions")
    fi
    
    # Check for ai_chats database
    if database_exists "ai_chats"; then
        available_dbs+=("ai_chats")
    fi
    
    # Check for notes database
    if database_exists "notes"; then
        available_dbs+=("notes")
    fi
    
    # Check for postgres database (always exists)
    available_dbs+=("postgres")
    
    echo "Found databases: ${available_dbs[*]}"
    echo ""
    
    # Show stats for each available database
    for db in "${available_dbs[@]}"; do
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        show_stats "$db"
        echo ""
    done
    
    # Overall summary
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    echo "Overall Summary:"
    echo "================"
    
    # Total size across all databases
    execute_sql "
    SELECT 
        COUNT(*) as total_databases,
        pg_size_pretty(SUM(pg_database_size(datname))) as total_size
    FROM pg_database 
    WHERE datname IN ('transcriptions', 'ai_chats', 'notes', 'postgres');" "postgres" || return 1
    
    echo ""
    echo "Individual database sizes:"
    execute_sql "
    SELECT 
        datname as database,
        pg_size_pretty(pg_database_size(datname)) as size
    FROM pg_database 
    WHERE datname IN ('transcriptions', 'ai_chats', 'notes', 'postgres')
    ORDER BY pg_database_size(datname) DESC;" "postgres" || return 1
}

# Export data
export_data() {
    local db="${1}"
    local output_file="${2:-${db}_export_$(date +%Y%m%d_%H%M%S).json}"
    
    echo "Exporting data from database '$db' to $output_file..."
    
    case "$db" in
        "transcriptions")
            psql -d "$(get_connection_string "" "" "$db")" -t -A -c "
            SELECT json_agg(row_to_json(t))
            FROM (
                SELECT t.*, 
                       array_agg(
                           json_build_object(
                               'chunk_number', tc.chunk_number,
                               'content', tc.content,
                               'character_count', tc.character_count
                           ) ORDER BY tc.chunk_number
                       ) as chunks
                FROM transcriptions t
                LEFT JOIN transcription_chunks tc ON t.id = tc.transcription_id
                GROUP BY t.id
            ) t;" > "$output_file" || {
                echo "Error exporting transcriptions" >&2
                return 1
            }
            ;;
        "ai_chats")
            psql -d "$(get_connection_string "" "" "$db")" -t -A -c "
            SELECT json_agg(row_to_json(t))
            FROM ai_chats t
            ORDER BY request_timestamp;" > "$output_file" || {
                echo "Error exporting ai_chats" >&2
                return 1
            }
            ;;
        "notes")
            psql -d "$(get_connection_string "" "" "$db")" -t -A -c "
            SELECT json_agg(row_to_json(t))
            FROM notes t
            ORDER BY last_modified DESC;" > "$output_file" || {
                echo "Error exporting notes" >&2
                return 1
            }
            ;;
        "postgres")
            psql -d "$(get_connection_string "" "" "$db")" -t -A -c "
            SELECT json_agg(row_to_json(t))
            FROM second_brain_vectors t;" > "$output_file" 2>/dev/null || {
                echo "Error exporting vectors (table may not exist)" >&2
                return 1
            }
            ;;
        *)
            echo "Error: Export not implemented for database '$db'" >&2
            echo "Supported databases: transcriptions, ai_chats, postgres" >&2
            return 1
            ;;
    esac
    
    echo "Export completed: $output_file"
}

# Backup database
backup_database() {
    local db="${1:-$DEFAULT_DB_NAME}"
    local backup_file="${2:-${db}_backup_$(date +%Y%m%d_%H%M%S).sql}"
    
    echo "Backing up database '$db' to $backup_file..."
    
    # Check if database exists
    if ! database_exists "$db"; then
        echo "Error: Database '$db' does not exist" >&2
        return 1
    fi
    
    # Get the container ID for the postgres service
    local container_id=$(podman ps --filter "label=io.containers.autoupdate=registry" --format "{{.ID}} {{.Names}}" | grep -E "(postgres|pgvector)" | awk '{print $1}' | head -1)
    
    if [[ -z "$container_id" ]]; then
        # Try another method - look for containers running postgres
        container_id=$(podman ps --format "{{.ID}} {{.Image}}" | grep -E "(postgres|pgvector)" | awk '{print $1}' | head -1)
    fi
    
    if [[ -z "$container_id" ]]; then
        echo "Error: Could not find PostgreSQL container" >&2
        return 1
    fi
    
    echo "Using container: $container_id"
    
    # Use pg_dump from inside the container to avoid version mismatch
    if podman exec "$container_id" pg_dump \
        -h localhost \
        -U "$DEFAULT_DB_USER" \
        -d "$db" \
        --no-owner \
        --no-acl \
        --clean \
        --if-exists \
        --create > "$backup_file" 2>&1; then
        
        # Compress the backup file
        echo "Compressing backup..."
        if gzip "$backup_file"; then
            backup_file="${backup_file}.gz"
            echo "Backup completed successfully: $backup_file"
            
            # Show backup file size
            local size=$(ls -lh "$backup_file" | awk '{print $5}')
            echo "Backup size: $size"
        else
            echo "Warning: Failed to compress backup, keeping uncompressed file"
            echo "Backup completed: $backup_file"
        fi
    else
        echo "Error: Failed to backup database '$db'" >&2
        return 1
    fi
}

# Backup all databases
backup_all_databases() {
    local backup_dir="${1:-backups_$(date +%Y%m%d_%H%M%S)}"
    
    echo "Backing up all databases to directory: $backup_dir"
    
    # Create backup directory
    if ! mkdir -p "$backup_dir"; then
        echo "Error: Failed to create backup directory '$backup_dir'" >&2
        return 1
    fi
    
    # Get list of databases to backup
    local databases=()
    
    # Check which databases exist
    if database_exists "transcriptions"; then
        databases+=("transcriptions")
    fi
    
    if database_exists "ai_chats"; then
        databases+=("ai_chats")
    fi
    
    if database_exists "notes"; then
        databases+=("notes")
    fi
    
    # Always include postgres database (for second_brain_vectors)
    databases+=("postgres")
    
    echo "Found databases to backup: ${databases[*]}"
    echo ""
    
    local success_count=0
    local fail_count=0
    
    # Backup each database
    for db in "${databases[@]}"; do
        echo "Backing up database: $db"
        local backup_file="${backup_dir}/${db}_backup_$(date +%Y%m%d_%H%M%S).sql"
        
        if backup_database "$db" "$backup_file"; then
            ((success_count++)) || true
        else
            ((fail_count++)) || true
            echo "Error: Failed to backup database '$db'" >&2
        fi
        echo ""
    done
    
    # Summary
    echo "Backup summary:"
    echo "  - Successful backups: $success_count"
    echo "  - Failed backups: $fail_count"
    echo "  - Backup directory: $backup_dir"
    
    # List backup files
    if [[ $success_count -gt 0 ]]; then
        echo ""
        echo "Backup files:"
        ls -lh "$backup_dir"/*.gz 2>/dev/null || ls -lh "$backup_dir"/*.sql 2>/dev/null
    fi
    
    return $([[ $fail_count -eq 0 ]] && echo 0 || echo 1)
}

# Archive old data
archive_old_data() {
    local db="${1}"
    local days="${2:-365}"
    local archive_file="${db}_archive_$(date +%Y%m%d_%H%M%S).json"
    
    echo "Archiving data from database '$db' older than $days days..."
    
    case "$db" in
        "transcriptions")
            # First export old transcriptions
            psql -d "$(get_connection_string "" "" "$db")" -t -A -c "
            SELECT json_agg(row_to_json(t))
            FROM (
                SELECT t.*, 
                       array_agg(
                           json_build_object(
                               'chunk_number', tc.chunk_number,
                               'content', tc.content,
                               'character_count', tc.character_count
                           ) ORDER BY tc.chunk_number
                       ) as chunks
                FROM transcriptions t
                LEFT JOIN transcription_chunks tc ON t.id = tc.transcription_id
                WHERE t.created_at < CURRENT_DATE - INTERVAL '$days days'
                GROUP BY t.id
            ) t;" > "$archive_file" || {
                echo "Error archiving transcriptions" >&2
                return 1
            }
            
            # Delete archived transcriptions
            execute_sql "DELETE FROM transcriptions WHERE created_at < CURRENT_DATE - INTERVAL '$days days';" "$db" || {
                echo "Error deleting old transcriptions" >&2
                return 1
            }
            ;;
        "ai_chats")
            # First export old ai_chats
            psql -d "$(get_connection_string "" "" "$db")" -t -A -c "
            SELECT json_agg(row_to_json(t))
            FROM ai_chats t
            WHERE created_at < CURRENT_DATE - INTERVAL '$days days'
            ORDER BY request_timestamp;" > "$archive_file" || {
                echo "Error archiving ai_chats" >&2
                return 1
            }
            
            # Delete archived ai_chats
            execute_sql "DELETE FROM ai_chats WHERE created_at < CURRENT_DATE - INTERVAL '$days days';" "$db" || {
                echo "Error deleting old ai_chats" >&2
                return 1
            }
            ;;
        "notes")
            # First export old notes
            psql -d "$(get_connection_string "" "" "$db")" -t -A -c "
            SELECT json_agg(row_to_json(t))
            FROM notes t
            WHERE last_modified < CURRENT_DATE - INTERVAL '$days days'
            ORDER BY last_modified;" > "$archive_file" || {
                echo "Error archiving notes" >&2
                return 1
            }
            
            # Delete archived notes
            execute_sql "DELETE FROM notes WHERE last_modified < CURRENT_DATE - INTERVAL '$days days';" "$db" || {
                echo "Error deleting old notes" >&2
                return 1
            }
            ;;
        "postgres")
            echo "Archiving not implemented for postgres database (vectors are typically persistent)"
            return 1
            ;;
        *)
            echo "Error: Archive not implemented for database '$db'" >&2
            echo "Supported databases: transcriptions, ai_chats, notes" >&2
            return 1
            ;;
    esac
    
    echo "Archived to: $archive_file"
}

# Get current host information
get_host_info() {
    local hostname=$(hostname)
    local tailscale_ip=""
    
    # Try to get Tailscale IP
    if command -v tailscale &> /dev/null; then
        tailscale_ip=$(tailscale ip -4 2>/dev/null || echo "")
    fi
    
    echo "${hostname}:${tailscale_ip}"
}

# Check if a host is the current host
is_current_host() {
    local host="$1"
    local current_hostname=$(hostname)
    local current_tailscale_ip=""
    
    # Get current Tailscale IP if available
    if command -v tailscale &> /dev/null; then
        current_tailscale_ip=$(tailscale ip -4 2>/dev/null || echo "")
    fi
    
    # Check if the host matches current hostname
    if [[ "$host" == "$current_hostname" ]] || [[ "$host" == "${current_hostname}."* ]]; then
        return 0
    fi
    
    # Check if the host matches current Tailscale IP
    if [[ -n "$current_tailscale_ip" ]] && [[ "$host" == "$current_tailscale_ip" ]]; then
        return 0
    fi
    
    return 1
}

# Check if a remote host is reachable
check_host_reachable() {
    local host="$1"
    local port="${2:-5432}"
    
    # Use timeout command to prevent hanging
    if timeout 3 bash -c "echo > /dev/tcp/${host}/${port}" 2>/dev/null; then
        return 0
    else
        return 1
    fi
}

# Get table schema information
get_table_info() {
    local db="$1"
    local table="$2"
    
    psql -d "$(get_connection_string "" "" "$db")" -tAc "
    SELECT column_name, data_type, is_nullable 
    FROM information_schema.columns 
    WHERE table_name = '$table' 
    ORDER BY ordinal_position;" 2>/dev/null
}

# Check if table has timestamp columns
table_has_timestamps() {
    local db="$1"
    local table="$2"
    
    local schema_info=$(get_table_info "$db" "$table")
    if echo "$schema_info" | grep -q "timestamp"; then
        return 0
    else
        return 1
    fi
}

# Add timestamps to pgvector table if missing
ensure_vector_table_timestamps() {
    # Check if table exists first
    local table_exists=$(psql -d "$(get_connection_string "" "" "postgres")" -tAc "
    SELECT 1 FROM information_schema.tables 
    WHERE table_name = 'second_brain_vectors' LIMIT 1;" 2>/dev/null || echo "0")
    
    if [[ "$table_exists" != "1" ]]; then
        return 0  # Table doesn't exist, nothing to do
    fi
    
    local has_timestamps=$(psql -d "$(get_connection_string "" "" "postgres")" -tAc "
    SELECT COUNT(*) FROM information_schema.columns 
    WHERE table_name = 'second_brain_vectors' 
    AND column_name IN ('created_at', 'updated_at');" 2>/dev/null || echo "0")
    
    if [[ "$has_timestamps" -lt 2 ]]; then
        echo "Adding timestamp columns to second_brain_vectors table..."
        execute_sql "
        ALTER TABLE second_brain_vectors 
        ADD COLUMN IF NOT EXISTS created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
        ADD COLUMN IF NOT EXISTS updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP;" "postgres" || {
            echo "Warning: Could not add timestamps to vector table"
        }
    fi
}

# Perform merge-based sync for a single table
merge_sync_table() {
    local source_host="$1"
    local db="$2"
    local table="$3"
    
    echo "      → Syncing table: $table"
    
    # Debug: Show connection info if debug is enabled
    if [[ "$DEBUG" == true ]]; then
        echo "        Debug: Connection string: $(get_connection_string "" "" "$db")"
        echo "        Debug: Host: $DEFAULT_DB_HOST, Port: $DEFAULT_DB_PORT, DB: $db, User: $DEFAULT_DB_USER"
    fi
    
    # Debug: Check if local table exists
    local table_exists=$(psql -d "$(get_connection_string "" "" "$db")" -tAc "SELECT 1 FROM information_schema.tables WHERE table_name='$table' LIMIT 1" 2>&1)
    if [[ "$table_exists" != "1" ]]; then
        echo "        ✗ Local table '$table' does not exist"
        [[ "$DEBUG" == true ]] && echo "        Debug info: $table_exists"
        # Try to create the schema if it's missing
        case "$db" in
            "transcriptions")
                echo "        → Attempting to initialize transcriptions schema..."
                init_transcriptions_schema
                ;;
            "ai_chats")
                echo "        → Attempting to initialize ai_chats schema..."
                init_ai_chats_schema
                ;;
        esac
    fi
    
    # Create temporary table for remote data
    local temp_table="${table}_remote_sync_$$"
    
    # Copy table structure (use regular table, not TEMP, due to session isolation)
    if [[ "$DEBUG" == true ]]; then
        echo "        Debug: Creating staging table: $temp_table"
        echo "        Debug: Executing: CREATE TABLE $temp_table (LIKE $table INCLUDING ALL);"
    fi
    local create_result=$(execute_sql "CREATE TABLE $temp_table (LIKE $table INCLUDING ALL);" "$db" 2>&1)
    local create_exit_code=$?
    if [[ $create_exit_code -ne 0 ]]; then
        echo "        ✗ Failed to create staging table"
        [[ "$DEBUG" == true ]] && echo "        Error: $create_result"
        return 1
    fi
    [[ "$DEBUG" == true ]] && echo "        Debug: Staging table created successfully"
    
    # Copy data from remote host
    local error_file="/tmp/psql_sync_error_$$"
    # Use a subshell to capture both exit codes
    if (
        set -o pipefail
        PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
            -h "$source_host" \
            -p "$DEFAULT_DB_PORT" \
            -U "$DEFAULT_DB_USER" \
            -d "$db" \
            -c "\\COPY $table TO STDOUT" 2>"${error_file}_remote" | \
        PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
            -h "$DEFAULT_DB_HOST" \
            -p "$DEFAULT_DB_PORT" \
            -U "$DEFAULT_DB_USER" \
            -d "$db" \
            -c "\\COPY $temp_table FROM STDIN" 2>"${error_file}_local"
    ); then
        
        # Perform the merge based on table type
        case "$table" in
            "transcriptions")
                # Merge using UUID and updated_at timestamp
                execute_sql "
                INSERT INTO $table 
                SELECT * FROM $temp_table
                ON CONFLICT (id) DO UPDATE SET
                    filename = EXCLUDED.filename,
                    file_path = EXCLUDED.file_path,
                    file_hash = EXCLUDED.file_hash,
                    transcribed_at = EXCLUDED.transcribed_at,
                    duration_seconds = EXCLUDED.duration_seconds,
                    model_used = EXCLUDED.model_used,
                    language = EXCLUDED.language,
                    metadata = EXCLUDED.metadata,
                    updated_at = GREATEST($table.updated_at, EXCLUDED.updated_at)
                WHERE $table.updated_at < EXCLUDED.updated_at;" "$db" >/dev/null 2>&1
                ;;
            
            "transcription_chunks")
                # Merge chunks - only insert if not exists (they shouldn't change)
                execute_sql "
                INSERT INTO $table 
                SELECT * FROM $temp_table
                ON CONFLICT (transcription_id, chunk_number) DO NOTHING;" "$db" >/dev/null 2>&1
                ;;
            
            "ai_chats")
                # Merge AI chats using UUID and updated_at timestamp
                execute_sql "
                INSERT INTO $table 
                SELECT * FROM $temp_table
                ON CONFLICT (id) DO UPDATE SET
                    prompt = EXCLUDED.prompt,
                    response = EXCLUDED.response,
                    provider = EXCLUDED.provider,
                    model = EXCLUDED.model,
                    request_timestamp = EXCLUDED.request_timestamp,
                    response_timestamp = EXCLUDED.response_timestamp,
                    duration_ms = EXCLUDED.duration_ms,
                    tokens_input = EXCLUDED.tokens_input,
                    tokens_output = EXCLUDED.tokens_output,
                    cost_input_usd = EXCLUDED.cost_input_usd,
                    cost_output_usd = EXCLUDED.cost_output_usd,
                    cost_total_usd = EXCLUDED.cost_total_usd,
                    prompt_hash = EXCLUDED.prompt_hash,
                    response_hash = EXCLUDED.response_hash,
                    tags = EXCLUDED.tags,
                    metadata = EXCLUDED.metadata,
                    updated_at = GREATEST($table.updated_at, EXCLUDED.updated_at)
                WHERE $table.updated_at < EXCLUDED.updated_at;" "$db" >/dev/null 2>&1
                ;;
            
            "ai_chat_contexts")
                # Merge AI chat contexts using UUID and updated_at timestamp
                execute_sql "
                INSERT INTO $table 
                SELECT * FROM $temp_table
                ON CONFLICT (context_uuid) DO UPDATE SET
                    name = EXCLUDED.name,
                    summary = EXCLUDED.summary,
                    summary_provider = EXCLUDED.summary_provider,
                    summary_model = EXCLUDED.summary_model,
                    summary_last_updated = EXCLUDED.summary_last_updated,
                    tags = EXCLUDED.tags,
                    ingested = EXCLUDED.ingested,
                    ingested_path = EXCLUDED.ingested_path,
                    ingested_at = EXCLUDED.ingested_at,
                    updated_at = GREATEST($table.updated_at, EXCLUDED.updated_at)
                WHERE $table.updated_at < EXCLUDED.updated_at;" "$db" >/dev/null 2>&1
                ;;
            
            "notes")
                # Merge notes using UUID and updated_at timestamp
                execute_sql "
                INSERT INTO $table 
                SELECT * FROM $temp_table
                ON CONFLICT (id) DO UPDATE SET
                    path = EXCLUDED.path,
                    para = EXCLUDED.para,
                    category = EXCLUDED.category,
                    content = EXCLUDED.content,
                    file_size = EXCLUDED.file_size,
                    last_modified = EXCLUDED.last_modified,
                    accessed_at = EXCLUDED.accessed_at,
                    metadata = EXCLUDED.metadata,
                    tags = EXCLUDED.tags,
                    updated_at = GREATEST($table.updated_at, EXCLUDED.updated_at)
                WHERE $table.updated_at < EXCLUDED.updated_at;" "$db" >/dev/null 2>&1
                ;;
            
            "second_brain_vectors")
                # For second_brain_vectors, we don't use timestamps for merge logic
                # Instead, we merge based on the ID and keep the latest version
                execute_sql "
                INSERT INTO $table (id, embedding, path, content_snippet, provider, model, metadata)
                SELECT id, embedding, path, content_snippet, provider, model, metadata
                FROM $temp_table
                ON CONFLICT (id) DO UPDATE SET
                    embedding = EXCLUDED.embedding,
                    path = EXCLUDED.path,
                    content_snippet = EXCLUDED.content_snippet,
                    provider = EXCLUDED.provider,
                    model = EXCLUDED.model,
                    metadata = EXCLUDED.metadata;" "$db" >/dev/null 2>&1
                ;;
            
            *)
                # Generic merge - insert only if not exists
                execute_sql "
                INSERT INTO $table 
                SELECT * FROM $temp_table
                ON CONFLICT DO NOTHING;" "$db" >/dev/null 2>&1
                ;;
        esac
        
        # Drop staging table
        execute_sql "DROP TABLE IF EXISTS $temp_table;" "$db" >/dev/null 2>&1
        
        # Clean up error files
        rm -f "${error_file}_remote" "${error_file}_local"
        echo "        ✓ Merged successfully"
        # Explicitly return 0 for success
        return 0
    else
        echo "        ✗ Failed to copy data from remote"
        # Display error messages if debug mode is enabled
        if [[ "$DEBUG" == true ]]; then
            if [[ -s "${error_file}_remote" ]]; then
                echo "        Remote error: $(cat "${error_file}_remote")"
            fi
            if [[ -s "${error_file}_local" ]]; then
                echo "        Local error: $(cat "${error_file}_local")"
            fi
        fi
        # Always clean up staging table on failure
        execute_sql "DROP TABLE IF EXISTS $temp_table;" "$db" >/dev/null 2>&1
        # Clean up error files
        rm -f "${error_file}_remote" "${error_file}_local"
        return 1
    fi
}

# Push data to remote host (bidirectional sync helper)
push_sync_table() {
    local target_host="$1"
    local db="$2"
    local table="$3"
    
    echo "      → Pushing table: $table"
    
    # Create temporary table on remote host for local data
    local temp_table="${table}_local_sync_$$"
    
    # Copy table structure on remote host
    if [[ "$DEBUG" == true ]]; then
        echo "        Debug: Creating staging table on remote: $temp_table"
    fi
    
    local create_result=$(PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
        -h "$target_host" \
        -p "$DEFAULT_DB_PORT" \
        -U "$DEFAULT_DB_USER" \
        -d "$db" \
        -c "CREATE TABLE $temp_table (LIKE $table INCLUDING ALL);" 2>&1)
    local create_exit_code=$?
    if [[ $create_exit_code -ne 0 ]]; then
        echo "        ✗ Failed to create staging table on remote"
        [[ "$DEBUG" == true ]] && echo "        Error: $create_result"
        return 1
    fi
    [[ "$DEBUG" == true ]] && echo "        Debug: Remote staging table created successfully"
    
    # Copy data to remote host
    local error_file="/tmp/psql_push_error_$$"
    if (
        set -o pipefail
        PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
            -h "$DEFAULT_DB_HOST" \
            -p "$DEFAULT_DB_PORT" \
            -U "$DEFAULT_DB_USER" \
            -d "$db" \
            -c "\\COPY $table TO STDOUT" 2>"${error_file}_local" | \
        PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
            -h "$target_host" \
            -p "$DEFAULT_DB_PORT" \
            -U "$DEFAULT_DB_USER" \
            -d "$db" \
            -c "\\COPY $temp_table FROM STDIN" 2>"${error_file}_remote"
    ); then
        
        # Perform the merge on remote host using the same logic
        case "$table" in
            "transcriptions")
                PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
                    -h "$target_host" \
                    -p "$DEFAULT_DB_PORT" \
                    -U "$DEFAULT_DB_USER" \
                    -d "$db" \
                    -c "
                    INSERT INTO $table 
                    SELECT * FROM $temp_table
                    ON CONFLICT (id) DO UPDATE SET
                        filename = EXCLUDED.filename,
                        file_path = EXCLUDED.file_path,
                        file_hash = EXCLUDED.file_hash,
                        transcribed_at = EXCLUDED.transcribed_at,
                        duration_seconds = EXCLUDED.duration_seconds,
                        model_used = EXCLUDED.model_used,
                        language = EXCLUDED.language,
                        metadata = EXCLUDED.metadata,
                        updated_at = GREATEST($table.updated_at, EXCLUDED.updated_at)
                    WHERE $table.updated_at < EXCLUDED.updated_at;" >/dev/null 2>&1
                ;;
            
            "transcription_chunks")
                PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
                    -h "$target_host" \
                    -p "$DEFAULT_DB_PORT" \
                    -U "$DEFAULT_DB_USER" \
                    -d "$db" \
                    -c "
                    INSERT INTO $table 
                    SELECT * FROM $temp_table
                    ON CONFLICT (transcription_id, chunk_number) DO NOTHING;" >/dev/null 2>&1
                ;;
            
            "ai_chats")
                PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
                    -h "$target_host" \
                    -p "$DEFAULT_DB_PORT" \
                    -U "$DEFAULT_DB_USER" \
                    -d "$db" \
                    -c "
                    INSERT INTO $table 
                    SELECT * FROM $temp_table
                    ON CONFLICT (id) DO UPDATE SET
                        prompt = EXCLUDED.prompt,
                        response = EXCLUDED.response,
                        provider = EXCLUDED.provider,
                        model = EXCLUDED.model,
                        request_timestamp = EXCLUDED.request_timestamp,
                        response_timestamp = EXCLUDED.response_timestamp,
                        duration_ms = EXCLUDED.duration_ms,
                        tokens_input = EXCLUDED.tokens_input,
                        tokens_output = EXCLUDED.tokens_output,
                        cost_input_usd = EXCLUDED.cost_input_usd,
                        cost_output_usd = EXCLUDED.cost_output_usd,
                        cost_total_usd = EXCLUDED.cost_total_usd,
                        prompt_hash = EXCLUDED.prompt_hash,
                        response_hash = EXCLUDED.response_hash,
                        tags = EXCLUDED.tags,
                        metadata = EXCLUDED.metadata,
                        updated_at = GREATEST($table.updated_at, EXCLUDED.updated_at)
                    WHERE $table.updated_at < EXCLUDED.updated_at;" >/dev/null 2>&1
                ;;
            
            "ai_chat_contexts")
                PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
                    -h "$target_host" \
                    -p "$DEFAULT_DB_PORT" \
                    -U "$DEFAULT_DB_USER" \
                    -d "$db" \
                    -c "
                    INSERT INTO $table 
                    SELECT * FROM $temp_table
                    ON CONFLICT (context_uuid) DO UPDATE SET
                        name = EXCLUDED.name,
                        summary = EXCLUDED.summary,
                        summary_provider = EXCLUDED.summary_provider,
                        summary_model = EXCLUDED.summary_model,
                        summary_last_updated = EXCLUDED.summary_last_updated,
                        tags = EXCLUDED.tags,
                        ingested = EXCLUDED.ingested,
                        ingested_path = EXCLUDED.ingested_path,
                        ingested_at = EXCLUDED.ingested_at,
                        updated_at = GREATEST($table.updated_at, EXCLUDED.updated_at)
                    WHERE $table.updated_at < EXCLUDED.updated_at;" >/dev/null 2>&1
                ;;
            
            "notes")
                PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
                    -h "$target_host" \
                    -p "$DEFAULT_DB_PORT" \
                    -U "$DEFAULT_DB_USER" \
                    -d "$db" \
                    -c "
                    INSERT INTO $table 
                    SELECT * FROM $temp_table
                    ON CONFLICT (id) DO UPDATE SET
                        path = EXCLUDED.path,
                        para = EXCLUDED.para,
                        category = EXCLUDED.category,
                        content = EXCLUDED.content,
                        file_size = EXCLUDED.file_size,
                        last_modified = EXCLUDED.last_modified,
                        accessed_at = EXCLUDED.accessed_at,
                        metadata = EXCLUDED.metadata,
                        tags = EXCLUDED.tags,
                        updated_at = GREATEST($table.updated_at, EXCLUDED.updated_at)
                    WHERE $table.updated_at < EXCLUDED.updated_at;" >/dev/null 2>&1
                ;;
            
            "second_brain_vectors")
                PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
                    -h "$target_host" \
                    -p "$DEFAULT_DB_PORT" \
                    -U "$DEFAULT_DB_USER" \
                    -d "$db" \
                    -c "
                    INSERT INTO $table (id, embedding, path, content_snippet, provider, model, metadata)
                    SELECT id, embedding, path, content_snippet, provider, model, metadata
                    FROM $temp_table
                    ON CONFLICT (id) DO UPDATE SET
                        embedding = EXCLUDED.embedding,
                        path = EXCLUDED.path,
                        content_snippet = EXCLUDED.content_snippet,
                        provider = EXCLUDED.provider,
                        model = EXCLUDED.model,
                        metadata = EXCLUDED.metadata;" >/dev/null 2>&1
                ;;
            
            *)
                PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
                    -h "$target_host" \
                    -p "$DEFAULT_DB_PORT" \
                    -U "$DEFAULT_DB_USER" \
                    -d "$db" \
                    -c "
                    INSERT INTO $table 
                    SELECT * FROM $temp_table
                    ON CONFLICT DO NOTHING;" >/dev/null 2>&1
                ;;
        esac
        
        # Drop staging table on remote host
        PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
            -h "$target_host" \
            -p "$DEFAULT_DB_PORT" \
            -U "$DEFAULT_DB_USER" \
            -d "$db" \
            -c "DROP TABLE IF EXISTS $temp_table;" >/dev/null 2>&1
        
        # Clean up error files
        rm -f "${error_file}_local" "${error_file}_remote"
        echo "        ✓ Pushed successfully"
        return 0
    else
        echo "        ✗ Failed to push data to remote"
        if [[ "$DEBUG" == true ]]; then
            if [[ -s "${error_file}_local" ]]; then
                echo "        Local error: $(cat "${error_file}_local")"
            fi
            if [[ -s "${error_file}_remote" ]]; then
                echo "        Remote error: $(cat "${error_file}_remote")"
            fi
        fi
        # Clean up staging table on failure
        PGPASSWORD="$DEFAULT_DB_PASSWORD" psql \
            -h "$target_host" \
            -p "$DEFAULT_DB_PORT" \
            -U "$DEFAULT_DB_USER" \
            -d "$db" \
            -c "DROP TABLE IF EXISTS $temp_table;" >/dev/null 2>&1
        # Clean up error files
        rm -f "${error_file}_local" "${error_file}_remote"
        return 1
    fi
}

# Sync databases with remote hosts (bidirectional)
sync_databases() {
    echo "Starting bidirectional database synchronization..."
    echo "Current host: $(get_host_info)"
    echo ""
    
    # Check if any hosts are configured
    if [[ ${#SYNC_HOSTS[@]} -eq 0 ]]; then
        echo "No sync hosts configured. Add hosts to SYNC_HOSTS array in the script."
        return 0
    fi
    
    # Check if any databases are configured
    if [[ ${#SYNC_DATABASES[@]} -eq 0 ]]; then
        echo "No databases configured for sync. Add databases to SYNC_DATABASES array in the script."
        return 0
    fi
    
    local sync_success=0
    local sync_failed=0
    local sync_skipped=0
    
    # Iterate through each configured host
    for host in "${SYNC_HOSTS[@]}"; do
        echo "Processing host: $host"
        
        # Skip if this is the current host
        if is_current_host "$host"; then
            echo "  → Skipping (current host)"
            ((sync_skipped++)) || true
            continue
        fi
        
        # Check if host is reachable
        if ! check_host_reachable "$host"; then
            echo "  → Host unreachable (soft fail)"
            ((sync_skipped++)) || true
            continue
        fi
        
        echo "  → Host is reachable, performing bidirectional sync..."
        
        # Sync each configured database
        for db in "${SYNC_DATABASES[@]}"; do
            echo "    Syncing database: $db"
            [[ "$DEBUG" == true ]] && echo "      Debug: Processing database: $db"
            
            # Ensure database exists locally
            if ! database_exists "$db"; then
                echo "      → Creating local database..."
                if [[ "$db" == "transcriptions" ]]; then
                    create_database
                elif [[ "$db" == "ai_chats" ]]; then
                    init_ai_chats_schema
                elif [[ "$db" == "notes" ]]; then
                    init_notes_schema
                else
                    execute_sql "CREATE DATABASE $db" "postgres" >/dev/null 2>&1
                fi
            fi
            
            # Get list of tables to sync based on database
            local tables_to_sync=()
            case "$db" in
                "transcriptions")
                    tables_to_sync=("transcriptions" "transcription_chunks")
                    ;;
                "ai_chats")
                    tables_to_sync=("ai_chat_contexts" "ai_chats")
                    ;;
                "notes")
                    tables_to_sync=("notes")
                    ;;
                "postgres")
                    # For postgres db, sync the vector table if it exists
                    if psql -d "$(get_connection_string "" "" "$db")" -tAc "SELECT 1 FROM information_schema.tables WHERE table_name='second_brain_vectors'" 2>/dev/null | grep -q 1; then
                        tables_to_sync=("second_brain_vectors")
                    else
                        echo "      → No second_brain_vectors table found, skipping"
                        continue
                    fi
                    ;;
                *)
                    echo "      → Unknown database type, skipping"
                    continue
                    ;;
            esac
            
            # Perform bidirectional sync for each table
            local db_sync_success=true
            for table in "${tables_to_sync[@]}"; do
                echo "      Bidirectional sync for table: $table"
                
                # PULL: Get data from remote host to local
                echo "      ← Pulling from remote..."
                if merge_sync_table "$host" "$db" "$table" || false; then
                    ((sync_success++)) || true
                else
                    ((sync_failed++)) || true
                    db_sync_success=false
                fi
                
                # PUSH: Send data from local to remote host
                echo "      → Pushing to remote..."
                if push_sync_table "$host" "$db" "$table" || false; then
                    ((sync_success++)) || true
                else
                    ((sync_failed++)) || true
                    db_sync_success=false
                fi
            done
            
            if [[ "$db_sync_success" == true ]]; then
                echo "      ✓ Bidirectional database sync completed"
            else
                echo "      ✗ Bidirectional database sync had errors"
            fi
        done
        
        echo ""
    done
    
    # Print summary
    echo "Bidirectional synchronization complete:"
    echo "  - Successful operations: $sync_success"
    echo "  - Failed operations: $sync_failed"
    echo "  - Skipped operations: $sync_skipped"
    
    # Return non-zero if all operations failed
    if [[ $sync_success -eq 0 ]] && [[ $sync_failed -gt 0 ]]; then
        return 1
    fi
    
    return 0
}

# Usage help
usage() {
    cat << EOF
Usage: $SCRIPT_NAME [OPTIONS] COMMAND

Universal PostgreSQL management script for local dotfiles.
Supports multiple databases: transcriptions, ai_chats, notes, postgres.

COMMANDS:
    setup               Setup database and all schemas (alias for init)
    init                Initialize all schemas (transcriptions + ai_chats + notes + second_brain)
    init-transcripts    Initialize transcriptions schema only
    init-ai             Initialize ai_chats schema only
    init-notes          Initialize notes schema only
    init-second-brain   Initialize second brain vector schema only
    maintain            Perform maintenance operations (vacuum, reindex, cleanup)
    stats               Show database statistics
    export [FILE]       Export data to JSON file
    archive [DAYS]      Archive data older than DAYS (default: 365)
    sync                Bidirectional sync with configured remote hosts (keeps latest records)
    backup [FILE]       Backup database to SQL dump file
    
OPTIONS:
    -h, --help          Show this help message
    -v, --verbose       Enable verbose output
    --debug             Enable debug output (shows detailed error messages)
    -d, --database DB   Specify target database (required for maintain/export/archive)
                        Available: transcriptions, ai_chats, notes, postgres
                        Optional for stats and backup (shows/backs up all databases if not specified)
    --all               Backup all databases (transcriptions, ai_chats, notes, postgres)
    
ENVIRONMENT VARIABLES:
    LOCAL_DB_HOST             Database host (default: 127.0.0.1)
    LOCAL_DB_PORT             Database port (default: 5432)
    LOCAL_DB_NAME             Database name (default: transcriptions)
    LOCAL_DB_USER             Database user (default: postgres)
    LOCAL_DB_PASSWORD         Database password (default: none)
    
    Backward compatibility (deprecated):
    TRANSCRIPTION_DB_*        Legacy environment variables (still supported)

EXAMPLES:
    # Initial setup
    $SCRIPT_NAME setup
    
    # Perform maintenance on specific databases
    $SCRIPT_NAME --database transcriptions maintain
    $SCRIPT_NAME --database ai_chats maintain
    
    # Show statistics for specific databases
    $SCRIPT_NAME --database transcriptions stats
    $SCRIPT_NAME --database ai_chats stats
    
    # Show statistics for all databases
    $SCRIPT_NAME stats
    
    # Export data from specific databases
    $SCRIPT_NAME --database transcriptions export my_transcriptions.json
    $SCRIPT_NAME --database ai_chats export my_chats.json
    
    # Archive old data from specific databases
    $SCRIPT_NAME --database transcriptions archive 180
    $SCRIPT_NAME --database ai_chats archive 365
    
    # Backup specific database
    $SCRIPT_NAME --database transcriptions backup
    $SCRIPT_NAME --database ai_chats backup my_chats_backup.sql
    
    # Backup all databases explicitly
    $SCRIPT_NAME --all backup
    $SCRIPT_NAME --all backup my_backups_dir
    
    # Backup all databases to a directory (default behavior)
    $SCRIPT_NAME backup
    $SCRIPT_NAME backup my_backups_dir
    
    # Bidirectional sync with remote hosts
    $SCRIPT_NAME sync
    
SYNC CONFIGURATION:
    Edit SYNC_DATABASES and SYNC_HOSTS arrays at the top of this script
    to configure which databases and hosts to sync with.
    
    Sync is bidirectional and uses timestamp-based conflict resolution:
    - Data is both pulled from and pushed to each remote host
    - Conflicts are resolved by keeping the most recent record (updated_at)
    - UUIDs prevent duplicate records across hosts
EOF
}

# Parse command line arguments
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -h|--help)
                usage
                exit 0
                ;;
            -v|--verbose)
                VERBOSE=true
                shift
                ;;
            --debug)
                DEBUG=true
                shift
                ;;
            -d|--database)
                TARGET_DATABASE="$2"
                if [[ -z "$TARGET_DATABASE" ]]; then
                    echo "Error: --database requires a database name" >&2
                    exit 1
                fi
                shift 2
                ;;
            --all)
                BACKUP_ALL=true
                shift
                ;;
            setup|init|init-transcripts|init-ai|init-notes|init-second-brain|maintain|stats|export|archive|backup|sync)
                ACTION="$1"
                shift
                break
                ;;
            *)
                echo "Unknown option: $1" >&2
                usage
                exit 1
                ;;
        esac
    done
    
    # Store remaining arguments
    ARGS=("$@")
}

# Main execution
main() {
    parse_args "$@"
    
    if [[ -z "$ACTION" ]]; then
        echo "Error: No command specified" >&2
        usage
        exit 1
    fi
    
    # Check psql availability
    if ! command -v psql &> /dev/null; then
        echo "Error: psql command not found. Please install PostgreSQL client." >&2
        exit 1
    fi
    
    # Commands that require --database option for single database operations
    case "$ACTION" in
        maintain|export|archive)
            if [[ -z "$TARGET_DATABASE" ]]; then
                echo "Error: Command '$ACTION' requires --database option" >&2
                echo "Available databases: transcriptions, ai_chats, postgres" >&2
                exit 1
            fi
            ;;
    esac
    
    case "$ACTION" in
        setup)
            setup_all
            ;;
        init)
            init_all_schemas
            ;;
        init-transcripts)
            init_transcriptions_schema
            ;;
        init-ai)
            init_ai_chats_schema
            ;;
        init-notes)
            init_notes_schema
            ;;
        init-second-brain)
            init_second_brain_schema
            ;;
        maintain)
            perform_maintenance "$TARGET_DATABASE"
            ;;
        stats)
            if [[ -n "$TARGET_DATABASE" ]]; then
                show_stats "$TARGET_DATABASE"
            else
                show_all_stats
            fi
            ;;
        export)
            export_data "$TARGET_DATABASE" "${ARGS[0]}"
            ;;
        archive)
            archive_old_data "$TARGET_DATABASE" "${ARGS[0]}"
            ;;
        backup)
            if [[ "$BACKUP_ALL" == true ]]; then
                # Backup all databases (explicit --all flag)
                backup_all_databases "${ARGS[0]:-}"
            elif [[ -n "$TARGET_DATABASE" ]]; then
                # Backup specific database
                backup_database "$TARGET_DATABASE" "${ARGS[0]:-}"
            else
                # Backup all databases (default behavior when no specific database is specified)
                backup_all_databases "${ARGS[0]:-}"
            fi
            ;;
        sync)
            # Run sync in a subshell to prevent set -e from exiting early
            (sync_databases) || true
            ;;
        *)
            echo "Unknown command: $ACTION" >&2
            usage
            exit 1
            ;;
    esac
}

# Run main function
main "$@"
