[Unit]
Description=RamaLama /var/home/zach/.local/share/ramalama/models/ollama/llama3.2:3b AI Model Service
After=local-fs.target

[Container]
AddDevice=-/dev/dri
AddDevice=-/dev/kfd
PublishPort=127.0.0.1:8080:8080
#Exec=llama-server --port 8080 -m /mnt/models/model.file -c 2048 --temp 0.8 -ngl 999 --host 0.0.0.0
Exec=llama-server --port 8080 -m /mnt/models/model.file -c 2048 --temp 0.8 --host 0.0.0.0
Image=quay.io/ramalama/ramalama

Mount=type=bind,src=/var/home/zach/.local/share/ramalama/models/ollama/llama3.2:3b,target=/mnt/models/model.file,ro,Z
ContainerName=llama

[Install]
# Start by default on boot
WantedBy=multi-user.target default.target
