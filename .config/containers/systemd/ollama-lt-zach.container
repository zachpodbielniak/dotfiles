[Unit]
Description=Ollama container
Wants=network-online.target
After=network-online.target

[Container]
Image=docker.io/ollama/ollama:0.12.11-rocm
PublishPort=127.0.0.1:11434:11434
PublishPort=100.72.0.41:11434:11434
AddDevice=/dev/kfd 
AddDevice=/dev/dri 
Volume=/var/home/zach/.ollama:/root/.ollama:z

# AMD ROCm specific - spoofs GPU version for compatibility with newer AMD GPUs
Environment=HSA_OVERRIDE_GFX_VERSION=11.5.1

# Number of parallel requests to handle simultaneously (default: 4 or 1 based on memory)
# Higher values allow more concurrent requests but require more VRAM
Environment=OLLAMA_NUM_PARALLEL=4

# Maximum number of models to keep loaded in VRAM simultaneously (default: 1)
# Allows faster switching between models at the cost of more VRAM usage
Environment=OLLAMA_MAX_LOADED_MODELS=2

# Reserved VRAM overhead in bytes (default: 10-20% of VRAM)
# Setting to 0 maximizes available VRAM for models but may cause OOM errors
Environment=OLLAMA_GPU_OVERHEAD=0

# Enable Flash Attention optimization for better memory efficiency (default: 0)
# Reduces memory usage and improves performance on supported GPUs
Environment=OLLAMA_FLASH_ATTENTION=1

# Force all model layers to load on GPU (default: auto-detects based on VRAM)
# Set to 999 to force maximum GPU usage, or specific number for partial offloading
Environment=OLLAMA_NUM_GPU=999

SecurityLabelDisable=true

[Service]
Restart=always

[Install]
WantedBy=default.target


